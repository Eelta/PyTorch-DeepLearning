# 线性模型

核心思想：

X：左横右竖（维度n）；Y：朝向与X相同（维度m）。

针对单个样本下的计算：X(1,n) × W(n,m) = Y(1,m)

如有seq_len：X(s,n) × W(n,m) = Y(s,m)，

如有Batch样本：X(b,s,n) × W(n,m) = Y(b,s,m)，这里其实是将(b,s,n)先变为(b*s,n)批量处理，再自动还原回(b,s,m)，因此pytorch可以处理任意维度的多维张量。每个 token 独立过线性层，互不影响，只是批量处理而已。

在Pytorch存储过程中，常将W以(m,n)的形式存储，运算过程中以WT(n,m)的形式计算。



设每次计算单个W对N个样本的的取值结果，则每次N个样本将输出N个预测值构成n维向量，将这个预测向量与实际向量（即标签）进行误差对比，对应的N个样本元素做减法，求平方，再除以N（即取N个样本的均值），可得出MSE，也就是损失函数。

每次对W进行测试都将输出一个损失值（针对n个样本的），则k次测试将生成k个损失值（1个Epoch中有k次iteration，每次iteration遍历batchsize个样本，每个Epoch结束时保留checkpoint）。最终目标是确定k次测试中的某个W，使得对应损失值最小。

**总结：**

1。使用单个权重，去乘以多个样本，获取结果作为预测值集合。

2。将预测值集合与真实值集合计算获取单个权重下的误差。

3。对权重更新，使得对应误差最小。

# 梯度下降

这里还有更好的方式：梯度下降（正规方程一般不使用）

梯度下降原理：设损失函数为L，对L求W（权重）的导数，通过导数大于或者小于0，判断L基于W下降的方向，不断靠近L的最低点（贪心算法）。 

W更新：W=W-αL‘，这里α是学习率，即每次向最低点移动的步长，过大会导致模型无法收敛。

局部最优：由于点是连续进行移动的，当点移动至某个位置时，导数可能接近0，但是此时只是局部最优点，而不是全局最优点（但实际上深度学习中，这样的局部最优点很少）。

鞍点：一段曲线内，导数值均为0，将导致点无法移动，模型无法继续迭代，解决方式：随机梯度下降（SGD）：在计算误差时，每次只使用一个样本。如每次选用一小组样本，则称为mini-Batch方法。

# 反向传播

在图上进行梯度传播，创建更具有弹性的模型结构。

<img src="笔记图保存\fb67ec5e2611c675e44696d20c730dc3.png" alt="fb67ec5e2611c675e44696d20c730dc3" style="zoom: 33%;" />

输入X是5维向量，第1层h(1)输出的Y是6维向量，则W只能是5行6列，即存放权重矩阵需要30个元素。

这里的点看作是X中的每个特征，连接线看作是W与X进行线性组合的匹配计算过程，例如这里（Pytorch中实际上常是X*W的形式）：

<img src="笔记图保存\78c83073c16ff4f5b488d4445d6a41db.png" alt="78c83073c16ff4f5b488d4445d6a41db" style="zoom: 50%;" />

神经元：针对变量指的是向量维数，而针对矩阵则是通道数量。

<img src="笔记图保存\0d7c23ddef101e97c105c132ac5b90b0.png" alt="0d7c23ddef101e97c105c132ac5b90b0" style="zoom:50%;" />

在一个神经网络中，权重*输入+偏置量=第1层

<img src="笔记图保存\2cd40aba1e9f50bbb868d767a113cb41.png" alt="2cd40aba1e9f50bbb868d767a113cb41" style="zoom: 33%;" />

问题：如果每次都类似上面进行计算，函数展开后形式将始终不变化，变换失去意义。为此，在每次结果输出时需要对结果向量做一个非线性函数的变换（即激活函数）。

<img src="笔记图保存\fa7ae0ef402295b7136744f5146b4dd5.png" alt="fa7ae0ef402295b7136744f5146b4dd5" style="zoom: 33%;" />

回忆链式法则（chain rule）：(f(g(x)))' = f'(g(x)) * g'(x)，它解决了什么问题？其实就是嵌套过多层函数时，多层求导难度较大的问题。也就是在多层网络中，L对W如何得出求导结果的问题。

<img src="笔记图保存\PixPin_2025-08-18_12-01-18.png" alt="PixPin_2025-08-18_12-01-18" style="zoom:33%;" />

这里，Z表示X和W的运算规则，知道Z函数表达式即可轻松获取Z对X导数、Z对W导数。

之后从Loss处返回损失函数L对于Z的导数，

目标是计算L对X的偏导数（这里需要计算的原因是，X不一定就代表数据输入，它可能是一个中间层输出，因此需要有对它进行计算的能力）、以及L对W的偏导数，此时再采用链式法则，由于先前Z对X导数、Z对W导数、L对于Z导数都已经获取，可直接计算出结果。

整体上，先前馈传播，再反向传播，最终得出L对W的梯度（导数）、L对X的梯度。

<img src="笔记图保存\bd4544408b1713c020fb50ddb2f0b4c6.png" alt="bd4544408b1713c020fb50ddb2f0b4c6" style="zoom: 33%;" />

PyTorch中最基础的数据结构是Tensor，它可以存储一维向量、矩阵、多维矩阵等，它是一个类，属性包括W的数值和损失函数对W的导数（这里data和grad均为Tensor（张量））。

```python
import torch

#这里以列表形式设置了初始数据
x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

#设置初始权重值
#可以看到这里设置的是一维权重，因为x_data中每列是一个样本，且单个样本的特征维度是1，Y的维度也是1。
w = torch.Tensor([1.0])
#此处设置了需要计算梯度。即设定之后会需要对这个参数进行更新。
w.requires_grad = True

#forward 前向传播（前面提及的Z函数的表达式）
def forward(x):
    return x * w
#这里w是Tensor，所以x * w自动重载为两个Tensor间的数乘运算，x直接被自动转换为Tensor类型以参与计算。
#针对此处，因为输入时w是需要计算梯度的，所以输出结果z=x * w这个Tensor也被设置为需要计算梯度（回忆之前内容中，存在着L对z求梯度的过程）
 
#定义损失函数，这里每运行一次，就构建一个“计算图”
def loss(x, y):
    y_pred = forward(x)
    return (y_pred - y) ** 2
 
#输出预测数值
print("predict (before training)", 4, forward(4).item())

#设置epoch=100，进行100次完整的样本训练
for epoch in range(100):
    #在每个epoch中，每次是针对单个样本计算，这里的计算方式是随机梯度下降SGD
    for x, y in zip(x_data, y_data):
        #构建“计算图”，获取损失函数的Tensor
        l = loss(x, y)
        #调用backward函数后，将“计算图”中所有设置的-需要梯度的地方计算对应梯度，并保存在对应Tensor中。之后释放“计算图”。
        #这里释放“计算图”，是因为每次构建的“计算图”可能是不同的，因此选择不保留，这种方式也使得Pytorch较为灵活。
        #区别：w.grad.data允许在张量层面进行运算并对梯度值进行更新，而w.grad.item()则只能在标量层面进行操作。基于w.grad.data这种方式可对梯度进行更新，同时不会修改“计算图”。
        l.backward()
        #展示梯度值和权重数值
        print('\tgrad:', x, y, w.grad.item(), w.data)
        #W更新：W=W-αL'
        w.data = w.data - 0.01 * w.grad.data
        print(w.data)
        #PyTorch 会自动将梯度累积到.grad属性中。也就是说，如果不清除梯度，它们会在每次迭代时叠加，导致梯度越来越大。w.grad.data.zero_()用于将梯度清空为0。
        w.grad.data.zero_()
        #展示更新后的权重数值和更新后的梯度（这里是0）
        print(w.data, w.grad.item())
	#展示每个epoch结束时对应的损失值
    print('progress:', epoch, l.item())
 
print('predict (after taining)', 4, forward(4).item())

#总结：总计3个步骤：
#1.前向：算损失
#2.反向：算梯度
#3.更新：更新W权重
```

```
print(w)  # Tensor，需构造计算图
print(w.data)  # Tensor，不构造计算图
print(w.data.item())
print(w.grad)  # Tensor，构造计算图，可发现实际上w.grad==w.grad.data
print(w.grad.item())
print(w.grad.data)  # Tensor，不构造计算图
print(w.grad.data.item())

w.grad.data这种操作，目前已过时，更推荐使用：
with torch.no_grad():
    w.grad += 1  # 安全地修改梯度

OUT PUT:
tensor([2.0000], requires_grad=True)
tensor([2.0000])
2.0000007152557373
tensor([0.])
0.0
tensor([0.])
0.0
```

# 用PyTorch实现线性回归

广播机制：不同形状的矩阵计算中，对矩阵进行自动扩充。即将空的维度部分复制补充至最高维。

<img src="笔记图保存\4c3e3ae5d09a602b788a84e05967f9c8.png" alt="4c3e3ae5d09a602b788a84e05967f9c8" style="zoom:50%;" />

```python
import torch

# 和先前不同的地方在于这里将初始数据直接设置为张量，数据集：样本量为3，X和Y的特征维度均为1。
x_data = torch.Tensor([[1.0], [2.0], [3.0]])
y_data = torch.Tensor([[2.0], [4.0], [6.0]])


# 自己想要实现的方式Pytorch难以实现：
# 1.如果可以使用Python基础方法实现，则将其封装为模型类（继承torch.nn.Module）。
# 2.如果求导效率不高，需要自己手动实现反向传播的计算块，可以继承Functions类，如继承Functions类则需要手动额外实现反向传播（因为反向传播过程涉及求导）。

# 这里将模型定义为继承自Module的类（因Module中具有较多合适方法）
class LinearModel(torch.nn.Module):
    # 这里模型至少需要设置2个函数：构造函数和forward函数
    # 1）设置构造函数，初始化对象时可进行属性定义
    def __init__(self):
        super(LinearModel, self).__init__()
        self.linear = torch.nn.Linear(1, 1)
        # torch.nn.Linear是一个类，此处在构造对象，torch.nn.Linear(in features, out features, bias=True)
        # 这里，in features=每一个输入样本（X）的特征维数，out features=每一个输出样本（Y）的维数，bias=是否需要偏置项，这里通过in features和out features可确定每一个权重矩阵W的形状。

    # 2）必须定义并实现forward函数，用于定义前馈计算Z
    # 此处实际上是重载了父类方法，类似_call_方法，当调用对象时会直接使用该函数中的内容。也称为magic method
    # 之后要使用该类时，直接：model=LinearModel(), model(x)即可，其内部做的就是：wx+b，返回的是y_pred
    def forward(self, x):
        y_pred = self.linear(x)
        return y_pred


# _call_方法示例：
class Shit:
    def __init__(self):
        pass

    def __call__(self, *args, **kwargs):
        print(args)
        print(kwargs)


# 当实例化对象时，隐式调用的是__init__
shit = Shit()
# 当直接调用对象时，调用的是__call__
shit(1, 2, 3, x=6, y=7)
'''
输出结果：
(1, 2, 3)
{'x': 6, 'y': 7}
'''

# 定义线性模型对象
model = LinearModel()

# 定义损失函数，此处设置为MSE，在 PyTorch 的新版本中，size_average 已经被弃用，
# 取而代之的是 reduction 参数。这个参数有三个选项：
# reduction='mean'：类似于 size_average=True，计算损失的平均值。
# reduction='sum'：类似于 size_average=False，计算损失的总和。
# reduction='none'：不进行任何平均或求和，返回每个样本的损失值。
criterion = torch.nn.MSELoss(reduction='mean')

# 构建对W进行更新的优化器（W=W-αL'），此处设置SGD随机梯度下降
# 但是实际上，SGD是一个方法类，确定是使用单个样本，还是全样本，还是mini-batch，是根据输入数据确定。类似此处使用的就是全样本进行统一计算，对所有样本计算损失均值，并更新W权重。Batch-SGD。
# 实例化了一个SGD类，model是LinearModel，LinearModel里有linear，而linear有parameters。
# 这里使用model.parameters()，目的是找出所有LinearModel中需要进行更新的权重参数。
# 设定固定学习率为0.01
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# 训练过程，这里对全样本进行计算更新
for epoch in range(1000):
    # 1.前馈计算，算出y_pred
    y_pred = model(x_data)
    # 2.计算损失，传入y_pred（预测）和y_data（真实），返回损失的Tensor
    loss = criterion(y_pred, y_data)
    # 展示迭代次数和对应Tensor的损失数值
    print(epoch, loss.data.item())
    # 3.先将优化器中的梯度值清零（防止在上一次迭代中遗留下W的梯度）
    optimizer.zero_grad()
    # 4.利用损失的Tensor进行反向传播
    loss.backward()
    # 5.利用优化器进行权值W和偏置b的迭代
    optimizer.step()

print('w = ', model.linear.weight.item())
print('b = ', model.linear.bias.item())

x_test = torch.Tensor([[4.0]])
y_test = model(x_test)
print('y_pred = ', y_test.data)
```

不同优化器的效果展示：

torch.optim.Adagrad
torch.optim.Adam
torch.optim.Adamax
torch.optim.ASGD
torch.optim.LBFGS
torch.optim.RMsprop
torch.optim.Rprop
torch.optim.SGD

<img src="笔记图保存\50606a723f3b467257ce560d69a616d8.png" alt="50606a723f3b467257ce560d69a616d8" style="zoom: 33%;" />

# Logistic（逻辑）回归

n分类问题：和回归问题不同，分类模型的输出应该是和为1的n个概率值，并最终判别结果属于哪个类（概率最大项）。

P(x∈Z) = n

Logistic回归是针对二分类问题的。

经典激活函数Sigmod：用于将线性回归的结果映射至[0, 1]区间内。其导函数类似正态分布形状。

考虑：当线性回归的预测值非常大（正负）时，Sigmoid函数的输出会接近0或1吗？

实际上是会的，解决方案：

1. 特征标准化（缩放特征为0至1范围内）。

2. L1、L2正则化，防止W矩阵调节程度过大。

3. 使用其它激活函数。


<img src="笔记图保存\7ed7aefe0d4cd592c66aea4fc7bc0bf0.png" alt="7ed7aefe0d4cd592c66aea4fc7bc0bf0" style="zoom: 33%;" />

常见的Sigmod函数：

<img src="笔记图保存\PixPin_2025-08-18_16-34-38.png" alt="PixPin_2025-08-18_16-34-38" style="zoom: 33%;" />

使用激活函数之后，仍只能获取范围在[0 ,1]区间的预测值集合。

此时损失函数不再是计算数值之间的距离，而是用于计算两个分布概率值之间的差异。可用KL散度（又称相对熵）、交叉熵等作为损失函数。

例如BCE损失（Binary Cross-Entropy Loss，二元交叉熵损失）的计算公式：

<img src="笔记图保存\4a07772035b7687da9bafad4d2cc8d9c.png" alt="4a07772035b7687da9bafad4d2cc8d9c" style="zoom:33%;" />

当Y_ac（实际值）=1时，Y_pre（线性回归结果经Sigmod映射后的数值）越接近1，将使得整体函数数值越小（类似先前内容中，预测值和真实值越接近，会导致损失函数越小）；Y_pre越接近0，整体函数数值越大。

当Y_ac=0时，Y_pre越接近0，整体函数数值越小；Y_pre越接近1，整体函数数值越大。

<img src="笔记图保存\4a0084535328759b85fce27478daba17.png" alt="4a0084535328759b85fce27478daba17" style="zoom:33%;" />

```python
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['font.size'] = '18'
matplotlib.use('TkAgg')

x_data = torch.Tensor([[1.0], [2.0], [3.0]])
# 真实数据Y变为类别0, 1，而不是数值类型
y_data = torch.Tensor([[0], [0], [1]])


class LogisticRegressionModel(torch.nn.Module):
    def __init__(self):
        super(LogisticRegressionModel, self).__init__()
        self.linear = torch.nn.Linear(1, 1)

    # 区别1，前向传播对预测值进行计算时，添加了sigmoid函数映射
    def forward(self, x):
        y_pred = torch.sigmoid(self.linear(x))
        return y_pred


model = LogisticRegressionModel()
# 区别2，定义BCE为损失函数
criterion = torch.nn.BCELoss(reduction='mean')
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

for epoch in range(1000):
    y_pred = model(x_data)
    loss = criterion(y_pred, y_data)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# 针对不同的x生成预测值并展示
x = np.linspace(0, 10, 200)
# reshape操作
x_t = torch.Tensor(x).view((200, 1))
y_t = model(x_t)
y = y_t.data.numpy()
plt.plot(x, y)
plt.plot([0, 10], [0.5, 0.5], c='r')
plt.xlabel('Hours')
plt.ylabel('Probability of Pass')
plt.grid()
plt.show()
```

# 处理多维特征的输入

假设对8维特征的N个样本进行Mini-Batch计算：

注意此处Sigmod函数进行的是向量化运算（即对一个列表中的每个元素，进行逐个元素的计算）。偏置项b直接通过广播机制进行扩充。

这里，Z代表函数值。X矩阵中，每行代表一个样本。

<img src="笔记图保存\3abb0df78cc8b50d3386dc0803db1937.png" alt="3abb0df78cc8b50d3386dc0803db1937" style="zoom:33%;" />

假设需要按照上面的方式进行调整，只需修改此处模型的输入维数即可。 

为什么有时会调整输出维度：

可以看作，这里只是调整中间步骤输出的维度，需要在进行多次变换后，最后再次转换为一维输出结果。

思想：通过多个层的线性变换，去拟合非线性变换。

神经网络目的：寻找一种非线性变换的空间函数。

```python
import numpy as np
import torch
from sklearn.datasets import load_diabetes

diabetes = load_diabetes()
x = diabetes.data
y = diabetes.target

# 这里使用32位浮点数，而不使用64位Double，是因为N卡一般支持32位浮点。
xy = np.array(np.concatenate([x, np.array([y]).T], axis=1), dtype=np.float32)
# 取numpy数据构造tensor
x_data = torch.from_numpy(xy[:, :-1])
# 这里[-1]表示取出的是矩阵，否则是向量
y_data = torch.from_numpy(xy[:, [-1]])


class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.linear1 = torch.nn.Linear(10, 6)
        self.linear2 = torch.nn.Linear(6, 4)
        self.linear3 = torch.nn.Linear(4, 1)
        # 这里定义Sigmoid模块，注意和之前定义的函数torch.sigmod()是不同的，这里定义一个模块用于多次调用，构建计算图。
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x):
        # 这里每层都可以设置不同的激活函数
        x = self.sigmoid(self.linear1(x))
        x = self.sigmoid(self.linear2(x))
        # 这里如果设置激活函数为Relu，所有小于0的数值将映射为0，因此最后一层一般设置为sigmod
        x = self.sigmoid(self.linear3(x))
        return x


model = Model()
criterion = torch.nn.BCELoss(reduction='mean')
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

for epoch in range(100):
    y_pred = model(x_data)
    loss = criterion(y_pred, y_data)
    print(epoch, loss.item())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

不同激活函数的对比：

[torch.nn — PyTorch 2.5 documentation](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)

<img src="笔记图保存\ce09fa1d32a28e0e735be501e744f704.png" alt="ce09fa1d32a28e0e735be501e744f704" style="zoom: 50%;" />

# 加载数据集

**对比 SGD 和 Mini-batch GD**

- **纯 SGD（单样本）**：每次迭代使用一个样本，随机性最强，有助于逃离局部极小值。**单次迭代速度快，但收敛过程不稳定，需要更多迭代才能达到较好的收敛效果**。
- **Mini-batch SGD**：每次迭代使用一个小批量样本（例如 Batch Size=32 或 64）。兼顾了**收敛速度、稳定性、计算效率**。
- **Batch GD（全数据）**：没有随机性，每次迭代使用全数据，存在鞍点问题。单次迭代**最慢**，不适合大规模数据。

因此，**Mini-batch SGD** 是实际应用中最常用的形式，它在效率和梯度波动之间找到了一个平衡点。



常用词汇解释：

**Epoch**：整个数据集被完整训练一次。

**Batch Size**：每次迭代时所处理的数据量。

**Iteration**：一次权重更新，处理一个Batch。

1个Epoch中，Iteration = 数据样本总数 / Batch Size

<img src="笔记图保存\f5d22739127664a70c8221a9b623b76d.png" alt="f5d22739127664a70c8221a9b623b76d" style="zoom: 33%;" />

Dataset：用于数据索引

DataLoader：用于数据加载mini-batch

<img src="笔记图保存\108f832436b8c37f1534360adfc7a0d7.png" alt="108f832436b8c37f1534360adfc7a0d7" style="zoom:33%;" />

```python
import numpy as np
import pandas as pd
import torch
from sklearn.datasets import load_diabetes

# 这里，Dataset是一个抽象类（不能被实例化，只能被继承）
from torch.utils.data import Dataset, DataLoader

# 读取并创建数据集
diabetes = load_diabetes()
x = diabetes.data
y = diabetes.target
data = np.array(np.concatenate([x, np.array([y]).T], axis=1), dtype=np.float32)
pd.DataFrame(data).to_csv('diabetes.csv', index=False)


class DiabetesDataset(Dataset):
    # 初始化读取数据：一般2种方式
    # 1.如果数据集不大，可直接全部加载进入内存。
    # 2.如果数据集较大，仅保存数据样本的索引，在getitem被调用时再实时读取数据。
    def __init__(self, filepath):
        xy = pd.read_csv(filepath)
        self.len = xy.shape[0]
        self.x_data = xy.iloc[:, :-1].values.astype('float32')
        self.y_data = xy.iloc[:, xy.shape[1] - 1:xy.shape[1]].values.astype('float32')

    # 这里__getitem__是一个magic method，当对象使用下标时自动调用该方法
    # 此处返回元组
    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    # 这里__len__也是一个magic method，当对象调用len()时自动调用该方法
    def __len__(self):
        return self.len


dataset = DiabetesDataset('diabetes.csv')
# 这里，num_workers代表设置去读取batch_size数据的并行线程数。当数据集较小时，使用多线程反而可能降低运行速度。
train_loader = DataLoader(dataset=dataset,
                          batch_size=32, 
                          shuffle=True,
                          num_workers=2)


class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.linear1 = torch.nn.Linear(10, 6)
        self.linear2 = torch.nn.Linear(6, 4)
        self.linear3 = torch.nn.Linear(4, 1)
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x):
        x = self.sigmoid(self.linear1(x))
        x = self.sigmoid(self.linear2(x))
        x = self.sigmoid(self.linear3(x))
        return x


model = Model()
criterion = torch.nn.BCELoss(reduction='mean')
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# 此处设置if __name__ == '__main__'，是为了防止进程并发时发生 运行时异常 错误
if __name__ == '__main__':
    # 设置epoch为100
    for epoch in range(100):
        # 这里采用enumerate获取iteration次数i
        # 此处，train_loader将X和Y数据进行封装，形成各个Batch下的数据矩阵（X和Y分别形成矩阵）
        for i, data in enumerate(train_loader, 1):
            inputs, labels = data
            y_pred = model(inputs)
            loss = criterion(y_pred, labels)
            print(epoch, i, loss.item())
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

```

```python
import numpy as np
import torch
import torch.nn as nn
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from torch.utils.data import Dataset, DataLoader

# 使用 sklearn 内置数据
data = load_diabetes()
x = data.data
y = data.target.reshape(-1, 1)

# 划分训练集和测试集
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42
)


# 自定义 Dataset
class MyDataset(Dataset):
    def __init__(self, x, y):
        self.x = torch.from_numpy(x).float()
        self.y = torch.from_numpy(y).float()

    def __getitem__(self, index):
        return self.x[index], self.y[index]

    def __len__(self):
        return len(self.x)


train_loader = DataLoader(dataset=MyDataset(x_train, y_train), batch_size=32, shuffle=True, num_workers=0)
val_loader = DataLoader(dataset=MyDataset(x_test, y_test), batch_size=32, shuffle=False, num_workers=0)


# 定义模型
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(10, 8),
            nn.ReLU(),
            nn.Linear(8, 4),
            nn.ReLU(),
            nn.Linear(4, 2),
            nn.ReLU(),
            nn.Linear(2, 1)
        )

    def forward(self, x):
        return self.network(x)


# 初始化
model = Net()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.MSELoss()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)

# 训练
epochs = 200
for epoch in range(epochs):
    model.train()
    for i, (inputs, labels) in enumerate(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        output = model(inputs)
        loss = loss_fn(output, labels)
        loss.backward()
        optimizer.step()

    if (epoch + 1) % 50 == 0:
        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}")

# 测试
model.eval()
all_preds = []
all_trues = []

with torch.no_grad():
    for inputs, labels in val_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)

        all_preds.append(outputs.cpu().numpy())
        all_trues.append(labels.cpu().numpy())

# 合并所有 batch 的结果
pred = np.concatenate(all_preds).flatten()
true = np.concatenate(all_trues).flatten()

# 计算指标
mse = mean_squared_error(true, pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(true, pred)
r2 = r2_score(true, pred)

print("\n" + "=" * 30)
print("最终测试集性能评估：")
print("=" * 30)
print(f"MSE:  {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAE:  {mae:.4f}")
print(f"R²:   {r2:.4f}")
print("=" * 30)

```

<img src="笔记图保存\6d409f6ee94cddcfec9afeb85f5ee46e.png" alt="6d409f6ee94cddcfec9afeb85f5ee46e" style="zoom:33%;" />

总结流程顺序：1.准备数据集 2.设置模型 3.构造损失和优化器 4.训练周期

# 多分类问题

第1种方式：将多分类看作多个二分类问题，将某种类别标签记作1，其它类别标签记作0，针对每种类别构造并训练分类器。

“一对多”方法的最大缺点是它将每个类别独立看待，忽视了类别之间的相似性、关系和层次结构。它通过多个独立的二分类器来处理多分类问题，但这可能导致以下问题：

1. 类别之间的相似性被忽视。
2. 决策边界可能过于复杂，不自然。
3. 类别之间的决策可能不一致。
4. 无法捕捉类别的层次结构。
5. 无法处理“新类别”的问题。

所希望的针对各类别的输出：

1.输出均大于零，

2.输出的和等于1，

解决方案：神经网络保持先前的sigmod层不变，输出层使用softmax。

这里，直接控制输出的预测值y_pre维数为样本类别数：

例如，设样本类别为k（k分类问题），输入x为n维（即特征维数），则进行变换的权重矩阵w的形状（n，k）。

<img src="笔记图保存\4285a03e0b7f7cf41970e730c3e4d3c8.png" alt="4285a03e0b7f7cf41970e730c3e4d3c8" style="zoom: 33%;" />

<img src="笔记图保存\PixPin_2025-08-18_16-58-14.png" alt="PixPin_2025-08-18_16-58-14" style="zoom: 33%;" />

对数似然损失：

将标签构造成为One-hot编码形式，例如有5种类别，该样本真实样本标签属于第4类，则对应标签表示为：[0，0，0，1，0]

将Softmax层输出的预测值结果（例如：[0.3，0.2，0.1，0.3，0.1]）和该列表（[0，0，0，1，0]）计算对数似然损失。因为计算过程中的真实标签有很多0项所以相乘为零，只看真实项（此处实际只计算第4类）。

为什么会设计这样的函数：目标是希望第4类预测值结果输出地尽可能接近1，当Y=1时，Y_pre越接近1，整体损失越接近0；Y_pre越接近0，损失越接近无穷大。

<img src="笔记图保存\904a9232babf7377f47b88a0c32de121.png" alt="904a9232babf7377f47b88a0c32de121" style="zoom:33%;" />

交叉熵：Softmax + Negative Log Likelihood Loss

<img src="笔记图保存\981e6b664c88e509a8a0fe35f254f44f.png" alt="981e6b664c88e509a8a0fe35f254f44f" style="zoom:33%;" />

对数据损失计算过程进行测试验证：

```python
import torch

'''
对数据损失计算过程进行测试验证
'''

# 定义损失函数，此处设置reduction='none'，返回每个样本的损失值而不是批量计算均值
criterion = torch.nn.CrossEntropyLoss(reduction='none')

# 设置真实样本类别
Y = torch.LongTensor([2, 0, 1, 0])

# 这里有4个样本，3个类别，在输入softmax层之前已控制每个样本的输出维度为3。
# 针对4个样本，依次大概率预测：2,0,1,0
Y_pred1 = torch.Tensor([[0.1, 0.2, 0.9], 
                        [1.1, 0.1, 0.2],
                        [0.2, 2.1, 0.1],
                        [1.2, 0.1, 0.3]])
# 大概率预测：0,2,2,1
Y_pred2 = torch.Tensor([[0.8, 0.2, 0.3],
                        [0.2, 0.3, 0.5],
                        [0.2, 0.2, 0.5],
                        [0.1, 0.7, 0.4]])

l1 = criterion(Y_pred1, Y)
l2 = criterion(Y_pred2, Y)
print("Loss1 = ", l1.data, "\nLoss2 = ", l2.data)

```

每张图像都可以看作矩阵形式，使用8位（1字节）来表示每个像素的灰度值，8位可以表示的整数范围是0到255（2^8=256）。

将（0-255）进行标准化，则可以构成数据集矩阵。例如下图是（1，28，28）（C(channel)，W(width)，H(height)）。

channel指颜色通道，RGB通道数为3，此处黑白则为1。

<img src="笔记图保存\dc115f30-be5c-4c96-a370-dbbc2695a18d.png" alt="dc115f30-be5c-4c96-a370-dbbc2695a18d" style="zoom:33%;" />

```python
import torch
from torchvision import transforms
from torchvision import datasets
from torch.utils.data import DataLoader
import torch.nn.functional as F

# 设置批量处理的样本数=64
batch_size = 64
# 构造对图像进行处理的转换器transform，对每个元素进行进行pipline处理：
# 首先将每张图片转换为（C，W，H）的Tensor形式，transforms.ToTensor() 会把图像像素从 [0, 255] 缩放到 [0.0, 1.0]
# 然后将每个Tensor进行标准化缩放，此处缩放至均值0.1307，标准差0.3081的正态分布（MNIST数据集图像的均值和标准差）。
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307,), (0.3081,))
                                ])

# 加载训练和测试数据集，并构造Dataset和DataLoader
train_dataset = datasets.MNIST(root='../dataset/mnist/',
                               train=True,
                               download=True,
                               transform=transform)
train_loader = DataLoader(train_dataset,
                          shuffle=True,
                          batch_size=batch_size, num_workers=0)
test_dataset = datasets.MNIST(root='../dataset/mnist/',
                              train=False,
                              download=True,
                              transform=transform)
test_loader = DataLoader(test_dataset,
                         shuffle=False,
                         batch_size=batch_size, num_workers=0)


class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 这里为何输入是784？因为将28*28的矩阵按行展开后再拼接，构造了一个（1，784）的向量。
        # 全连接网络通常要求输入是一维向量。
        # 因此，如果将一个 28x28 的图像输入到一个全连接网络（例如MLP）时，
        # 通常需要将其展平（flatten）为一个一维向量。
        # 这是因为全连接层将每个输入像素与一个神经元连接，不能直接处理多维结构。
        # 此处以全连接网络为例
        self.l1 = torch.nn.Linear(784, 512)
        self.l2 = torch.nn.Linear(512, 256)
        self.l3 = torch.nn.Linear(256, 128)
        self.l4 = torch.nn.Linear(128, 64)
        # 为何输出10？因为MNIST数据集的图像有10种类别（0-9）
        self.l5 = torch.nn.Linear(64, 10)

    def forward(self, x):
        # 通过view()函数改变Tensor形状。-1代表设置列数后，行数将进行自动计算。
        x = x.view(-1, 784)
        x = F.relu(self.l1(x))
        x = F.relu(self.l2(x))
        x = F.relu(self.l3(x))
        x = F.relu(self.l4(x))
        # 返回时，最后一层无需再做Relu激活，因为交叉熵函数自身将使用Softmax进行激活，并计算对数似然损失
        return self.l5(x)


# 设置模型在GPU上运行
# 2步：1将模型迁移至GPU；2将数据迁移至GPU
model = Net()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)

# 设置损失函数和优化器
# 定义交叉熵损失函数
criterion = torch.nn.CrossEntropyLoss(reduction='mean')
# momentum=0.5：动量方法通过考虑过去梯度的累积来调整当前梯度更新的方向和幅度。缩短训练时间。
# 通常在0到1之间，越接近1，历史梯度的影响越大。
# 简单来说，动量方法在每一步的梯度更新中不仅考虑当前的梯度，还会加上一部分之前更新方向的“记忆”，
# 从而在优化过程中起到“惯性”作用。
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)


# 分别将每个epoch的训练和测试过程封装为函数
def train(epoch):
    # 定义累计损失
    running_loss = 0.0
    for batch_idx, data in enumerate(train_loader, 0):

        inputs, target = data
        inputs, target = inputs.to(device), target.to(device)
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()

        # 每300轮iteration输出1次平均损失（对累计损失求300次的平均），而不是等待epoch结束
        running_loss += loss.item()
        if batch_idx % 300 == 299:
            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))
            running_loss = 0.0


# 定义测试过程的函数
def test():
    # 定义正确数量和总数量
    correct = 0
    total = 0
    # 设置不需要计算梯度，因为此处没有任何训练过程，只验证模型结果的准确性
    with torch.no_grad():
        # 多个Batch，这里的循环次数为：测试集样本总数/Mini-Batch(64)
        for data in test_loader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            # 针对单个Batch的样本，每次输出的是64个10维向量，10维表示其对10个类别的预测概率。
            # 目标是在对应类别的向量位置上，数据尽可能地相比其它类别大。
            # 注意此处还未经过Softmax层和对数似然损失，这里是Relu层输出，目标就是在计算损失前，让该层输出结果尽可能大。
            # （Softmax层不会改变原始数据的大小分布情况）
            outputs = model(images)
            # 针对64个样本按行拼接构成的矩阵，每次按列找出最大值及其对应索引。
            # 其中detach()表示获取数值。dim=1表示按列进行操作并计算最大值。返回=（最大值，最大值索引）。
            _, predicted = torch.max(outputs.detach(), dim=1)
            # 计算真实标签矩阵形状的第1个维度（这里是64）并进行累加
            total += labels.size(0)
            # 计算每64个样本里分类正确的样本数，并将其累加
            correct += (predicted == labels).sum().item()
    # 计算分类的准确率：即分类正确的样本数/总样本数
    print('Accuracy on test set: %d %%' % (100 * correct / total))


if __name__ == '__main__':
    # 在每个epoch中，每次训练完就进行测试，展示模型性能变化
    for epoch in range(10):
        train(epoch)
        test()

```

```python
# 生成特定Batch、特定维度、特定数量的训练和测试集，以对模型测试
import torch

num_batches = 5
batch_size = 32
train_loader = []
test_loader = []

for i in range(num_batches):
    data = torch.randn(batch_size, 1, 28, 28)
    target = torch.randint(0, 10, (batch_size,))
    train_loader.append((data, target))


for i in range(num_batches):
    data = torch.randn(batch_size, 1, 28, 28)
    target = torch.randint(0, 10, (batch_size,))
    test_loader.append((data, target))
```

模型训练一般流程：

1. **划分出测试集（Hold-out Test Set）** → 保留到最后，**绝对不参与任何训练/调参过程**。

2. **剩余数据用于“训练+验证”** → 这部分可以使用交叉验证。

3. **在“训练+验证”部分上做 K 折交叉验证** → 用于选择超参数、比较模型等。

4. **选定最佳模型和超参数后，用整个“训练+验证”数据重新训练最终模型**。

5. **最后，在测试集上评估一次最终模型性能。**

   ✅ 交叉验证只在“训练+验证”部分进行。
   ✅ 测试集**永远不参与交叉验证**。
   ✅ 交叉验证的目的是**更可靠地估计模型在未知数据上的表现（用于调参）**，而不是最终评估。 

# 卷积神经网络

相比先前全连接神经网络中，直接将图像矩阵展开并构造为一维向量形式。卷积神经网络选择保留图像空间信息，并将输入设置为原始图像矩阵，且首先需确定输入数据通道信息和输出的通道信息，步骤： 

1 输入

2 特征提取（做卷积变换）

3 分类（将卷积最后层按照一维展开，并输入全连接层，再通过softmax层映射，对数似然损失）

<img src="笔记图保存\2f8e3e96-40ee-4d81-8b16-01c31c8d4149.png" alt="2f8e3e96-40ee-4d81-8b16-01c31c8d4149" style="zoom:33%;" />

卷积的过程：对每一小块图像进行遍历，遍历时对小块图像进行卷积操作。

<img src="笔记图保存\4c5a652c-c14b-42f0-a0f2-0f90c026461c.png" alt="4c5a652c-c14b-42f0-a0f2-0f90c026461c" style="zoom:33%;" />

单通道卷积操作：按卷积核对矩阵元素进行数乘（注意不是矩阵乘法），遍历整个图像，将结果数值放入输出矩阵。

共享权重：同一个卷积核（filter 或 kernel）在整个输入数据上滑动时，使用相同的权重参数进行计算。

<img src="笔记图保存\b22b8342-4a10-4dd2-af06-6e413729a2a7.png" alt="b22b8342-4a10-4dd2-af06-6e413729a2a7" style="zoom:33%;" />

多通道卷积操作：分别使用3个卷积核对每个通道进行卷积操作，并将获取的输出矩阵求和。

<img src="笔记图保存\80ee78aa-1a13-4ace-b925-c741dcf829e3.png" alt="80ee78aa-1a13-4ace-b925-c741dcf829e3" style="zoom:33%;" />

输入为n个通道时，采用的卷积核通道数也应为n，并输出通道数为1的卷积结果。

<img src="笔记图保存\efbfeb98-8d2f-47b7-a39d-74a09405c673.png" alt="efbfeb98-8d2f-47b7-a39d-74a09405c673" style="zoom:33%;" />

当进一步将结果输出通道数设定为m，则将以上处理的n通道卷积组数量增多至m个，进行m次运算后将卷积结果堆叠可形成m个通道：

<img src="笔记图保存\0500c0a3-9d2e-40fb-8db9-ceee67de90a5.png" alt="0500c0a3-9d2e-40fb-8db9-ceee67de90a5" style="zoom:33%;" />

当输入通道数为n，输出通道数为m时，将卷积层设定为4维张量（m，n，w，h）：

类似先前线性回归时的矩阵乘法，通过输入和输出的通道维度，可确定权重矩阵的通道维度。

<img src="笔记图保存\f8d04610-b055-4d6c-a38d-6da561a4d79b.png" alt="f8d04610-b055-4d6c-a38d-6da561a4d79b" style="zoom:33%;" />

演示计算的进行过程：

```python
import torch

in_channels, out_channels = 5, 10
width, height = 100, 100
kernel_size = (3, 3)
batch_size = 1

# 随机生成一个批量大小为1，形状（5，100，100）（C，W，H）的输入张量
input = torch.randn(batch_size,
                    in_channels,
                    width,
                    height)

# 设置卷积层，必须的参数：输入通道、输出通道、卷积核形状
# 卷积层与图像输入的W和H没有关系
conv_layer = torch.nn.Conv2d(in_channels,
                             out_channels,
                             kernel_size=kernel_size)

output = conv_layer(input)

print(input.shape)
# 此处输出的W和H为什么是98：
# 因为卷积核是3，减去中心=2列，即在输入张量中，移动至极限时的左边距和右边距相加等于2，
# 使用100-2=98=卷积核在输入张量中部的可移动距离。
# 只要确定了输入张量的W和H，输出张量的W和H可自动确定。
print(output.shape)
print(conv_layer.weight.shape)

```

原始输入矩阵为5×5，卷积核3×3，因此结果=3×3（5-3+1），但如果想让输出结果=5×5（即保持输出尺寸的W、H=输入尺寸的W、H）？

对原始矩阵填充1圈均为0的padding，使结果输出矩阵变为5×5。

填充圈数：

x=填充后输入维度，input=i（填充前输入维度）, kernel=k（卷积核维度）, out=o（填充后输出维度）,

x-(k-1)=o, 则x=o-1+k,

圈数=(填充后输入维度-填充前输入维度)/2 = (x-i)/2 = (o-1+k-i)/2

因为这里o=i（填充后输出维度=填充前输入维度）, 所以圈数= **(k-1)/2** =3-1/2=1

<img src="笔记图保存\04e7569e-59bb-4458-a09a-4086aa5f51b2.png" alt="04e7569e-59bb-4458-a09a-4086aa5f51b2" style="zoom:33%;" />

填充padding的代码实现：

```python
import torch

in_channels, out_channels = 1, 1

# 手动设置输入的数据
input = [3, 4, 6, 5, 7,
         2, 4, 6, 8, 2,
         1, 6, 7, 8, 4,
         9, 7, 4, 6, 2,
         3, 7, 5, 4, 1]

input = torch.Tensor(input).view(1, in_channels, 5, 5)

# 和先前的不同在于对卷积层设置padding=1
conv_layer = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels,
                             kernel_size=(3, 3), padding=1, bias=False)

# 手动设置卷积核数据
# 注意此处，先out_channels, 再in_channels（不像先前Conv2d中，固定先设置输入通道数，再输出通道数）
kernel = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(out_channels, in_channels, 3, 3)
# kernel.data是 kernel 的一个属性，
# 它返回一个与 kernel 共享相同数据的张量（tensor），但不会跟踪计算历史（即不会参与自动求导）。
# 其存在是为了在不影响计算图的情况下访问张量的数据。它通常用于在不需要梯度的情况下操作张量。
conv_layer.weight.data = kernel.data

output = conv_layer(input)
print(output)

```

设置stride：即设置卷积核移动的步长

填充圈数：

x=填充后输入维度，input=i（填充前输入维度）, kernel=k（卷积核维度）, out=o（填充后输出维度）, 移动步长为s

卷积核中部可移动距离 = x-(k-1) = move

o = 1+(move-1)/s = 1+[x-(k-1)-1]/s = **1+(x-k)/s**, （此处可根据该公式，将x看作输入并计算获取输出维度）则 x = s(o−1)+k

圈数=(填充后输入维度-填充前输入维度)/2 = (x-i)/2 = [s(o−1)+k-i]/2

因为这里o=i（填充后输出维度=填充前输入维度）, 所以圈数= **(si−s-i+k)/2**

整体公式：

<img src="笔记图保存\PixPin_2025-08-20_11-47-50.png" alt="PixPin_2025-08-20_11-47-50" style="zoom:50%;" />

<img src="笔记图保存\7ec22b15-e1f8-45f1-8659-00b3c849c661.png" alt="7ec22b15-e1f8-45f1-8659-00b3c849c661" style="zoom:33%;" />

设置stride的代码实现：

```python
conv_layer = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, 
                             stride=2, # 只需修改卷积层中设置参数即可
                             kernel_size=(3, 3), bias=False)
```

Max Pooling最大池化：2×2的Max Pooling先将输入矩阵划分为2*2的子块集合，在每个划分的子块集里寻找最大值后进行组合（这里默认步长为2）。

因此，Max Pooling不会改变通道数，只会改变图像大小（经过Max Pooling后的图像大小将变为原先的1/2）

<img src="笔记图保存\253b7ad9-4fe8-4212-a8e3-9338820b80bb.png" alt="253b7ad9-4fe8-4212-a8e3-9338820b80bb" style="zoom:33%;" />

Max Pooling层代码实现：

```python
input = [3, 4, 6, 5,
         2, 4, 6, 8,
         1, 6, 7, 8,
         9, 7, 4, 6]
input = torch.Tensor(input).view(1, 1, 4, 4)

# 设置Max Pooling层
maxpooling_layer = torch.nn.MaxPool2d(kernel_size=2)
 
output = maxpooling_layer(input)
```

接下来，使用以下架构为例的CNN进行实现：

<img src="笔记图保存\836ba80e-c19b-4c05-9355-f709284c2e94.png" alt="836ba80e-c19b-4c05-9355-f709284c2e94" style="zoom:33%;" />

<img src="笔记图保存\a0fe1a32-af3d-46fd-ad0c-0f1dac5f4fe3.png" alt="a0fe1a32-af3d-46fd-ad0c-0f1dac5f4fe3" style="zoom:33%;" />

基础神经网络实现代码（这里直接在先前 多分类全连接神经网络的基础上，修改模型定义部分即可）：

```python
import torch
from torchvision import transforms
from torchvision import datasets
from torch.utils.data import DataLoader
import torch.nn.functional as F

batch_size = 64
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307,), (0.3081,))
                                ])

train_dataset = datasets.MNIST(root='../dataset/mnist/',
                               train=True,
                               download=True,
                               transform=transform)
train_loader = DataLoader(train_dataset,
                          shuffle=True,
                          batch_size=batch_size, num_workers=0)
test_dataset = datasets.MNIST(root='../dataset/mnist/',
                              train=False,
                              download=True,
                              transform=transform)
test_loader = DataLoader(test_dataset,
                         shuffle=False,
                         batch_size=batch_size, num_workers=0)


class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 修改此处，设置卷积层、池化层、全连接层
        # torch.nn.Conv2d 不需要传入 batch_size，原因在于：卷积操作是逐样本（per-sample）独立进行的，不依赖于批量大小（batch size）。
        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=(5, 5))
        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=(5, 5))
        self.pooling = torch.nn.MaxPool2d(2)
        self.fc = torch.nn.Linear(320, 10)

    def forward(self, x):
        batch_sizes = x.size(0)
        # 这里由于卷积操作是线性的，通过激活函数引入非线性操作
        # 先池化降低W和H，再Relu，减少计算复杂度和运算量
        x = F.relu(self.pooling(self.conv1(x)))
        x = F.relu(self.pooling(self.conv2(x)))
        # 将输出数据（1个batch-size）展平为一维后输入全连接层：64×20×4×4 -> 64×320
        x = x.view(batch_sizes, -1)
        # 此处无需激活，后面函数中包含了Softmax操作
        x = self.fc(x)
        return x


model = Net()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)

criterion = torch.nn.CrossEntropyLoss(reduction='mean')
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)


def train(epoch):
    running_loss = 0.0
    for batch_idx, data in enumerate(train_loader, 0):

        inputs, target = data
        inputs, target = inputs.to(device), target.to(device)
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if batch_idx % 300 == 299:
            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))
            running_loss = 0.0


def test():
    correct = 0
    total = 0

    with torch.no_grad():
        for data in test_loader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)

            _, predicted = torch.max(outputs.detach(), dim=1)

            total += labels.size(0)

            correct += (predicted == labels).sum().item()

    print('Accuracy on test set: %d %%' % (100 * correct / total))


if __name__ == '__main__':

    for epoch in range(10):
        train(epoch)
        test()

```

# 卷积神经网络（高级）

之前所学习的多层感知机（即全连接神经网络）、卷积神经网络，在网络架构上都是穿行结构，此处介绍几类更复杂架构。

## GoogleNet

可发现其中存在大量重复卷积模块，称为Inception。

<img src="笔记图保存\6c5281e5-3f8a-4eee-bbc9-25ee2f84bb7b.png" alt="6c5281e5-3f8a-4eee-bbc9-25ee2f84bb7b" style="zoom:33%;" />

<img src="笔记图保存\585e561f-9b37-4520-845c-1cd6ab1d14d3.png" alt="585e561f-9b37-4520-845c-1cd6ab1d14d3" style="zoom:33%;" />

Inception模块中提供了多类卷积核组合预选，通过训练计算权重后，将自动选择其中较优的组合方式。

其中Concatenate：将卷积结果按照通道的方向进行拼接（因此要求W和H必须相同，而C可以不同）。

Average Pooling：均值池化（Max Pool是取区域内最大保留，Average Pooling则是对区域求均值并保留）。

其中，针对3×3、5×5卷积核下的卷积操作，做padding即可保证图像W和H不变。针对AveragePooling也可做padding实现类似效果（类似卷积操作，但并没有卷积核进行运算，而是求卷积核范围内的均值）。

1×1 卷积的主要目标是调整特征图的通道数（同时还有“信息融合”的作用，将多通道信息汇聚在一起），而不改变W和H。

对于（C，W，H）的图像，进行1×1卷积后均变为（1，W，H），如需要输出通道数为m，则设置卷积层为m个即可（m，C，1，1）。

1×1 卷积可以起到类似降维的作用，相较于直接通过卷积操作改变通道C数量，基于1×1卷积能大幅降低运算量。

针对Inception模块的实现代码如下：

<img src="笔记图保存\edc59757-2002-40b7-b6d0-d593a7a1d9a5.png" alt="edc59757-2002-40b7-b6d0-d593a7a1d9a5" style="zoom: 33%;" />

```python
import torch

from torchvision import transforms
from torchvision import datasets
from torch.utils.data import DataLoader

import torch.nn.functional as F
import torch.optim as optim
import torch.nn as nn

batch_size = 64
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307,), (0.3081,))
                                ])

train_dataset = datasets.MNIST(root='../dataset/mnist',
                               train=True,
                               download=True,
                               transform=transform)
train_loader = DataLoader(train_dataset,
                          shuffle=True,
                          batch_size=batch_size)

test_dataset = datasets.MNIST(root='../dataset/mnist',
                              train=False,
                              download=True,
                              transform=transform)
test_loader = DataLoader(test_dataset,
                         shuffle=False,
                         batch_size=batch_size)


# 定义Inception模型
class InceptionA(nn.Module):
    def __init__(self, in_channels):
        super(InceptionA, self).__init__()
        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)

        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)
        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)

        self.branch3x3_1 = nn.Conv2d(in_channels, 16, kernel_size=1)
        self.branch3x3_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)
        self.branch3x3_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)

        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch5x5 = self.branch5x5_1(x)
        branch5x5 = self.branch5x5_2(branch5x5)

        branch3x3 = self.branch3x3_1(x)
        branch3x3 = self.branch3x3_2(branch3x3)
        branch3x3 = self.branch3x3_3(branch3x3)

        # 均值池化，有函数方法可直接调用
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)

        # 连接4个branch的输出张量，dim=1表示按照通道拼接
        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]
        return torch.cat(outputs, dim=1)


# 定义GoogleNet模型，其中包含Inception模块
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)

        self.incep1 = InceptionA(in_channels=10)
        self.incep2 = InceptionA(in_channels=20)

        self.mp = nn.MaxPool2d(2)
        # 此处，1408=[[28-(5-1)]/2 - (5-1)]/2 * (16 + 24*3)
        self.fc = nn.Linear(1408, 10)

    def forward(self, x):
        in_size = x.size(0)
        x = F.relu(self.mp(self.conv1(x)))
        x = self.incep1(x)
        # 在这步操作之后，经过inception模块的张量通道数均为24*3+16=88，所以先前设置conv2的输入通道数为88
        x = F.relu(self.mp(self.conv2(x)))
        x = self.incep2(x)
        x = x.view(in_size, -1)
        x = self.fc(x)

        return x


model = Net()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)


def train(epoch):
    running_loss = 0.0
    for batch_idx, data in enumerate(train_loader, 0):
        inputs, target = data
        inputs, target = inputs.to(device), target.to(device)
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if batch_idx % 300 == 299:
            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 2000))
            running_loss = 0.0


def test():
    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_loader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, dim=1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print('Accuracy on test set: %d %%' % (100 * correct / total))


if __name__ == '__main__':
    for epoch in range(10):
        train(epoch)
        test()

```

当全连接层的输入维度难以确定时，可尝试使用一个张量作为输入，并直接输出结果形状，不用自己手动计算：

```python
import torch
import torch.nn.functional as F
import torch.nn as nn


class InceptionA(nn.Module):
    def __init__(self, in_channels):
        super(InceptionA, self).__init__()
        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)

        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)
        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)

        self.branch3x3_1 = nn.Conv2d(in_channels, 16, kernel_size=1)
        self.branch3x3_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)
        self.branch3x3_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)

        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch5x5 = self.branch5x5_1(x)
        branch5x5 = self.branch5x5_2(branch5x5)

        branch3x3 = self.branch3x3_1(x)
        branch3x3 = self.branch3x3_2(branch3x3)
        branch3x3 = self.branch3x3_3(branch3x3)

        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]
        return torch.cat(outputs, dim=1)


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)

        self.incep1 = InceptionA(in_channels=10)
        self.incep2 = InceptionA(in_channels=20)

        self.mp = nn.MaxPool2d(2)

    def forward(self, x):
        in_size = x.size(0)
        x = F.relu(self.mp(self.conv1(x)))
        x = self.incep1(x)
        x = F.relu(self.mp(self.conv2(x)))
        x = self.incep2(x)
        x = x.view(in_size, -1)

        return x


net = Net()
print(net(torch.randn(64, 1, 28, 28)).data.shape[1])

```

## Residual Network (ResNet)

梯度消失问题：在链式法则中，当每次乘以的梯度都小于1，梯度将会越来越小，最终W将无法有效更新。（如果每次相乘的梯度都大于1则称为梯度爆炸问题，梯度过大将导致模型无法有效收敛）

老方法：划分输入层、多个隐藏层和输出层。逐层训练隐藏层，在对应隐藏层训练完毕后将其锁住，继续训练下一层，以解决梯度消失问题。该方法难以实现，因为神经网络中层数过多。

引入残差连接：输入x经过2个卷积层后得到F（x），先不激活，将F（x）与x相加后再通过Relu激活。

F（x）与x做加法，意味着其C，W，H均要一致。

<img src="笔记图保存\d3b88e18-dae9-4d86-ae5f-0fa79f7d23c5.png" alt="d3b88e18-dae9-4d86-ae5f-0fa79f7d23c5" style="zoom:33%;" />

使用以下网络结构为例进行ResNet实现：

<img src="笔记图保存\eb0081f6-7d11-484b-9367-817bfe2ef048.png" alt="eb0081f6-7d11-484b-9367-817bfe2ef048" style="zoom:33%;" />

<img src="笔记图保存\1675bb24-b189-41b7-9802-5c6e36ecb1cf.png" alt="1675bb24-b189-41b7-9802-5c6e36ecb1cf" style="zoom:33%;" />

```python
import torch

from torchvision import transforms
from torchvision import datasets
from torch.utils.data import DataLoader

import torch.nn.functional as F
import torch.optim as optim
import torch.nn as nn

batch_size = 64
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307,), (0.3081,))
                                ])

train_dataset = datasets.MNIST(root='../dataset/mnist',
                               train=True,
                               download=True,
                               transform=transform)
train_loader = DataLoader(train_dataset,
                          shuffle=True,
                          batch_size=batch_size)

test_dataset = datasets.MNIST(root='../dataset/mnist',
                              train=False,
                              download=True,
                              transform=transform)
test_loader = DataLoader(test_dataset,
                         shuffle=False,
                         batch_size=batch_size)


# 此处定义残差块
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        # 定义输入的通道数
        self.channels = channels
        # 这里为保证最后x能够与F（x）相加，需保证通道数不变，故设置卷积层的输入输出不变。
        # 同时设置padding=1保证W和H不变
        self.conv1 = nn.Conv2d(channels, channels,
                               kernel_size=(3, 3), padding=1)
        self.conv2 = nn.Conv2d(channels, channels,
                               kernel_size=(3, 3), padding=1)

    def forward(self, x):
        y = F.relu(self.conv1(x))
        y = self.conv2(y)
        # 注意此处顺序，先相加，后激活
        return F.relu(x + y)


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=(5, 5))
        self.conv2 = nn.Conv2d(16, 32, kernel_size=(5, 5))
        self.mp = nn.MaxPool2d(2)

        self.rblock1 = ResidualBlock(16)
        self.rblock2 = ResidualBlock(32)

        self.fc = nn.Linear(512, 10)

    def forward(self, x):
        in_size = x.size(0)
        x = self.mp(F.relu(self.conv1(x)))
        x = self.rblock1(x)
        x = self.mp(F.relu(self.conv2(x)))
        x = self.rblock2(x)
        x = x.view(in_size, -1)
        x = self.fc(x)

        return x


model = Net()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)


def train(epoch):
    running_loss = 0.0
    for batch_idx, data in enumerate(train_loader, 0):
        inputs, target = data
        inputs, target = inputs.to(device), target.to(device)
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if batch_idx % 300 == 299:
            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 2000))
            running_loss = 0.0


def test():
    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_loader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, dim=1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print('Accuracy on test set: %d %%' % (100 * correct / total))


if __name__ == '__main__':
    for epoch in range(10):
        train(epoch)
        test()
```

# 循环神经网络

RNN思想：针对带有时序特征的数据，使用权重共享（例如CNN中，对一张图像使用相同的卷积核进行滑动，因此权重数量少）的概念，减少权重运算的计算量。考虑数据序列前后具有依赖关系。

## RNNCell

在RNN Cell中进行循环计算，每次接收上次计算的输出和本次输入进行运算。

<img src="笔记图保存\e4957b6e-00ce-4d77-8a61-4a935bf18760.png" alt="e4957b6e-00ce-4d77-8a61-4a935bf18760" style="zoom:33%;" />

1.输入数据xt维度是i，输出隐藏层ht维度是h，则wih表示线性变换矩阵是i*h

2.输入上个隐层ht-1的维度是h，输出隐藏层ht维度是h，则whh表示线性变换矩阵是h*h

3.先将1和2中进行线性变换的结果相加，再通过tanh进行非线性激活（取值-1至+1），输出隐层ht，并作为下次循环运算的输入

4.此处实际整体上只进行了一次线性层运算：

<img src="笔记图保存\5152085a-dfbc-412e-a30a-15dafc2481b1.png" alt="5152085a-dfbc-412e-a30a-15dafc2481b1" style="zoom: 50%;" />

<img src="笔记图保存\59d88b0c-3b9a-462a-ac53-6a09d03dab4a.png" alt="59d88b0c-3b9a-462a-ac53-6a09d03dab4a" style="zoom:33%;" />

在Pytorch中，实现RNN有2种方式（按照输入分别设置为数据列表中单个向量/全部向量）：

1.定义RNN Cell，并自己实现循环

```python
import torch

batch_size = 1
seq_len = 3
input_size = 4
hidden_size = 2

# 定义RNNCell，需要的定义参数有输入维度和隐藏层维度
cell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)

# 设置数据集：序列长度，批量大小，输入维度
dataset = torch.randn(seq_len, batch_size, input_size)
# 设置隐藏层（初始设置为0）：批量大小，隐藏层维度
hidden = torch.zeros(batch_size, hidden_size)
print(dataset)

# 遍历数据集序列，将每次的序列数据和上次输出作为当前输入
for idx, input in enumerate(dataset):
    print('=' * 20, idx, '=' * 20)
    # 将当前序列数据、隐藏层输入RNNCell中计算，获取hidden作为输出（也是下次的输入）
    hidden = cell(input, hidden)
    print('outputs size: ', hidden.shape)
    print(hidden)

```

由观察可知，RNNCell只要设置输入输出的维度即可，**无需设置Batch**。而输入数据和输出数据则均需要设置维度和批量大小（输入还需设置序列长度）。总结：

**RNNCell：Inputsize、Hiddensize**

**Input：Seqlen、Batchsize、Inputsize**

**Out：Batchsize、Hiddensize**

## RNN

2.直接使用RNN，不用自己实现循环。

其中，输入设置为整个序列集合，输出包括隐藏层序列集合(out)+最终隐藏层输出结果(hidden)，且可设置RNN层数。

<img src="笔记图保存\cf875432-c9bb-4dd9-98e6-859b7572babc.png" alt="cf875432-c9bb-4dd9-98e6-859b7572babc" style="zoom:33%;" />

本质上，多层RNN在单个RNN的基础上，将当前时刻输入修改为上层输出。这样RNN便完成了空间上的数据变换。

<img src="笔记图保存\1d47b07a-8478-4035-b374-5b264264d637.png" alt="1d47b07a-8478-4035-b374-5b264264d637" style="zoom:33%;" />

```python
import torch

batch_size = 1
seq_len = 3
input_size = 4
hidden_size = 2
num_layers = 1

# 直接定义RNN，参数：输入维度、隐藏层维度、RNN层数
cell = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size,
                    num_layers=num_layers)

# 定义输入，参数：序列长度、批量大小、输入维度
inputs = torch.randn(seq_len, batch_size, input_size)
# 定义初始隐藏层，参数：RNN层数、批量大小、隐藏层维度
hidden = torch.zeros(num_layers, batch_size, hidden_size)

# 获取结果：最后一层Cell的输出结果集合（上），最终时序于不同层下的输出结果（右）
# 这里，out的最后一个输出应该等于hidden的最后一个输出
out, hidden = cell(inputs, hidden)
print('Output size: ', out.shape)
print('Output: ', out)
print('Hidden size: ', hidden.shape)
print('Hidden: ', hidden)
```

RNN可以设置参数batch_first使得BatchSize和序列长度进行位置交换：

<img src="笔记图保存\bd304dfb-a619-4d44-9a7b-3f8d9898ffda.png" alt="bd304dfb-a619-4d44-9a7b-3f8d9898ffda" style="zoom: 33%;" />

```python
import torch

batch_size = 1
seq_len = 3
input_size = 4
hidden_size = 2
num_layers = 1

# 设置交换顺序
cell = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size,
                    num_layers=num_layers, batch_first=True)

# 输入数据：交换顺序
inputs = torch.randn(batch_size, seq_len, input_size)
# 注意这里不要交换顺序
hidden = torch.zeros(num_layers, batch_size, hidden_size)

out, hidden = cell(inputs, hidden)
print('Output size: ', out.shape)
print('Output: ', out)
print('Hidden size: ', hidden.shape)
print('Hidden: ', hidden)
```

总结：

**RNN：Inputsize、Hiddensize、Numlayers**

**Input：Batchsize、Seqlen、Inputsize**

**Out：Numlayers、Batchsize、Hiddensize**

## RNNCell-Seq2Seq

以针对“hello”单词的处理为例，进行序列到序列（Sequence-to-Sequence，常简写为Seq2Seq）的转换任务：

RNN只能处理向量形式，如何将“hello”单词转换为向量序列？

<img src="笔记图保存\84f724e2-7e56-4362-b404-c4c20768e220.png" alt="84f724e2-7e56-4362-b404-c4c20768e220" style="zoom:33%;" />

1.使用RNNCell进行实现

```python
import torch

batch_size = 1
# 因此处构建ont-hot矩阵来表示向量，因此输入维度为4
input_size = 4
hidden_size = 4

# 定义字典
idx2char = ['e', 'h', 'l', 'o']
# 对应输入：hello
x_data = [1, 0, 2, 2, 3]
# 对应输出：0hlol
y_data = [3, 1, 2, 3, 2]

# 构造one_hot索引矩阵
one_hot_lookup = [[1, 0, 0, 0],
                  [0, 1, 0, 0],
                  [0, 0, 1, 0],
                  [0, 0, 0, 1]]
# 定义输入的向量序列，维度：Seqlen, inputsize
x_one_hot = [one_hot_lookup[x] for x in x_data]

inputs = torch.Tensor(x_one_hot).view(-1, batch_size, input_size)
labels = torch.LongTensor(y_data).view(-1, 1)


class Model(torch.nn.Module):
    def __init__(self, input_size, hidden_size, batch_size):
        super(Model, self).__init__()
        # self.num_layers = num_layers
        self.batch_size = batch_size
        self.input_size = input_size
        self.hidden_size = hidden_size
        # 注意这里是RNNCell而不是RNN
        self.rnncell = torch.nn.RNNCell(input_size=self.input_size,
                                        hidden_size=self.hidden_size)

    # 1次前向传播，就是进行1次RNN的input、hidden输入和hidden输出
    def forward(self, input, hidden):
        hidden = self.rnncell(input, hidden)
        return hidden

    # 在init中设置batch_size的目的，就是在此处初始化hidden，实际上这里的初始化操作也可放在函数外部
    def init_hidden(self):
        return torch.zeros(self.batch_size, self.hidden_size)


# 分别定义模型、损失函数、优化器
net = Model(input_size, hidden_size, batch_size)
# 将其看作多分类问题
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=0.1)

for epoch in range(15):
    loss = 0
    optimizer.zero_grad() 
    # 初始化输入的hidden
    hidden = net.init_hidden()
    print('Predicted string: ', end='')
    # 这里实际上就是按seq_len在遍历向量列表
    for input, label in zip(inputs, labels):
        hidden = net(input, hidden)
        # 损失函数？计算hidden和label的对比
        # 在序列建模任务中（如文本生成、时间序列预测），一个样本通常指一个完整的序列。
        #（此处示例中，相当于1次完整循环中只有1个样本“hello”）
        #（可发现，RNN中，单个样本序列元素中每个元素都有label标签，但是整体也算作一个样本）
        #（序列中的每个元素（如每个字符）对应一个时间步。每个时间步的输入和输出可能都有标签，但这些时间步的标签共同构成一个样本的标签序列。）
        # 注意这里，loss是在单个样本（"hello"）中进行累加计算，且没有使用.item()，因为需要构造计算图。
        # 通过下图说明了RNN和CNN在loss值更新中的差异。
        loss += criterion(hidden, label)
        _, idx = hidden.max(dim=1)
        print(idx2char[idx.item()], end='')


    loss.backward()
    optimizer.step()
    print(', Epoch [%d/15] loss = %.4f' % (epoch + 1, loss.item()))

```

<img src="笔记图保存\0c83098f-6e5a-4d7a-a05d-608ee3a4df6e.png" alt="0c83098f-6e5a-4d7a-a05d-608ee3a4df6e" style="zoom: 50%;" />

## RNN-Seq2Seq

2.使用RNN进行实现

```python
import torch

batch_size = 1
# 添加定义了序列长度
seq_len = 5
input_size = 4
hidden_size = 4
# 添加定义了RNN层次
num_layers = 1

idx2char = ['e', 'h', 'l', 'o']
x_data = [1, 0, 2, 2, 3]
y_data = [3, 1, 2, 3, 2]

one_hot_lookup = [[1, 0, 0, 0],
                  [0, 1, 0, 0],
                  [0, 0, 1, 0],
                  [0, 0, 0, 1]]
x_one_hot = [one_hot_lookup[x] for x in x_data]
inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size)
labels = torch.LongTensor(y_data)


class Model(torch.nn.Module):
    def __init__(self, input_size, hidden_size, batch_size, num_layers=1):
        super(Model, self).__init__()
        # 此处就是多定义了一个RNN层次
        self.num_layers = num_layers
        self.batch_size = batch_size
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.rnn = torch.nn.RNN(input_size=self.input_size,
                                hidden_size=self.hidden_size,
                                num_layers=num_layers)

    def forward(self, input):
        # 初始化hidden输入
        hidden = torch.zeros(self.num_layers,
                             self.batch_size,
                             self.hidden_size)
        # RNN针对整个数据序列（就算是整个序列，也属于单个样本）进行处理，可忽视中间对序列的遍历过程
        # 最终输出：最后1层每个Cell的输出集合，最后1个时间步上的各层输出
        # 这里的输出结果：out(seqlen, batchsize, hiddensize)
        out, _ = self.rnn(input, hidden)
        # 返回结果：(seqlen × batchsize, hiddensize)，这里将前2个维度拼接在一起，方便后续计算
        return out.view(-1, self.hidden_size)


net = Model(input_size, hidden_size, batch_size, num_layers)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=0.05)

for epoch in range(15):
    # 这里因为使用的是RNN，发现没有对序列的循环过程
    optimizer.zero_grad()
    # 返回结果：(seqlen × batchsize, hiddensize)
    outputs = net(inputs)
    # 这里labels的维度：(seqlen × batchsize, 1)，直接与(seqlen × batchsize, hiddensize)进行交叉熵计算
    # 注意：CrossEntropyLoss此处 处理的是两个序列向量集合，而不是先前RNNCell中成双的向量，
    # 默认对所有时间步的损失求平均。若需与RNNCell的累加结果一致，需设置reduction='sum'。
    # 一般采用求均值的方式更优。
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    _, idx = outputs.max(dim=1)
    idx = idx.data.numpy()
    print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')
    print(', Epoch [%d/15] loss = %.3f' % (epoch + 1, loss.item()))

```

## Embedding和线性变换

在进行自然语言处理时，独热向量存在哪些缺点？

1.维度过高（例如，词级别进行划分？每个向量都是万级维度，维度诅咒）

2.分布稀疏（万级节点仅映射至坐标轴上一个点）

3.硬编码（一 一对应，1个向量特征的独热向量固定）

能否变为：1.低维度 2.稠密 3.可学习？

为此引入**EMBEDDING**（嵌入层），进行数据降维。

<img src="笔记图保存\f5e5109b-256d-4307-a9f0-0c6c58ebf6ff.png" alt="f5e5109b-256d-4307-a9f0-0c6c58ebf6ff" style="zoom:33%;" />

关键参数：词汇表维度（inputSize）和嵌入维度（embeddingSize）。

根据对应元素在词汇表中位置，索引寻找对应向量即可。

<img src="笔记图保存\f997ae82-0557-4f35-bd1b-bd24f407a38b.png" alt="f997ae82-0557-4f35-bd1b-bd24f407a38b" style="zoom:33%;" />

<img src="笔记图保存\039d3583-c19c-422d-a3f7-5555c62fed7a.png" alt="039d3583-c19c-422d-a3f7-5555c62fed7a" style="zoom:33%;" />

使用时，在序列输入RNN之前，对序列元素（维度是one-hot）分别进行embedding映射（输入需要是长整形）即可。

最后可以接一个线性层，将hiddensize映射至需要分类的维度。

（这里，隐藏层维度hiddensize未必等于需分类维度。有时隐藏层需足够大，以编码中间特征）

这里的线性层，可对长序列中每个元素进行处理（参考先前corssenpty）

<img src="笔记图保存\f68aa152-7027-457a-9bcc-9fd9a83d6df1.png" alt="f68aa152-7027-457a-9bcc-9fd9a83d6df1" style="zoom:33%;" />

案例实现

```python
import torch

# 批量大小设置1
batch_size = 1
# 序列长度设置5（例：样本hello）
seq_len = 5

# 词汇表维度设置4（词汇表索引中的行数）
input_size = 4
# 嵌入维度设置10（词汇表索引中的列数）
embedding_size = 10

# 隐藏层设置8（这里不是分类类别数）
hidden_size = 8
# 2层RNN
num_layers = 2
# 定义分类类别数
num_class = 4

idx2char = ['e', 'h', 'l', 'o']
# 因为之后设置batch_first，输入数据需将batchsize设置为第1个维度
x_data = [[1, 0, 2, 2, 3]]
y_data = [3, 1, 2, 3, 2]
# embedding的输入需要长整形
inputs = torch.LongTensor(x_data)
labels = torch.LongTensor(y_data)


class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        # 定义embedding层，作用：将整数索引映射为稠密向量，注意理解这里不是对输入数据进行映射
        self.emb = torch.nn.Embedding(input_size, embedding_size)
        # 设置RNN，batch_first=True
        self.rnn = torch.nn.RNN(input_size=embedding_size,
                                hidden_size=hidden_size,
                                num_layers=num_layers,
                                batch_first=True)
        # 设置线性层
        self.fc = torch.nn.Linear(hidden_size, num_class)

    def forward(self, x):
        # 设置初始隐层
        hidden = torch.zeros(num_layers, batch_size, hidden_size)
        # 将初始数据x(batch_size,seqlen)输入Embedding(input_size,embedding_size)，
        # 由Embedding转换为x(batch_size,seqlen,embedding_size)
        x = self.emb(x)
        # 将x(batch_size,seqlen,embedding_size)输入RNN，
        # 获取各个时间步上最后层输出(batch_size,seqlen,hidden_size)
        x, _ = self.rnn(x, hidden)
        # 将最后层输出(batch_size,seqlen,hidden_size)基于线性层变换为(batch_size,seqlen,num_class)
        x = self.fc(x)
        # 返回(batch_size×seqlen,num_class)
        return x.view(-1, num_class)


net = Model()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=0.05)

for epoch in range(15):
    optimizer.zero_grad()
    outputs = net(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    _, idx = outputs.max(dim=1)
    idx = idx.data.numpy()

    print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')
    print(', Epoch [%d/15] loss = %.3f' % (epoch + 1, loss.item()))

```

## GRU

<img src="笔记图保存\161eadaf-bf2f-44b8-bb08-1126a9c5daef.png" alt="161eadaf-bf2f-44b8-bb08-1126a9c5daef" style="zoom:33%;" />

问题描述：输入名字，输出对应国家（18种）。这里由于不是Seq2Seq任务，而是分类任务，

所采用的模型可以简化，直接输出最后1个时间步上最后1层RNNCell输出的hidden即可，以下以GRU为例进行实现。

<img src="笔记图保存\8a11da1f-219a-4f4d-a106-827cc7bd5afc.png" alt="8a11da1f-219a-4f4d-a106-827cc7bd5afc" style="zoom:33%;" />

<img src="笔记图保存\ebdd4dcf-34a2-43cc-b275-4fe2866a9634.png" alt="ebdd4dcf-34a2-43cc-b275-4fe2866a9634" style="zoom:33%;" />

数据预处理：如何将原始姓名列表转换为向量列表？

先划分为字符，再转换为ASCLL表（每个字母都可看作128维独热向量）

<img src="笔记图保存\0f23ae06-611a-4bfb-bc03-4fff5e45492d.png" alt="0f23ae06-611a-4bfb-bc03-4fff5e45492d" style="zoom:33%;" />

词汇表中每行序列的长短不一？对序列进行padding（按0填充），使得输入模型的seq_len统一

<img src="笔记图保存\fa269a0b-8962-44d7-9f26-fc90ec51694d.png" alt="fa269a0b-8962-44d7-9f26-fc90ec51694d" style="zoom:33%;" />

label怎么获取？统计国家，作出词典映射即可

<img src="笔记图保存\f5412d16-d2d1-4ef0-9ab1-1acfd456a116.png" alt="f5412d16-d2d1-4ef0-9ab1-1acfd456a116" style="zoom:33%;" />

```python
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pack_padded_sequence
import time
import matplotlib.pyplot as plt
import numpy as np
import gzip
import csv
import math

HIDDEN_SIZE = 100
BATCH_SIZE = 256
N_LAYER = 2
N_EPOCHS = 100
N_CHARS = 128
USE_GPU = True


# 数据集获取设置，注意这里继承了Dataset
class NameDataset(Dataset):

    # 这里使用is_train_set作为类初始化时的输入参数，之后可选择数据集是否为训练集/测试集
    def __init__(self, is_train_set=True):
        filename = 'names_train.csv.gz' if is_train_set else 'names_test.csv.gz'
        # 读取压缩文件中的数据
        with gzip.open(filename, 'rt') as f:
            reader = csv.reader(f)
            # 这里获取的是一个类似[['Adsit', 'Czech'], ['Ajdrna', 'Czech']]的二维列表数组（名字，国家）
            rows = list(reader)
        # 获取姓名样本和总样本数
        self.names = [row[0] for row in rows]
        self.len = len(self.names)
        # 获取姓名样本对应的国家类别标签列表（之后需转换为数字索引形式）
        self.countries = [row[1] for row in rows]
        # set：去重，sorted：排序，list：转列表
        self.country_list = list(sorted(set(self.countries)))
        # 设置国家字典，（键：国家名称，值：对应的数值索引0，1，2，3...）
        self.country_dict = self.getCountryDict()
        # 获取国家类别数（即最终分类数）
        self.country_num = len(self.country_list)

    # 为数据集提供索引访问，magic方法，当对象使用下标时自动调用该方法。获取样本、和对应标签
    # 此处：样本-字符串。标签-数字索引
    def __getitem__(self, index):
        return self.names[index], self.country_dict[self.countries[index]]

    # 这里__len__也是一个magic method，当对象调用len()时自动调用该方法
    def __len__(self):
        return self.len

    # 将国家列表转换为字典
    def getCountryDict(self):
        country_dict = dict()
        for idx, country_name in enumerate(self.country_list, 0):
            country_dict[country_name] = idx

        return country_dict

    # 方法：返回国家类别数（即最终分类数）
    def getCountriesNum(self):
        return self.country_num


# 数据准备
trainset = NameDataset(is_train_set=True)
trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)
testset = NameDataset(is_train_set=False)
testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)
N_COUNTRY = trainset.getCountriesNum()


# 函数：将创建的tensor迁移至GPU运行
def create_tensor(tensor):
    if USE_GPU:
        device = torch.device("cuda:0")
        tensor = tensor.to(device)

    return tensor


# 模型设计
class RNNClassifier(torch.nn.Module):
    """
    这里：
    input_size=词汇表维度
    hidden_size=词汇表嵌入维度=GRU隐藏层维度
    output_size=最终分类数
    n_layers=GRU层数
    bidirectional=设置双向网络
    """

    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):

        super(RNNClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.n_directions = 2 if bidirectional else 1
        # 定义嵌入层、GRU、线性层
        self.embedding = torch.nn.Embedding(input_size, hidden_size)
        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers,
                                bidirectional=bidirectional)
        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size)

    def _init_hidden(self, batch_size):

        hidden = torch.zeros(self.n_layers * self.n_directions,
                             batch_size, self.hidden_size)

        # 设置初始隐藏层时，设置了在GPU上创建
        return create_tensor(hidden)

    # 一次前向传播的过程，
    # 分类器进行函数调用时需要的输入参数包括：输入的姓名向量列表，seq_len列表
    def forward(self, input, seq_lengths):

        # 此处.t()方法表示对矩阵进行转置操作
        # input.size(0)：每个姓名的长度。input.size(1)：处理姓名的batch大小
        input = input.t()
        batch_size = input.size(1)

        # 初始化隐层、嵌入层
        hidden = self._init_hidden(batch_size)
        embedding = self.embedding(input)

        # pack_padded_sequence是PyTorch中用于处理变长序列数据的一个重要工具，
        # 其主要作用是高效地跳过填充部分的计算，从而节省计算资源并提高模型性能。
        # 在调用 pack_padded_sequence 之前，将 seq_lengths 移动到 CPU
        # 先前保留seq_lengths、以及对序列进行降序排列的目的，就是在此处使用
        seq_lengths = seq_lengths.cpu()
        gru_input = pack_padded_sequence(embedding, seq_lengths)

        # 将处理后的数据传入GRU处理，并获得输出
        output, hidden = self.gru(gru_input, hidden)
        if self.n_directions == 2:
            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)
        else:
            hidden_cat = hidden[-1]
        # 将hidden输出，输入线性层进行维度变换
        fc_output = self.fc(hidden_cat)

        return fc_output


# 函数：将单个人名转换为对应ASCLL值的列表，同时返回人名列表和人名长度
def name2list(name):
    arr = [ord(c) for c in name]

    return arr, len(arr)


# 函数主要工作：对人名（padding操作）、国家进行处理
def make_tensors(names, countries):
    # 获取（ASCLL人名，长度）的列表
    sequences_and_lengths = [name2list(name) for name in names]
    # 单独获取ASCLL人名列表
    name_sequences = [s1[0] for s1 in sequences_and_lengths]
    # 单独获取长度列表（转换为LongTensor类型）
    seq_lengths = torch.LongTensor([s1[1] for s1 in sequences_and_lengths])

    # 创建全0向量矩阵（向量长度设置为最长人名）
    seq_tensor = torch.zeros(len(name_sequences), seq_lengths.max()).long()

    # 同时遍历人名、长度列表、全0向量矩阵，
    # 对矩阵遍历的过程中，每行前seq_len个元素被设置为人名，其余元素保持为0（即padding操作）
    for idx, (seq, seq_len) in enumerate(zip(name_sequences, seq_lengths), 0):
        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)

    # 对人名样本，按长度进行排序（同时，国家标签也对应修改顺序）
    # 返回值：
    # seq_lengths：排序后的新长度列表（从大到小）。
    # perm_idx：一个索引张量，表示原始数据应该按什么顺序排列才能得到排序后的结果。
    """
        举例 ：
        若原始 seq_lengths = [3, 5, 2]，
        排序后 seq_lengths = [5, 3, 2]，
        perm_idx = [1, 0, 2]（表示原索引1的数据排第一，索引0的排第二，索引2的排第三）。
    """
    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)
    # 对人名，按长度进行样本排序
    seq_tensor = seq_tensor[perm_idx]
    # 对国家，同样按照索引进行排序
    countries = countries[perm_idx]

    # 返回：迁移至GPU运行、并进行排序后的
    # 人名样本、长度序列、国家标签
    return create_tensor(seq_tensor), \
        create_tensor(seq_lengths), \
        create_tensor(countries)


# 计算耗时的函数
def time_since(since):
    # 当前时间-过去时间=运行时长
    s = time.time() - since
    m = math.floor(s / 60)
    s -= m * 60

    return '%dm %ds' % (m, s)

# 模型的训练函数
def trainModel():
    total_loss = 0
    for i, (names, countries) in enumerate(trainloader, 1):
        inputs, seq_lengths, target = make_tensors(names, countries)

        # 在调用 pack_padded_sequence 之前，将 seq_lengths 移动到 CPU
        seq_lengths = seq_lengths.cpu()

        output = classifier(inputs, seq_lengths)
        loss = criterion(output, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # 对每个batch的平均损失进行累加
        total_loss += loss.item()
        # 每10个batch进行一次结果输出
        if i % 10 == 0:
            print(f'[{time_since(start)}] Epoch {epoch}', end='')
            print(f'[{i * len(inputs)} / {len(trainset)}]', end='')
            print(f'loss = {total_loss / (i * len(inputs))}')

    return total_loss

# 模型的测试函数
def testModel():
    correct = 0
    total = len(testset)
    print("evaluating trained model ...")
    with torch.no_grad():
        for i, (names, countries) in enumerate(testloader, 1):
            inputs, seq_lengths, target = make_tensors(names, countries)
            output = classifier(inputs, seq_lengths)
            pred = output.max(dim=1, keepdim=True)[1]
            correct += pred.eq(target.view_as(pred)).sum().item()

        percent = '%.2f' % (100 * correct / total)
        print(f'Test set: Accuracy {correct} / {total} {percent} %')

    return correct / total


if __name__ == '__main__':
    # 此处GRU分类器，输入定义：词汇表维度，隐藏层维度，国家数（分类数），GRU层数
    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)
    # 将分类器迁移至GPU运行
    if USE_GPU:
        device = torch.device("cuda:0")
        classifier.to(device)

    # 构造损失函数和优化器
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)

    # 记录训练开始时间
    start = time.time()
    # 进行模型训练与测试（已分别封装为模块）
    print("Traning for %d epochs..." % N_EPOCHS)
    acc_list = []
    for epoch in range(1, N_EPOCHS + 1):
        trainModel()
        acc = testModel()
        # 将每个epoch的测试结果添加进列表
        acc_list.append(acc)

epoch = np.arange(1, len(acc_list) + 1, 1)
acc_list = np.array(acc_list)
plt.plot(epoch, acc_list)
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.grid()
plt.show()

```

改进的双向循环神经网络：

对序列进行正向递归计算，同时对序列进行方向递归计算，并将正向与反向的递归结果拼接。

<img src="笔记图保存\fccc6872-c9ff-4425-88e3-5681e8ed86a0.png" alt="fccc6872-c9ff-4425-88e3-5681e8ed86a0" style="zoom:33%;" />

输出是2倍（结果进行拼接）

<img src="笔记图保存\73f77a7e-e80a-403f-98b5-ee7623d12d7f.png" alt="73f77a7e-e80a-403f-98b5-ee7623d12d7f" style="zoom:33%;" />

​                                                                                                                                                                                                                                                                                                                                                                                                                 打包的原理：对变长序列排序后，进行拼接操作

<img src="笔记图保存\27d3505e-78d0-4790-9d48-6c5eb2c06186.png" alt="27d3505e-78d0-4790-9d48-6c5eb2c06186" style="zoom:33%;" />

## LSTM

重点见书籍

对于LSTM的基本原理，其形式基本与GRU相同，
不同点在于，LSTM需要两个初始隐藏状态张量：h_0（隐藏状态）和 c_0（细胞状态），
而GRU和RNN类似，只需一个初始隐藏状态张量 h_0。 

## Mamba

### Mamba-1

### Mamba-2

# Transformer

见B站视频和书籍，重点在于QKV的注意力机制。

[第四节 2021 - 自注意力机制(Self-attention)(上)_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Wv411h7kN?spm_id_from=333.788.videopod.episodes&vd_source=f931e43e5bf927b47a1004851b8915f3&p=38)

# TTT



# GNN

## 1.图神经网络基础知识

1.同质图（节点和边的类型，在1张图中一样）；异质图
2.无向图；有向图；混合图
3.赋权图；无权图

同构图和同质图区别

<img src="笔记图保存\c6d2fb3e-3644-4dfe-9a2e-beed08555014.png" alt="c6d2fb3e-3644-4dfe-9a2e-beed08555014" style="zoom:50%;" />

邻接矩阵（节点-节点）和关联矩阵（节点-边）

<img src="笔记图保存\ef4bda0a-3663-45fc-ba22-4bda496eb529.png" alt="ef4bda0a-3663-45fc-ba22-4bda496eb529" style="zoom: 33%;" />

邻居、度、入度、出度

<img src="笔记图保存\ef6c1325-2d6b-4682-82a0-4f5d5719e312.png" alt="ef6c1325-2d6b-4682-82a0-4f5d5719e312" style="zoom:50%;" />

下面这张图是一个**图神经网络“族谱”或“技术分解图”**，帮助我们理解不同GNN模型的核心组件和设计思想。它将复杂的GNN模型分解为三个核心模块：**传播模块（Propagation Module）**、**采样模块（Sampling Module）和池化模块（Pooling Module）**。

<img src="笔记图保存\3-1.png" alt="3-1" style="zoom: 33%;" />

**1. 传播模块 (Propagation Module)**

这是GNN最核心的部分，负责定义节点如何聚合其邻居节点的信息来更新自身的表示（Representation）。这个过程就像在社交网络中，你的想法会受到朋友们的影响一样。该模块主要分为三类：

- **卷积算子 (Convolution Operator)**：这是目前最主流的传播方式，借鉴了卷积神经网络（CNN）的思想。它又分为两类：
  - **谱方法 (Spectral)**：这类方法基于图谱理论，将图信号转换到谱空间（频域）进行处理，理论性很强。代表模型有 **GCN (图卷积网络)** 和 **ChebNet**。
  - **空间方法 (Spatial)**：这类方法直接在图的空间结构上（即节点和其邻居）进行信息聚合，更直观、灵活。它又可细分为：
    - **基础 (Basic)**：如 **GraphSAGE**，它通过聚合邻居节点的特征来学习。
    - **注意力 (Attentional)**：如 **GAT (图注意力网络)**，它在聚合邻居信息时引入了注意力机制，可以为不同的邻居分配不同的重要性权重。
    - **框架 (Framework)**：如 **MPNN (消息传递神经网络)**，它提供了一个统一的框架来描述各种空间方法。
- **循环算子 (Recurrent Operator)**：这类方法借鉴了循环神经网络（RNN）的思想，通过迭代更新节点状态直至收敛。这在早期的GNN模型中比较常见，例如 **GNN** 和 **GGNN (门控图神经网络)**。
- **跳跃连接 (Skip Connection)**：借鉴于ResNet，这种技术允许信息从浅层直接“跳跃”到深层，以构建更深、更强大的GNN模型，有效缓解了深度网络中的梯度消失问题。代表模型有 **JKN (Jumping Knowledge Networks)** 和 **DeepGCN**。

**2. 采样模块 (Sampling Module)**

当图的规模非常巨大（例如，拥有数百万甚至数十亿个节点的社交网络或电商推荐网络）时，将所有节点的邻居信息都纳入计算是不现实的。采样模块就是为了解决这个问题，它通过对图进行采样来缩小计算规模。

- **节点采样 (Node)**：如 **GraphSAGE** 和 **PinSAGE** (由Pinterest提出)，它们为每个节点采样固定数量的邻居，而不是使用所有邻居。
- **层采样 (Layer)**：如 **FastGCN**，它在每一层都独立地采样一部分节点，进一步提高了效率。
- **子图采样 (Subgraph)**：如 **ClusterGCN** 和 **GraphSAINT**，它们将图划分为多个子图（Cluster），然后在子图上进行训练，大大减少了计算和内存开销。

**3. 池化模块 (Pooling Module)**

池化（或称“读出”，Readout）操作用于将图中所有节点的特征聚合成一个单一的、固定大小的向量，从而得到整个图的表示。这对于图分类、图属性预测等任务至关重要。

- **直接池化 (Direct)**：这类方法比较简单，例如直接对所有节点的特征进行求和、求平均或使用更复杂的 **Set2Set** 模型。
- **层次化池化 (Hierarchical)**：这类方法借鉴了CNN中池化层的思想，通过逐步“粗化”（Coarsening）或对节点进行聚类来学习图的层次化表示。代表模型有 **DiffPool** 和 **SAGPool**。

## 2.使用NetworkX可视化图

```python
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np

# 创建有向图
G = nx.DiGraph()

# 添加节点
G.add_nodes_from(['A', 'B', 'C'])
# 添加边
G.add_edges_from([
    ('A', 'A'),
    ('A', 'B'),
    ('B', 'C'),
    ('C', 'B')
])

# 设置节点特征
node_feature = {
    'A': '[1,3]',
    'B': '[2,4]',
    'C': '[5,7]'
}
nx.set_node_attributes(G, node_feature, name='node_feature')
# 设置边权重
edge_weights = {
    ('A', 'B'): 4.5,
    ('B', 'C'): 2.0,
    ('C', 'B'): 6.0,
    ('A', 'A'): 1.0
}
nx.set_edge_attributes(G, edge_weights, name='edge_weights')

# 获取属性用于绘图
node_labels = nx.get_node_attributes(G, 'node_feature')
edge_labels = nx.get_edge_attributes(G, 'edge_weights')

# 绘图
plt.figure(figsize=(8, 6))
# 设置布局格式
pos = nx.spring_layout(G, seed=42)
# 创建基础绘图
nx.draw(G, pos, with_labels=True)
# 添加节点属性
nx.draw_networkx_labels(G, pos, labels=node_labels)
# 添加边权重
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
plt.show()

# 获取图信息
print(G.nodes)  # 获取节点
print(G.edges)  # 获取边
print(G.degree())  # 获取各个节点的度

# 判断图信息
print(nx.is_directed(G))  # 是否为有向图
print(nx.is_weighted(G))  # 是否为加权图

# 邻接矩阵
A = nx.adjacency_matrix(G)
print(A)
print(A.todense())

# 度矩阵
D = np.diag(list(dict(nx.degree(G)).values()))
print(D)
print(G.in_degree)  # 节点入度数
print(G.out_degree)  # 节点出度数
print(list(G.neighbors('A')))  # 获取某个节点的邻居

```

## 3.理解图神经网络和信息传递机制

<img src="笔记图保存\3-2.png" alt="3-2" style="zoom:50%;" />

1.挑选信息 2.聚合邻居信息 3.将聚合后的邻居信息与自身信息聚合。

消息传递机制：

```python
import torch
from torch_geometric.nn import MessagePassing

# 定义4个节点特征
x = torch.tensor([[1, 2],
                  [2, 3],
                  [8, 3],
                  [2, 4]])
# 定义边（2维矩阵形式，第1行：入，第2行：出）
edge_index = torch.tensor([[0, 0, 1, 2, 2, 3],
                           [0, 1, 2, 1, 3, 2]])


class MessagePassingLayer(MessagePassing):
    def __init__(self):
        # 设置邻居的聚合方式为相加
        super(MessagePassingLayer, self).__init__(aggr='add')

    def forward(self, x, edge_index):
        # propagate()内部调用message()函数，自动拆分edge_index为索引对应的节点特征，并将x_j传入给message()
        return self.propagate(x=x, edge_index=edge_index)

    def message(self, x_i, x_j):
        print(x_i)  # 中心节点特征（出）
        print(x_j)  # 邻居节点特征（入）
        # 返回邻居节点特征+自身特征，这里在求和前进行运算，对每条边的消息进行信息设置
        return x_j + x_i


MessagePassingLayer = MessagePassingLayer()
output = MessagePassingLayer(x, edge_index)
print(output)

```

<img src="笔记图保存\25024bd8-67d4-4261-9ce4-c1ab3afd662f.png" alt="25024bd8-67d4-4261-9ce4-c1ab3afd662f" style="zoom:50%;" />

<img src="笔记图保存\01d956a6-9868-44e5-9134-197c6d148d63.png" alt="01d956a6-9868-44e5-9134-197c6d148d63" style="zoom:50%;" />

## 4.图卷积网络(GCN)的原理与实现

邻接矩阵*特征矩阵=对节点特征的1次更新（这里直接作矩阵乘法，是特征相加）

<img src="笔记图保存\3a3e1dd5-0bea-44fa-96e4-c66d3613907f.png" alt="3a3e1dd5-0bea-44fa-96e4-c66d3613907f" style="zoom: 33%;" />

但由于图结构不包含自环，邻接矩阵对角线上均为0，造成无法结合节点自身特征实现更新，因此常在原始邻接矩阵A上加上自环（即对角线上元素为1）。

此处，考虑对特征进行缩放，可采用度矩阵的逆。而在GCN过程中，度和图论中不同，自环在此处的度只增加1（图论中由于同时增加入度和出度，会增加2）。

此处，使用度矩阵对邻接矩阵进行左乘，实现行变换，使邻接矩阵数值缩放（不再是1），可看成对特征进行了加权，例如度是3时，将每个邻居节点的特征都*1/3。

但此时仅实现行变换而缺少列变换（对发送节点、接收节点均需要进行归一化），因此使用度矩阵对邻接矩阵进行右乘，实现列变换。同时因为做了2次变换，需要平衡一下所以开了根号。

整体上，GCN就是以节点的度为依据，对边的权重进行了修改缩放，并实现信息传递。GCN的规范化，使得中心节点本身和邻居特征聚合的权重发生变化；是一种根据度矩阵（连接数）加权的聚合方式；对中心节点而言，邻居的度越大，传递给中心节点的信息越少。

<img src="笔记图保存\d67081d0-d3f7-4640-967d-9c04b2c28ac2.png" alt="d67081d0-d3f7-4640-967d-9c04b2c28ac2" style="zoom:50%;" />

代码实现，核心公式：

<img src="笔记图保存\dfab5805-1292-48b8-8fd0-fd93f6739b66.png" alt="dfab5805-1292-48b8-8fd0-fd93f6739b66" style="zoom:33%;" />

```python
from typing import Optional, Union, List, Dict, Any

import torch
from torch.nn import Linear, Parameter
from torch_geometric.nn import MessagePassing, Aggregation
from torch_geometric.utils import add_self_loops, degree

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class GCNConv(MessagePassing):
    # 初始化类，这里添加了（in_channels, out_channels）的输入定义
    def __init__(self, aggr: Optional[Union[str, List[str], Aggregation]] = 'sum', *,
                 aggr_kwargs: Optional[Dict[str, Any]] = None, flow: str = "source_to_target", node_dim: int = -2,
                 decomposed_layers: int = 1, in_channels, out_channels) -> None:
        super().__init__(aggr, aggr_kwargs=aggr_kwargs, flow=flow, node_dim=node_dim,
                         decomposed_layers=decomposed_layers)
        # 设置线性变换层
        self.lin = Linear(in_channels, out_channels)
        # 设置偏置项
        # Parameter() 函数的作用是将一个 Tensor 包装成模型的可学习参数，使其能够被自动管理并在训练过程中更新。
        self.bias = Parameter(torch.empty(out_channels))

    def forward(self, x, edge_index):
        # 添加自环信息
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # 对节点特征进行线性变换
        x = self.lin(x)

        # 获取输入（邻居节点xj）和输出（中心节点xi）的索引
        xj, xi = edge_index
        # 获取中心节点的度
        deg = degree(xi, x.size(0), dtype=x.dtype)
        # 计算中心节点度的-1/2次方
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0

        # 边权重缩放：入度^-1/2 * 出度^-1/2
        norm = deg_inv_sqrt[xj] * deg_inv_sqrt[xi]

        # 调用propagate函数，传入edge_index、特征x、和归一化计算方法norm
        # 内部调用message()函数，自动拆分edge_index为索引对应的节点特征，并将x_j传入给message()
        out = self.propagate(edge_index, x=x, norm=norm)

        # 最终添加偏置项
        out += self.bias

        return out

    def message(self, x_j, norm):
        # 边信息处理（优先于邻居加法聚合）：入度^-1/2 * 出度^-1/2 * 邻居节点特征
        return norm.view(-1, 1) * x_j


if __name__ == '__main__':
    # 定义4个节点属性
    x = torch.tensor([[1, 2],
                      [2, 3],
                      [8, 3],
                      [2, 4]], dtype=torch.float32)
    # 定义边（2维矩阵形式，第1行：入，第2行：出）
    edge_index = torch.tensor([[0, 0, 1, 2, 2, 3],
                               [0, 1, 2, 1, 3, 2]], dtype=torch.int64)

    x = x.to(device)
    edge_index = edge_index.to(device)
    gcn_conv = GCNConv(in_channels=2, out_channels=4).to(device)

    result = gcn_conv(x, edge_index)
    print(result)

```

# 计算机视觉

图像分类、目标检测、风格转换

问题在于，如使用全连接神经网络，图像过大：

1M图像有1000*1000个像素，再乘以3个通道是输入X的特征维度，假设第1层神经元维度1000，中间矩阵：1000 * 1000*1000 * 3，计算过于复杂，且参数量过大极易导致过拟合。

## CNN

**边缘探测**

如下图，对垂直信息进行检测，如果左右列向量很类似的话，会被相加抵消，如果差距较大，则会出现数值。（寻找逻辑：忽视中间数据，而左边很明亮，右边较暗的垂直边缘）

<img src="笔记图保存\2eca5bbc-4a28-47a9-b964-c8da79f3e6a0.png" alt="2eca5bbc-4a28-47a9-b964-c8da79f3e6a0" style="zoom:33%;" />

检测的 正负边界：卷积结果数值和原本的1、-1 是成正或者负相关

<img src="笔记图保存\f77fe6cc-7c43-47e6-93d2-00a9b1b22896.png" alt="f77fe6cc-7c43-47e6-93d2-00a9b1b22896" style="zoom:33%;" />

算子可有多种，可考虑不手动设置，而是利用反向传播学习对应数值，目的是能够更准确地划分出目标边界。

<img src="笔记图保存\c7d44e65-a45b-4eb6-9693-22fb1ba9b22e.png" alt="c7d44e65-a45b-4eb6-9693-22fb1ba9b22e" style="zoom:33%;" />

**填充**

初始卷积缺点：

1.每次卷积将缩小图片，导致图片不断变小

2.对图中角落像素的使用次数过少，失去图片靠近边界的信息

<img src="笔记图保存\14a69009-a79a-4dba-a148-3e60f272ce25.png" alt="14a69009-a79a-4dba-a148-3e60f272ce25" style="zoom:33%;" />

通过padding，能充分防止图像变小，且能有效利用边缘像素信息

<img src="笔记图保存\3de285d4-1e05-4409-b6a9-b0f4901e1c40.png" alt="3de285d4-1e05-4409-b6a9-b0f4901e1c40" style="zoom:33%;" />

同时，如下可以很好地解释，为何算子都是奇数，如3×3，5×5：p=(f-1)/2

<img src="笔记图保存\f73e496b-bf22-40ed-baa9-c552898812c7.png" alt="f73e496b-bf22-40ed-baa9-c552898812c7" style="zoom:33%;" />

**步长**

设置步长，即进行卷积时上下都会跳过特定步数

<img src="笔记图保存\5ae89261-be0f-49c6-a029-8e1fd7a28afe.png" alt="5ae89261-be0f-49c6-a029-8e1fd7a28afe" style="zoom:33%;" />

问题：卷积算子有可能会超出图像边界，解决方法：向下取整

<img src="笔记图保存\25a4233a-819d-4ac5-8f6f-fa7039efe487.png" alt="25a4233a-819d-4ac5-8f6f-fa7039efe487" style="zoom:33%;" />

卷积整体公式：

<img src="笔记图保存\88d8295c-e322-4ad7-8e8c-8407c39aebba.png" alt="88d8295c-e322-4ad7-8e8c-8407c39aebba" style="zoom:33%;" />

数学教材中，实际上会对卷积进行翻转操作，而在机器学习中，则实际上会忽略这个步骤，

如果在数学上，不进行翻转的操作实际上应该称为cross-correlation而不是convolution。

深度学习中的“卷积”实际上是 cross-correlation，但大家仍习惯称为 convolution。

<img src="笔记图保存\c4b13662-a93f-486f-9bc7-d1f118b9d565.png" alt="c4b13662-a93f-486f-9bc7-d1f118b9d565" style="zoom:33%;" />

**3层卷积**

可分别对RGB3个通道，实现不同的卷积操作

<img src="笔记图保存\9f0d6c9d-83ac-4788-b4e9-1037f66edcfc.png" alt="9f0d6c9d-83ac-4788-b4e9-1037f66edcfc" style="zoom:33%;" />

可采用不同的滤波器组，实现水平、垂直....边缘检测，最后将卷积结果叠在一起

<img src="笔记图保存\d80991b2-b682-4192-9a04-1c04cf495c56.png" alt="d80991b2-b682-4192-9a04-1c04cf495c56" style="zoom:33%;" />

## MobileNet

深度可分离卷积：

**Depthwise卷积（逐通道卷积）**

这里，逐通道卷积将保留中间结果而不是直接求和。

<img src="笔记图保存\09f4a18b-d5b7-43c1-a136-411ae43eac9d.png" alt="09f4a18b-d5b7-43c1-a136-411ae43eac9d" style="zoom:33%;" />

**Pointwise卷积（逐点卷积）**

5个1 * 1 * 3的滤波器，对先前逐通道卷积的结果进一步卷积计算

<img src="笔记图保存\432add1f-1216-40a2-bdfb-3ea8ef1de21a.png" alt="432add1f-1216-40a2-bdfb-3ea8ef1de21a" style="zoom:33%;" />

**MobileNetV2**

特点：增加了残差结构、增加了Expansion和Projection操作（Bottleneck）

<img src="笔记图保存\526dc68b-1f54-4141-b26a-219a02bca1b7.png" alt="526dc68b-1f54-4141-b26a-219a02bca1b7" style="zoom:33%;" />



## EfficientNet

EfficientNet：考虑特定设备，对原始图像分辨率（r），神经网络深度（d），滤波器宽度（w），

通过一个复合缩放方法（compound scaling）同时、均衡地缩放这三个维度。

<img src="笔记图保存\045c780b-3dc3-4587-8740-e1dfac97ec1a.png" alt="045c780b-3dc3-4587-8740-e1dfac97ec1a" style="zoom:33%;" />

## 迁移学习

### 概念

迁移学习一般发生在已有数据集较小的情况下。

**特征提取**：可选择性冻结某些层，固定其参数设置，仅对一部分层的参数进行训练（只改动softmax？只改动后几层？替换后几层？...等等）。此类情况一般在数据集比较小的时候适用，数据集越大，越不需要实现迁移学习，可训练的层数和参数也越多。

**全量微调**：极端情况下，可下载别人的全部权重，作为初始化参数对整个模型更新，而这可能也比自己的初始化参数设置更好。

<img src="笔记图保存\17e96f6b-1c86-48ff-a99a-5a1b25498cbe.png" alt="17e96f6b-1c86-48ff-a99a-5a1b25498cbe" style="zoom:33%;" />

目前最主流的迁移学习，PEFT方法可以被系统地分为以下几类 。
**增量式方法** (Additive Methods)：在冻结原始模型参数的同时，向模型中添加新的、可训练的模块或参数。这些新增的模块规模很小，专门用于学习任务特定的知识。代表性技术包括适配器（Adapters）、前缀调优（Prefix-Tuning）和提示调优（Prompt Tuning）。
**选择式方法** (Selective Methods)：不引入新参数，而是选择性地微调原始模型参数的一个小子集。例如，只微调模型中的偏置项（bias terms）或特定层的某些参数。
**重参数化方法** (Reparameterization-based Methods)：通过低秩分解等技术对模型的权重矩阵进行重参数化，训练时只更新这些低秩因子。这种方法的代表是低秩适应（Low-Rank Adaptation, LoRA）。
**混合与统一框架** (Hybrid and Unified Frameworks)：结合上述多种方法的思想，以期获得更好的性能与效率权衡。

<img src="笔记图保存\PixPin_2025-09-11_21-38-05.png" alt="PixPin_2025-09-11_21-38-05" style="zoom:50%;" />

常见对比

| 微调策略     | 可训练参数 (%) | 训练显存占用 | 存储成本 | 推理延迟                 | 性能 vs 全量微调 | 最适用场景                                                   |
| ------------ | -------------- | ------------ | -------- | ------------------------ | ---------------- | ------------------------------------------------------------ |
| **全量微调** | 100%           | 非常高       | 非常高   | 无额外延迟               | 基准             | 追求极限性能且计算资源充足。                                 |
| **LoRA**     | < 1%           | 中等         | 非常低   | 无额外延迟（可合并）     | 相当或略优       | 需要高效训练和部署，且对推理速度要求苛刻。                   |
| **QLoRA**    | < 1%           | 非常低       | 非常低   | 微小额外延迟（需反量化） | 相当             | 训练显存极度受限，是 democratizing LLM fine-tuning 的关键技术。 |
| **提示微调** | < 0.1%         | 低           | 非常低   | 微小额外延迟             | 略低于全量微调   | 任务相对简单，追求极致的参数效率和存储效率。                 |
| **UniPT**    | < 1%           | 低           | 非常低   | 存在额外延迟（并行网络） | 相当             | 需要一种与模型无关、通用性强的PEFT方法，跨多种模型和任务。   |

### 参数高效微调（Parameter-Efficient Fine-Tuning, PEFT）

PEFT官网教程：https://huggingface.co/docs/peft/index

LoRA 通过低秩分解将权重更新 ∆W 表示为两个较小的矩阵（称为*更新矩阵* ）。这些新矩阵可以进行训练以适应新数据，同时保持较低的参数总数。原始权重矩阵保持不变，不会进行任何进一步的更新。为了生成最终结果，需要将原始权重和额外的自适应权重合并。还可以将适配器权重与基础模型合并，以消除推理延迟。

查看右边，先缩放维度，再扩张维度，将d×d变为d×r @ r×d。

<img src="笔记图保存\lora_animated.gif" alt="lora_animated" style="zoom:50%;" />

LoRA的训练，是不影响模型结构的。

原则上，LoRA 可以应用于神经网络中权重矩阵的任何子集，以减少可训练参数的数量。然而，为了简化操作并进一步提高参数效率，LoRA 通常仅应用于 Transformer 模型中的注意力模块。LoRA 模型中可训练参数的数量取决于更新矩阵的大小，而更新矩阵的大小主要由秩 `r` 和原始权重矩阵的形状决定。

类似LoRA的最新方法还包括：AdaLoRA, IA3, Llama-Adapter, LoHa, LoKr, LoRA, X-LoRA, Lycoris, Multitask Prompt Tuning, OFT, BOFT, Polytrapon, P-tuning, Prefix tuning, Prompt tuning, Layernorm tuning, VeRA, FourierFT, VB-LoRA, HRA, CPT, Bone, Trainable Tokens, RandLora, SHiRA, C3A, MiSS等，均可通过PEFT调用。

```python
# ==============================
# 第一步：导入所有必需库
# ==============================
import os

os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = '/home/gjk/hugging_face_cache'
from datasets import load_dataset  # 加载 Hugging Face 数据集
from transformers import (
    AutoImageProcessor,  # 图像预处理器（用于 ViT）
    AutoModelForImageClassification,  # 预训练视觉分类模型
    TrainingArguments,  # 训练超参数配置
    Trainer,  # 训练器（封装训练循环）
)
from peft import (
    LoraConfig,  # LoRA 配置类
    get_peft_model,  # 将基础模型转换为 PEFT 模型
)
import torch
from torchvision.transforms import (  # 图像增强和预处理
    CenterCrop,
    Compose,
    Normalize,
    RandomHorizontalFlip,
    RandomResizedCrop,
    Resize,
    ToTensor,
)

# ==============================
# 第二步：加载并预处理数据集（Food-101）
# ==============================

# 加载 Food-101 数据集（101 类食物图像）
ds = load_dataset("food101")

# 获取标签名称列表，并创建标签 <-> ID 的双向映射字典
labels = ds["train"].features["label"].names
label2id = {label: i for i, label in enumerate(labels)}
id2label = {i: label for i, label in enumerate(labels)}
# 示例：查看 ID=2 对应的类别
print(f"ID 2 对应的类别: {id2label[2]}")  # 输出: baklava

# 加载与预训练模型配套的图像处理器（负责标准化、缩放等）
image_processor = AutoImageProcessor.from_pretrained("google/vit-base-patch16-224-in21k")

# 定义图像增强和标准化流程
normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)

# 训练集数据增强：随机裁剪 + 水平翻转 + 转 Tensor + 标准化
train_transforms = Compose([
    RandomResizedCrop(image_processor.size["height"]),  # 随机裁剪到 224x224
    RandomHorizontalFlip(),  # 随机水平翻转（数据增强）
    ToTensor(),  # PIL Image → Tensor
    normalize,  # 标准化
])

# 验证集预处理：缩放 + 中心裁剪 + 转 Tensor + 标准化（无增强）
val_transforms = Compose([
    Resize(image_processor.size["height"]),  # 缩放到 224
    CenterCrop(image_processor.size["height"]),  # 中心裁剪
    ToTensor(),
    normalize,
])


# 定义预处理函数：将原始图像转为模型可接受的 pixel_values
def preprocess_train(example_batch):
    """对训练集每个 batch 应用图像增强"""
    example_batch["pixel_values"] = [
        train_transforms(image.convert("RGB")) for image in example_batch["image"]
    ]
    return example_batch


def preprocess_val(example_batch):
    """对验证集每个 batch 应用图像预处理"""
    example_batch["pixel_values"] = [
        val_transforms(image.convert("RGB")) for image in example_batch["image"]
    ]
    return example_batch


# 应用预处理函数（on-the-fly，dataloader取数据时才动态执行预处理，节省内存）
train_ds = ds["train"]
val_ds = ds["validation"]
train_ds.set_transform(preprocess_train)
val_ds.set_transform(preprocess_val)


# 定义数据整理器（Data Collator）：将多个样本打包成一个 batch
def collate_fn(examples):
    """将一批样本整理成模型输入格式"""
    pixel_values = torch.stack([example["pixel_values"] for example in examples])
    labels = torch.tensor([example["label"] for example in examples])
    return {"pixel_values": pixel_values, "labels": labels}


# ==============================
# 第三步：加载预训练模型并应用 LoRA
# ==============================
# 加载预训练 ViT 模型（ImageNet-21k 预训练）
model = AutoModelForImageClassification.from_pretrained(
    "google/vit-base-patch16-224-in21k",
    label2id=label2id,  # 传递标签映射，确保输出层适配新任务
    id2label=id2label,
    ignore_mismatched_sizes=True,  # 忽略分类头尺寸不匹配（因为 Food-101 是 101 类，原模型是 21k 类）
).to("cuda")

# 配置 LoRA 参数
"""
在 PEFT (LoRA) 中，默认情况下，原始预训练模型的所有权重都是被冻结（frozen）的，只有新插入的 LoRA 适配器矩阵（A 和 B）会被训练。
modules_to_save 是一个白名单机制，它允许额外指定某些原始模型的模块（层）在训练过程中也保持可训练状态。
modules_to_save 中列出的模块，其原始权重不会被冻结，会像普通微调一样进行梯度更新。 
"""
peft_config = LoraConfig(
    r=16,  # 低秩矩阵的秩（rank），越大参数越多，表达能力越强
    lora_alpha=32,  # 缩放因子，不影响参数量
    target_modules=["query", "value"],  # 在 ViT 的注意力层中，对 query 和 value 矩阵应用 LoRA
    lora_dropout=0.1,  # LoRA 层的 dropout 概率
    bias="none",  # 不训练偏置项（节省参数）
    modules_to_save=["classifier"],  # 关键，除了 LoRA 层，额外保存分类头（classifier）的参数
)

# 将基础模型转换为 PEFT 模型（仅训练 LoRA 参数 + classifier）
model = get_peft_model(model, peft_config)

# 打印可训练参数信息（验证 LoRA 是否生效）
model.print_trainable_parameters()
# 示例输出: trainable params: 667,493 || all params: 86,543,818 || trainable%: 0.77%

# ==============================
# 第四步：配置训练参数并开始训练
# ==============================

# 设置训练超参数
training_args = TrainingArguments(
    output_dir="./vit-food101-lora",  # 模型和日志保存路径
    remove_unused_columns=False,  # 保留所有数据列（避免自动删除 pixel_values 等必要字段）
    evaluation_strategy="epoch",  # 每个 epoch 结束后评估
    save_strategy="epoch",  # 每个 epoch 结束后，保存一次模型检查点（checkpoint）
    learning_rate=5e-3,  # 学习率（LoRA 可使用较大学习率）
    per_device_train_batch_size=32,  # 每张 GPU 的训练 batch size（根据显存调整）
    gradient_accumulation_steps=4,  # 梯度累积步数（模拟更大 batch size）
    per_device_eval_batch_size=32,  # 验证 batch size
    fp16=True,  # 启用混合精度训练（节省显存，加速训练）
    num_train_epochs=5,  # 训练轮数
    logging_steps=10,  # 每 10 步打印一次训练日志
    load_best_model_at_end=True,  # 自动保存epoch下的最优模型
    metric_for_best_model="eval_loss",  # 以验证损失作为最优模型选择标准
    label_names=["labels"],  # 指定标签字段名（避免警告）
)
# 创建 Trainer 实例
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    tokenizer=image_processor,  # 传入 image_processor（用于预处理图像）
    data_collator=collate_fn,  # 使用自定义数据整理器
    # compute_metrics=compute_metrics,  # 如需自定义评估指标（如 accuracy），可在此处添加
)
# 开始训练，只要.to("cuda")，默认GPU运行
trainer.train()

# ==============================
# 第五步：保存模型
# ==============================
"""
# 登录你的 Hugging Face 账户（首次运行时需要输入 token）
# from huggingface_hub import notebook_login
# notebook_login()
# model.push_to_hub("your-username/vit-base-food101-lora")
"""
# 模型训练完成后，使用 save_pretrained 函数将模型保存到目录中，保存 LoRA 适配器
model.save_pretrained("./vit-food101-lora")

# ==============================
# 第六步：推理 - 加载模型并预测新图像
# ==============================
from peft import PeftModel  # 用于加载 PEFT 适配器
from PIL import Image
import requests

# 重新加载基础模型（必须与训练时一致）
base_model = AutoModelForImageClassification.from_pretrained(
    "google/vit-base-patch16-224-in21k",
    label2id=label2id,
    id2label=id2label,
    ignore_mismatched_sizes=True,
)

# 加载训练好的 LoRA 适配器
model = PeftModel.from_pretrained(base_model, "./vit-food101-lora")

# 将模型移至 GPU 并设为评估模式
model = model.to("cuda").eval()

# 下载一张测试图片（示例：beignets）
url = "https://huggingface.co/datasets/sayakpaul/sample-datasets/resolve/main/beignets.jpeg"
image = Image.open(requests.get(url, stream=True).raw).convert("RGB")

# 使用图像处理器预处理图像
encoding = image_processor(image, return_tensors="pt")
encoding = {k: v.to("cuda") for k, v in encoding.items()}  # 移至 GPU

# 执行推理
with torch.no_grad():
    outputs = model(**encoding)
    logits = outputs.logits
    predicted_class_idx = logits.argmax(-1).item()

# 输出预测结果
print(f"预测类别: {model.config.id2label[predicted_class_idx]}")
# 预期输出: "beignets"

```

### prompt微调（待更新）



## 数据增强

对计算机视觉任务来说，和机器学习不同，数据集越大越好（其它机器学习任务中，可能数据集达到一定规模后就不用再继续增加了）

**几何变换**

镜像、随机裁剪、旋转、Shearing（剪切变换，变为平行四边形）、Local Warping（局部扭曲）等

<img src="笔记图保存\ead322fc-fbdd-4d2f-a6a3-0088c1dc2b48.png" alt="ead322fc-fbdd-4d2f-a6a3-0088c1dc2b48" style="zoom:33%;" />

**颜色变换**

例如对RGB进行轻微修改（Alex-Net：PCA色彩增强？）

<img src="笔记图保存\bedc544c-3228-4577-aedf-6fd9e5c95139.png" alt="bedc544c-3228-4577-aedf-6fd9e5c95139" style="zoom:33%;" />

通常来说，CPU对数据增强的部分，和GPU进行训练的过程，会在多个进程下同步进行（CPU 数据增强通常在多进程下实现，GPU 训练在主进程中进行，整体采用“多进程数据加载 + 单进程 GPU 训练”的混合架构，以最大化硬件利用率）。

<img src="笔记图保存\9865c591-6900-43c8-a847-c053854a21dc.png" alt="9865c591-6900-43c8-a847-c053854a21dc" style="zoom:33%;" />

## YOLO算法

**目标定位**

图像分类：判断整个图像的类别

目标定位：通常是单个对象，判断类别的同时，需要框出对应位置

目标检测：通常是多个对象，判断类别的同时，需要框出对应位置

<img src="笔记图保存\94a7f3f2-6aca-492e-8f01-d5ed3bff017d.png" alt="94a7f3f2-6aca-492e-8f01-d5ed3bff017d" style="zoom:33%;" />

标签：图像类别 + x(框中心横坐标),y(框中心纵坐标),w(框宽度),h(框高度)

softmax不仅需要输出4种类别，还需输出4个对应的框坐标

<img src="笔记图保存\7afae81f-2bbf-4f5d-8659-d572909c8f4f.png" alt="7afae81f-2bbf-4f5d-8659-d572909c8f4f" style="zoom:33%;" />

第1个元素：0-1，表示是否存在对象

4个元素：位置

4个元素：具体类别/具体类别概率

如果第1个元素为0，剩下的元素预测都不会在乎

<img src="笔记图保存\dfa271ea-db1c-497a-b494-f3074c2fdb0a.png" alt="dfa271ea-db1c-497a-b494-f3074c2fdb0a" style="zoom:33%;" />

损失函数如何设计：分段函数

例如，针对C1-C4：交叉熵？针对bx-bw：均方根误差？Pc：逻辑回归？

<img src="笔记图保存\cc62c2cb-6495-480f-898d-790d03501a41.png" alt="cc62c2cb-6495-480f-898d-790d03501a41" style="zoom: 50%;" />

**特征点检测**

例如人脸关键点检测：

第1个元素表示是否识别到脸

64*2个关键点表示脸部关键特征点位置（x和y）

对于人体骨骼检测，同理。

<img src="笔记图保存\05da6967-66a9-4e3c-90a5-2680f41caf3d.png" alt="05da6967-66a9-4e3c-90a5-2680f41caf3d" style="zoom:33%;" />

**目标检测**

训练集：裁剪好的贴近车身的图片，带有标签：0-1（是否为汽车）

<img src="笔记图保存\304ba2f5-482a-48ec-a75b-a41a5f98797e.png" alt="304ba2f5-482a-48ec-a75b-a41a5f98797e" style="zoom:33%;" />

通过不同大小的窗口在图中滑动，判定每个方框内的区域是否含有车（0-1）

<img src="笔记图保存\3ba73454-747d-489d-a759-8c2fb305be54.png" alt="3ba73454-747d-489d-a759-8c2fb305be54" style="zoom:33%;" />

**卷积滑窗检测**

下图可见，全连接层的操作均可通过卷积操作替代

<img src="笔记图保存\54a6a7bc-1a0f-449a-96e0-8a94b208841e.png" alt="54a6a7bc-1a0f-449a-96e0-8a94b208841e" style="zoom:33%;" />

同时，不是通过滑动窗口分别对不同区域进行卷积（会存在大量重复的区域操作），

而是直接基于卷积对整个图片进行变换，使用1次前向传播过程代替多次。

最终的卷积结果，就是对应在原图相应位置窗口上依次滑动进行的卷积操作结果。

<img src="笔记图保存\8f8d13a6-b61c-4446-b527-9a0892bfd8b6.png" alt="8f8d13a6-b61c-4446-b527-9a0892bfd8b6" style="zoom:33%;" />

<img src="笔记图保存\546aa0c3-0b4a-4193-a1fe-235bf3b08ad2.png" alt="546aa0c3-0b4a-4193-a1fe-235bf3b08ad2" style="zoom:33%;" />

**边框预测**

YOLO（You Only Look Once）算法：

将图像划分为N*N的区域，每个区域具有标签：

Pc：0-1是否存在目标（中心点是否在对应网格内）（此处如中间图，虽含有部分目标，但目标中心点未在对应区域，因此也为0）

bx-bw：框位置

c1-c3：类别判断

例如针对此处整个图，卷积操作应该输出：3×3×8

此时每个网格中，不能出现超过1个的检测物

<img src="笔记图保存\bc5e4950-18d3-4a9f-82f6-699a2a93e151.png" alt="bc5e4950-18d3-4a9f-82f6-699a2a93e151" style="zoom:33%;" />

bx，by：相对于网格（3*3边框中的1个）左上角的距离确定（0-1之间）

bh，bw：相对于网格（3*3边框中的1个）的长度比例决定（可大于1）

<img src="笔记图保存\0f5418d8-11e9-4dd3-b2c5-fd1f58789175.png" alt="0f5418d8-11e9-4dd3-b2c5-fd1f58789175" style="zoom:33%;" />

**交并比**

交并比 IOU（Intersection Over Union）：

预测、实际检测框的交集/并集，一般取0.5作为预测正确基准，越高表示检测效果越好（1重合）

<img src="笔记图保存\e8fc1b17-95d8-41aa-a342-e85d26cb6942.png" alt="e8fc1b17-95d8-41aa-a342-e85d26cb6942" style="zoom:33%;" />

**非极大值抑制**

由于很多检测框可能指向同个对象，因此进行非极大值抑制，如下为单类别检测时的做法（筛选、排序、去重）：

1.去除所有小于某个Pc的检测框

2.排序模型预测的可能性Pc，找出最高Pc对应的检测框（循环步骤）

3.对于最高Pc对应的检测框，计算其它检测框与其IOU，并去除其中 IOU>0.5 的检测框，以防止对同一目标重复检测

<img src="笔记图保存\3c87225a-6640-444f-9b8f-6309dc2a09f0.png" alt="3c87225a-6640-444f-9b8f-6309dc2a09f0" style="zoom:33%;" />

如果是多类别？

<img src="笔记图保存\8ac86d5b-9b13-460d-8658-99d63cd459bc.png" alt="8ac86d5b-9b13-460d-8658-99d63cd459bc" style="zoom: 50%;" />

对每一个类别，都独立执行一次NMS流程

1.去除所有小于某个阈值（Pc×类别概率）的检测框

2.排序模型预测的可能性得分（Pc×类别概率），找出最高得分（Pc×类别概率）对应的检测框（循环步骤）

3.对于最高得分检测框，计算其它检测框与其IOU，并去除其中 IOU>0.5 的检测框，以防止对同一目标重复检测

**锚框（Anchor box）**

在先前模型中，单个网格中仅能检测单个对象，如要检测多个对象，则可利用锚框思想。

锚框：在预定义好的形状上微调检测框，而不是从头开始去预测目标。

每次检测时，如下图所示，向量中依次包含对每个锚框的检测结果，实现一个网格中的多目标检测。

<img src="笔记图保存\7154b9c0-0461-40b7-a3c3-478ddda2dc17.png" alt="7154b9c0-0461-40b7-a3c3-478ddda2dc17" style="zoom:33%;" />

预定义好不同的锚框形状，定义好锚框的初始位置。

<img src="笔记图保存\PixPin_2025-08-18_15-00-23.png" alt="PixPin_2025-08-18_15-00-23" style="zoom: 50%;" />

<img src="笔记图保存\PixPin_2025-08-18_15-09-24.png" alt="PixPin_2025-08-18_15-09-24" style="zoom:50%;" />

<img src="笔记图保存\PixPin_2025-08-18_15-06-23.png" alt="PixPin_2025-08-18_15-06-23" style="zoom:50%;" />

最后，通过NMS等后处理步骤，去除掉对于同一个物体的大量重叠的、置信度较低的预测框，保留下最终的检测结果。

## R-CNN

针对整个图片进行卷积操作，可能会针对大量不包含目标的无效区域进行，造成浪费。

R-CNN（Region-based Convolutional Neural Network）：对图片进行区域划分，然后针对每个区域（约2000个）放置边界框，并针对特定对应边界框进行卷积运算。

<img src="笔记图保存\PixPin_2025-08-18_15-35-25.png" alt="PixPin_2025-08-18_15-35-25" style="zoom:50%;" />

R-CNN问题：针对每个区域都要单独进行卷积操作，速度太慢

Fast R-CNN：使用卷积滑窗检测，1次卷积操作替代对每个区域的窗口卷积生成整个图的特征，并将候选区域映射到特征图上

Faster R-CNN：Fast R-CNN进行区域聚类的速度还是很慢，提出用 RPN（Region Proposal Network）替代 Selective Search，实现完全端到端的区域建议生成（即使用卷积操作进行区域划分），进一步提升速度

Mask R-CNN：在 Faster R-CNN 基础上增加分支用于实例分割

但大部分的应用中，区域划分下的目标检测算法，通常还是会比YOLO算法慢一些。

## U-Net

**语义分割**

和目标检测不同，图像分割任务将输出整个图的标签，对不同元素进行划分

<img src="笔记图保存\PixPin_2025-08-19_09-14-59.png" alt="PixPin_2025-08-19_09-14-59" style="zoom:50%;" />

原始卷积：先缩小-再缩小，语义分割：先缩小-再放大

核心在于，取消卷积中将图像进一步缩小的操作后，通过什么操作将图像放大？

<img src="笔记图保存\PixPin_2025-08-19_09-16-29.png" alt="PixPin_2025-08-19_09-16-29" style="zoom:50%;" />

**反卷积**

将原始图像中的元素，与对应滤波器中的每个元素依次相乘，将结果放入待生成区域。

下图中，输出期望4×4，padding=1，stride=2

<img src="笔记图保存\PixPin_2025-08-19_09-26-23.png" alt="PixPin_2025-08-19_09-26-23" style="zoom:50%;" />

<img src="笔记图保存\PixPin_2025-08-19_09-30-10.png" alt="PixPin_2025-08-19_09-30-10" style="zoom:50%;" />

**U-Net架构**

先卷积W-，H-，C+，后反卷积W+，H+，C-。

跳跃连接（Skip connection）：将左边的信息沿通道C方向与右侧信息拼接。

U-Net 采用编码器（下采样路径）和解码器（上采样路径）的对称结构：

- 编码器：通过卷积和池化操作提取高层次特征，逐步降低空间分辨率，增强语义信息。
- 解码器：通过上采样和卷积操作逐步恢复空间分辨率，实现像素级预测。

这种结构能有效结合低层细节和高层语义信息。

<img src="笔记图保存\PixPin_2025-08-19_09-53-39.png" alt="PixPin_2025-08-19_09-53-39" style="zoom: 33%;" />



## 人脸识别

**单样本学习**

人脸识别场景中，往往只有单个人的脸部样本，称为单样本（one-shot）问题。

此时并没有足够数据支持模型分类学习，思路是利用神经网络寻找一种函数，d(img1,img2)=N，用于确定2张图片之间的差异。

通过设定N的阈值，确定2张图片是否属于同一人。

**孪生神经网络**

孪生神经网络（Siamese network）：训练2个神经网络，分别生成128维向量，根据损失函数输出任意2张图片的差距。

<img src="笔记图保存\PixPin_2025-08-19_10-29-41.png" alt="PixPin_2025-08-19_10-29-41" style="zoom: 50%;" />

**三元损失函数**

三元即分别查看3张图片：Anchor（锚照片）、Positive（正例照片）、Negative（负例照片）

Anchor和Positive：同一人

Anchor和Negative：不同人

<img src="笔记图保存\PixPin_2025-08-19_15-12-24.png" alt="PixPin_2025-08-19_15-12-24" style="zoom: 50%;" />

||f(A) - f(P)||^2 <= ||f(A) - f(N)||^2

公式中，||f(A) - f(P)||^2 可看作 d（A，P）即Anchor和Positive间的距离

||f(A) - f(N)||^2 可看作 d（A，N）即Anchor和Negative间的距离

转换公式，变为 ||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 <= 0，但此时神经网络可能会倾向于将f(A)、 f(P)、f(N)全部输出0，

为防止退化解，这里添加超参数margin：||f(A) - f(P)||^2 - ||f(A) - f(N)||^2  + margin <= 0

整体损失函数：

<img src="笔记图保存\PixPin_2025-08-19_15-46-45.png" alt="PixPin_2025-08-19_15-46-45" style="zoom:50%;" />

此时，训练数据集需要A，P，N的三元组形式，意味着必须需要有些成对的A和P，如果1个人只有1张图片，将无法进行训练。

如果随意寻找A和N，则 d（A，N）一定很大，这个式子很容易得到满足，导致神经网络无法学习有效内容，

因此需寻找较“难”学习的训练数据，也就是||f(A) - f(P)||^2和||f(A) - f(N)||^2需很接近。

在 FaceNet 等系统中，使用的就是“孪生式三元网络”（有时叫 Triplet Network），即三个共享权重的子网络分别处理 A、P、N，然后计算三元损失。严格来说，使用三元损失的网络有时被称为 三胞胎网络（Triplet Network），是孪生网络思想的扩展。

## **神经风格迁移**

**迁移学习的代价函数**

下图中展示了代价函数的构成：

内容损失+风格损失

内容损失：原图C-生成图G

风格损失：风格图S-生成图G

<img src="笔记图保存\PixPin_2025-08-19_16-38-09.png" alt="PixPin_2025-08-19_16-38-09" style="zoom:50%;" />

1.随机生成特定大小图片（充满噪点）

2.利用梯度下降，最小化代价函数，使得图像发生变化

<img src="笔记图保存\PixPin_2025-08-19_16-41-01.png" alt="PixPin_2025-08-19_16-41-01" style="zoom:50%;" />

**内容代价函数**

使用预训练网络，输出中间层（可自选），计算C和G特征图的差异，作为内容损失。

对应矩阵在对应通道上做减法，求平方，做和，每个通道得到一个数值，

算出每个通道的差平方和后，最后再相加，得到一个数值类型标量。

<img src="笔记图保存\PixPin_2025-08-19_16-47-23.png" alt="PixPin_2025-08-19_16-47-23" style="zoom:50%;" />

**风格代价函数**

如何量化1张图的风格？

选择一个中间层输出，计算其不同通道之间的相关性（correlation）

具体如何衡量：将第k个和第k'个通道上的激活元相乘再求和

<img src="笔记图保存\PixPin_2025-08-19_16-55-31.png" alt="PixPin_2025-08-19_16-55-31" style="zoom:50%;" />

最终可以得到一个Gram矩阵，用于衡量一张图片的风格：

<img src="笔记图保存\PixPin_2025-08-19_17-03-51.png" alt="PixPin_2025-08-19_17-03-51" style="zoom:50%;" />

而要定义风格代价函数，对S和G的Gram矩阵作差求平方即可。

对矩阵每个位置的差值平方求和，最终得到一个数值类型标量。

<img src="笔记图保存\PixPin_2025-08-19_17-08-17.png" alt="PixPin_2025-08-19_17-08-17" style="zoom:50%;" />

这里仅选择一个中间层输出进行风格代价函数计算，但实际上可自选多个（如不同的早期层、晚期层以实现风格控制）（同步设置多个权重参数），使结果更优。

# Docker使用

Docker和虚拟机区别：

<img src="笔记图保存\PixPin_2025-08-20_09-06-26.png" alt="PixPin_2025-08-20_09-06-26" style="zoom: 33%;" />

<img src="笔记图保存\PixPin_2025-08-20_09-06-39.png" alt="PixPin_2025-08-20_09-06-39" style="zoom: 33%;" />

镜像可看作容器的安装包，一个镜像可做出多个容器

<img src="笔记图保存\PixPin_2025-08-20_09-08-54.png" alt="PixPin_2025-08-20_09-08-54" style="zoom: 33%;" />

具体细节和安装，见B站视频[40分钟的Docker实战攻略，一期视频精通Docker_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1THKyzBER6/?spm_id_from=333.1387.favlist.content.click&vd_source=f931e43e5bf927b47a1004851b8915f3)

<img src="笔记图保存\PixPin_2025-08-20_10-35-23.png" alt="PixPin_2025-08-20_10-35-23" style="zoom: 33%;" />

```
下载镜像：
docker pull 容器（名字或ID均可，下同）

查看所有下载过的镜像：
docker images
删除镜像：
docker rmi 容器

使用镜像创建并运行容器，后台运行不占用窗口（detached mode）：
docker run 容器 -d
强制删除容器：
docker rm -f 容器

查看容器进程状态（process status）：
docker ps
```

<img src="笔记图保存\PixPin_2025-08-20_15-33-16.png" alt="PixPin_2025-08-20_15-33-16" style="zoom: 33%;" />

```
端口（post）映射：
docker run XXX -p 宿主机端口:容器端口
```

<img src="笔记图保存\PixPin_2025-08-20_15-36-51.png" alt="PixPin_2025-08-20_15-36-51" style="zoom:33%;" />

```
挂载时，使用宿主机目录覆盖容器内目录

绑定挂载卷（volume）：
docker run XXX -v 宿主机目录:容器目录
```

```
命名卷第一次挂载时，使用容器已有内容对宿主机初始化

创建一个存储空间（命名卷挂载）：
docker volume create 卷名
docker run XXX -v 卷名：容器目录

查看命名卷在宿主机中的位置：
docker volume inspect 卷名

查看所有创建过的卷：
docker volume list

删除卷：
docker volume rm 卷名

删除没有任何容器在使用的卷：
docker volume prune -a
```

<img src="笔记图保存\PixPin_2025-08-20_15-38-51.png" alt="PixPin_2025-08-20_15-38-51" style="zoom:33%;" />

```
将配置信息作为环境变量传入：
docker run -e MONGO_INITDB_ROOT_USERNAME=teCh

重命名容器名称（必须唯一）：
docker run --name 自定义 原容器名

进入一个正在运行的容器内部：
docker exec -it 容器ID /bin/sh

控制台进入容器交互，容器停止时删除
docker run -it --rm 容器

容器停止了，立即重启（--unless-stopped：手动停止的不会再重启）
docker run --restart always
```

```
docker run命令，在每次运行时都会产生新的容器，如果只想在原有容器中进行调试，可使用：

docker start 容器
docker stop 容器
```

制作容器：Dockerfile

FastAPI

```python
from fastapi import FastAPI
import uvicorn

app = FastAPI()


# 表示在localhost:8000/，即主页面上获取信息
@app.get("/")
async def root():
    return {"message": "Hello World"}


# 表示在localhost:8000/hello/{name}获取信息，name实时可变
@app.get("/hello/{name}")
async def say_hello(name: str):
    return {"message": f"Hello {name}"}


# 可选，建议在Dockerfile中配置端口及启动，而不是在程序内部
if __name__ == '__main__':
    uvicorn.run(app, host="0.0.0.0", port=8000)

```

requirements.txt

```
fastapi
uvicorn
```

Dockerfile

```
FROM python:3.12-slim

# 类似cd，进行容器目录
WORKDIR /app

# 第1个点：当前目录，第2个点：容器目录，表示将当前目录下所有内容复制到容器中
COPY . .

# 安装依赖包
RUN pip install -r requirements.txt

# 暴露端口（仅说明，不开启映射）
EXPOSE 8000

# 运行 FastAPI 服务
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]

```

```
创建一个容器，“.”表示当前路径：
docker build -t 容器名 .
```

docker compose：打包多个容器联合使用

<img src="笔记图保存\PixPin_2025-08-20_16-38-59.png" alt="PixPin_2025-08-20_16-38-59" style="zoom:33%;" />

```
启动compose中的所有容器：
docker compose up -d

停止/启动：
docker compose stop
docker compose start

删除：
docker compose down

启动特定文件名的compose：
docker composee -f test.yaml up -d

对于更复杂的企业容器管理，则需用到kubernetes
```

# Git使用

## 说明

Git是开源的版本控制系统，常用于项目代码管理、文件管理等场景

可以将Git理解为存储文件的仓库，方便多个用户将文件集中存储到服务器中，或从服务器下载文件副本到本地磁盘。

文件的类型不受限制，可以是代码等文本文件，也可以是图片、视频等媒体文件。

<img src="笔记图保存\PixPin_2025-08-20_08-55-44.png" alt="PixPin_2025-08-20_08-55-44" style="zoom: 67%;" />

Git服务并不是简单地存储文件，它会记录每一次文件修改。用户可翻看历史版本的文件，或者删除某些历史修改以还原文件。

<img src="笔记图保存\PixPin_2025-08-20_08-56-12.png" alt="PixPin_2025-08-20_08-56-12" style="zoom:67%;" />

在实际开发过程中，会经常出现一些功能回退。就是以前好用的，现在却出现BUG的情况。

通过翻看代码文件的历史版本，可较快速地定位哪次修改影响了这个功能，也能知道团队中哪个人做了这次修改。

<img src="笔记图保存\PixPin_2025-08-20_08-57-58.png" alt="PixPin_2025-08-20_08-57-58" style="zoom: 67%;" />

相对于SVN，Git允许分布式部署。另外，Git需要的存储空间也会相对少一些，Git是按元数据存储的，而SVN则是按文件存储。
Git服务是一个系统核心，用户可选择功能更加丰富的平台服务，如GitHub、Gitlab。

Git可以理解为系统核心，是没有界面的。
Gitlab、GitHub是在Git基础上建设的平台，里面包含Git服务，这些平台拥有更加完善的后台管理网站。也拥有更加丰富的功能，如项目管理、版本视图、权限管理等。

所以一般不直接使用或部署Git服务，而是使用功能更加完善的Gitlab、GitHub平台服务，其中Gitlab支持私有化部署。

Docker部署Gitlab

```
下载Gitlab镜像：
docker pull gitlab/gitlab-ce

启动Gitlab容器：
docker volume create gitlab_data

docker run --detach --publish 8080:80 --name gitlab --restart always -e TZ="Asia/Shanghai" -v gitlab_data:/var/opt/gitlab gitlab/gitlab-ce

```

首先是新建、管理远程仓库。虽然Git客户端也可以完成这一操作，但是操作起来比较麻烦，所以一般新建、管理仓库都在服务端提供的管理网站完成。以Gitlab为例，在后台管理页面即可新建远程仓库。

分支是一个远程仓库内的多个独立副本，每个分支都是完全独立互不影响的，包括文件、修改记录等都是完全独立的。
一个远程仓库默认有一个主分支，默认情况下，文件会存储到主分支。用户创建新分支时，需要基于某个修改记录、某个分支、或者某个tag才能创建。

一个分支的多次修改可一次全部更新到别的分支，称为合并。
合并操作实际上是对目标分支提交一次新修改，但是如果目标分支也修改过某个文件，则可能由于修改冲突而失败，这时候也需要通过人工修改文件更新到目标分支。

tag是一个标识，用于标记某个修改记录。
便于归档对应分支下截止到此次修改的文件，防止因为修改记录太多而造成混乱，方便版本迭代管理。
tag与分支是不同的，tag只是标记修改记录方便查看，是不允许更新操作的，而分支是一个独立的副本，允许更新文件。

## 操作

```
复制项目（需要有对应权限账号）：
git clone 链接

github下载慢，添加： https://gitclone.com/github.com/

设置全局HTTP代理
git config --global http.proxy "127.0.0.1:7890"
git config --global https.proxy "127.0.0.1:7890"

关闭全局HTTP代理
git config --global --unset http.proxy
git config --global --unset https.proxy
```

下载后一般是只有main分支（主要功能），需要自己在其他分支上进行修改（扩展功能）

<img src="笔记图保存\PixPin_2025-08-21_16-59-32.png" alt="PixPin_2025-08-21_16-59-32" style="zoom: 50%;" />

<img src="笔记图保存\PixPin_2025-08-21_17-01-09.png" alt="PixPin_2025-08-21_17-01-09" style="zoom:50%;" />

commit和push：

commit：对本地仓库更新

push：对远程仓库更新

后续：先将更新发送到release分支，测试没问题再正式合并到main分支

<img src="笔记图保存\PixPin_2025-08-21_17-19-34.png" alt="PixPin_2025-08-21_17-19-34" style="zoom:50%;" />

为防止在自己提交前，其他人的分支先对代码进行了改动，造成本地代码不是最新的情况（即没有其他人的最新更新），一般在push前会先pull一下

<img src="笔记图保存\PixPin_2025-08-21_17-32-38.png" alt="PixPin_2025-08-21_17-32-38" style="zoom: 33%;" />

<img src="笔记图保存\PixPin_2025-08-21_17-38-02.png" alt="PixPin_2025-08-21_17-38-02" style="zoom:50%;" />

如果main（即git clone的代码）分支，在远程已经发生了变化？

merge/rebase：自己的分支需要pull新代码用rebase；公共分支pull代码用merge，一般都用merge。

<img src="笔记图保存\PixPin_2025-08-21_17-45-50.png" alt="PixPin_2025-08-21_17-45-50" style="zoom:50%;" />

<img src="笔记图保存\PixPin_2025-08-21_18-02-58.png" alt="PixPin_2025-08-21_18-02-58" style="zoom:50%;" />

<img src="笔记图保存\PixPin_2025-08-21_18-03-20.png" alt="PixPin_2025-08-21_18-03-20" style="zoom:50%;" />

# Hugging Face

## 确认模型

找Checkpoints（即模型参数加载），一般存放在pytorch_model.bin（查看说明寻找位置），将对应大小*1.2即运行对应模型所需的显存量。

<img src="笔记图保存\PixPin_2025-08-24_08-52-35.png" alt="PixPin_2025-08-24_08-52-35" style="zoom:50%;" />

在页面可知晓如何基于Transformer库加载对应模型

<img src="笔记图保存\PixPin_2025-08-24_08-50-50.png" alt="PixPin_2025-08-24_08-50-50" style="zoom: 33%;" />

```
Transformer库版本：pip install transformers==4.41.2

配置下载路线环境变量：
变量名：HF_ENDPOINT
变量值：https://hf-mirror.com

变量名：HF_HOME
变量值：E:\Python312\hugging_face_hub

Linux：
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = '/home/gjk/hugging_face_cache'
```

## NLP

**流程**

1. 原始文本输入
   ↓
2. 文本清洗与标准化（轻量级预处理）
   ↓
3. 分词（Tokenizer） + 向量化（Embedding）
   ↓
4. 输入预训练模型（如 BERT）
   ↓
5. 微调（Fine-tuning）或提示（Prompting）
   ↓
6. 输出预测结果（分类、实体、生成等）
   ↓
7. 部署与推理（API、服务化）

**对话**

```python
from transformers import pipeline
from transformers import Conversation
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 1.创建一个pipline，明确任务和模型
chatbot = pipeline(task="conversational",
                   # model="Qwen/Qwen2.5-72B-Instruct",
                   model= "facebook/blenderbot-400M-distill",
                   device=device)

# 2.创建一个对话并加载用户信息
user_messages = [
    {"role": "user", "content": "Who are you?"},
]
conversation = Conversation(user_messages)

# 3.将对话通过pipline，打印回复
conversation = chatbot(conversation)
print(conversation)

# 继续对话，聊天机器人可能会提供不相关的回应，因为它不记得任何之前的对话。
# 为将之前的对话包含在 LLM 的上下文中，可以使用“add message”以包含之前的聊天历史记录。
conversation.add_message(
    {
        "role": "user",
        "content": "yes, and i like your dog"
    }
)
conversation = chatbot(conversation)
print(conversation)

```

**翻译与总结**

```python
from transformers import pipeline
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 翻译文本
translator = pipeline(task="translation",
                      model="facebook/nllb-200-distilled-600M",
                      #  压缩模型
                      torch_dtype=torch.bfloat16,
                      device=device)
text = "我操"

# 翻译需输入原始和目标语言，具体见官方模型介绍
user_messages = translator(text, src_lang='zho_Hans', tgt_lang='eng_Latn')
print(user_messages)

# 清理内存
import gc

del translator
gc.collect()

# 总结文本
summarizer = pipeline(task="summarization",
                      model="facebook/bart-large-cnn",
                      torch_dtype=torch.bfloat16,
                      device=device)

text = """Paris is the capital and most populous city of France, with
          an estimated population of 2,175,601 residents as of 2018,
          in an area of more than 105 square kilometres (41 square
          miles). The City of Paris is the centre and seat of
          government of the region and province of Île-de-France, or
          Paris Region, which has an estimated population of
          12,174,880, or about 18 percent of the population of France
          as of 2017."""

summary = summarizer(text, min_length=10, max_length=100)
print(summary)

del summary
gc.collect()

```

**句子相似性**

句子相似度模型将输入文本转换为向量，或所谓的嵌入(embedding)

```python
import torch
from sentence_transformers import SentenceTransformer
from sentence_transformers import util

device = 'cuda' if torch.cuda.is_available() else 'cpu'

model = SentenceTransformer("all-MiniLM-L6-v2", device=device)

sentences1 = ['The cat sits outside',
              'A man is playing guitar',
              'The movies are awesome']
sentences2 = ['The dog plays in the garden',
              'A woman watches TV',
              'The new movie is so great']

# 1.将句子转换为嵌入向量
embeddings1 = model.encode(sentences1, convert_to_tensor=True)
embeddings2 = model.encode(sentences2, convert_to_tensor=True)

# 2.计算嵌入向量之间的相似性（cos）
cosine_scores = util.cos_sim(embeddings1, embeddings2)

# 打印2个句子配对向量间的相似性
for i in range(len(sentences1)):
    print("{} \t\t {} \t\t Score: {:.4f}".format(sentences1[i],
                                                 sentences2[i],
                                                 cosine_scores[i][i]))

```

## 音频

**零样本音频分类**

声音在一段时间内的数值是无限的（或者说是连续的），人类使用麦克风等设备对声音按一定的频率进行采样（计算机所能处理的数值是离散的）。

但Transformer模型对音频的采样率，在训练时就是固定的，它无法处理不同输入长度（采样率不同）的音频数据。

<img src="笔记图保存\PixPin_2025-08-25_16-39-55.png" alt="PixPin_2025-08-25_16-39-55" style="zoom:33%;" />

```python
"""
pip install datasets==2.17.0
pip install soundfile
pip install librosa
pip install sounddevice
注意ffmpeg的安装（下载后添bin目录环境变量即可），用于音频解码
"""
import torch
from datasets import load_dataset
import sounddevice as sd
from transformers import pipeline
from datasets import Audio

# 加载数据集（下载前10条，加载第1条）
dataset = load_dataset("ashraq/esc50", split="train[0:10]")
audio_sample = dataset[0]
print(audio_sample)

# 获取声音数组和采样率
audio_array = audio_sample["audio"]["array"]
sampling_rate = audio_sample["audio"]["sampling_rate"]

# 播放狗叫
sd.play(audio_array, samplerate=sampling_rate)
sd.wait()

# 加载0样本音频分类模型（无需微调）
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
zero_shot_classifier = pipeline(task="zero-shot-audio-classification", model="laion/clap-htsat-unfused", device=device)

# 对比模型训练和数据样本采样率
print(zero_shot_classifier.feature_extractor.sampling_rate)
print(sampling_rate)
# 利用datasets.Audio，将数据样本转为模型sampling_rate，读取第1个样本并查看信息
dataset = dataset.cast_column("audio", Audio(sampling_rate=48_000))
audio_sample = dataset[0]
print(audio_sample)

# clap接收文本或者音频格式，并计算2者之间的相似度
# 候选标签
candidate_labels = ["Sound of a dog", "Sound of vacuum cleaner"]
# 计算候选标签与声音间的相似度
classifier = zero_shot_classifier(audio_sample["audio"]["array"], candidate_labels=candidate_labels)
print(classifier)

```

**自动语音识别**

```python
import torch
from datasets import load_dataset, Audio
from transformers import pipeline
import numpy as np
import librosa

"""
对语音集设置使用流式处理，数据不会一次性全部下载并加载到内存中，而是按需逐个加载样本，避免内存溢出
返回的是一个可迭代的数据流（IterableDataset），而不是传统的随机访问Dataset
但此处设置False，因为网速过慢
"""
dataset = load_dataset("PolyAI/minds14", "en-US", split="train", streaming=False)
example = dataset[0]
print(example)

# 加载语音识别模型
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
asr = pipeline(task="automatic-speech-recognition",
               model="distil-whisper/distil-small.en",
               device=device)

# 先对比采样率
print(asr.feature_extractor.sampling_rate)
print(example['audio']['sampling_rate'])
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))
example = dataset[0]
# 这里有些不一样，因为该数据集侧重于意图的简化版转录文本，不一定是逐字逐句的真实语音内容。
print(asr(example["audio"]["array"]))
print(example["transcription"])

"""
假设一段音频存在问题：
1多空间通道
2采样率与模型不同
3时长超过模型输入长度
"""
audio = example["audio"]["array"]
sampling_rate = example["audio"]["sampling_rate"]
print(audio.shape)
# 1：先将数组转置，形成（通道、样本）格式
audio_transposed = np.transpose(audio)
# 利用librosa转为单空间通道
audio_mono = librosa.to_mono(audio_transposed)

# 2：利用librosa修改样本采样率
audio_16KHz = librosa.resample(audio_mono,
                               orig_sr=sampling_rate,
                               target_sr=16000)

# 3：对于时长超过模型输入长度的问题，将输入划分为多个chunk（可重叠），
# 分别处理后再合并结果，batch_size表示1次并行处理多少chunk
print(asr(
    audio_16KHz,
    chunk_length_s=30,
    batch_size=4,
    return_timestamps=True,
)["chunks"])

```

<img src="笔记图保存\PixPin_2025-08-26_18-06-31.png" alt="PixPin_2025-08-26_18-06-31" style="zoom:50%;" />

**文本转语音**

```python
"""
pip install timm
pip install inflect
pip install phonemizer
注意espeak的安装，这需要linux系统
"""
import torch
from transformers import pipeline

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
narrator = pipeline("text-to-speech", model="kakao-enterprise/vits-ljs", device=device)

text = "man, what can i say? mamba out!"
narrated_text = narrator(text)

```

## CV

**物体检测**

```python
"""
pip install gradio
pip install timm
pip install inflect
pip install phonemizer
pip install helper
"""

from transformers import pipeline
from PIL import Image, ImageDraw, ImageFont
from matplotlib import pyplot as plt
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 加载模型
od_pipe = pipeline("object-detection", "facebook/detr-resnet-50", device=device)


# 可视化：在图像上绘制检测结果
def show_result_pic(raw_image):
    # # 加载图像
    # print(image)
    # raw_image = Image.open(image)
    raw_image.resize((569, 491))

    # 将图像放入模型
    pipeline_output = od_pipe(raw_image)

    # 创建一个可绘制的图像副本
    image_with_boxes = raw_image.copy()
    draw = ImageDraw.Draw(image_with_boxes)
    for result in pipeline_output:
        label = result['label']
        score = result['score']
        box = result['box']
        # 提取坐标
        x1, y1, x2, y2 = box['xmin'], box['ymin'], box['xmax'], box['ymax']
        # 绘制矩形框
        draw.rectangle([x1, y1, x2, y2], outline='green', width=2)
        # 绘制标签和置信度
        text = f"{label}: {score:.2f}"
        # 使用 textbbox 获取文本大小
        draw.text((x1, y1), text, fill='red', font=ImageFont.truetype("arial.ttf", 16))
    # 显示图像
    # plt.figure(dpi=600)
    # plt.imshow(image_with_boxes)
    # plt.axis('off')
    # plt.show()
    return image_with_boxes


# 利用Gradio做一个简单的应用
import os
import gradio as gr

if __name__ == '__main__':
    demo = gr.Interface(
        fn=show_result_pic,
        inputs=gr.Image(label="Input image",
                        type="pil"),
        outputs=gr.Image(label="Output image with predicted instances",
                         type="pil")
    )
    demo.launch(share=True)
    demo.close()

```

**图像分割**

图像分割（Image Segmentation）通常分为三类：

1. **语义分割（Semantic Segmentation）**：为每个像素分配类别标签，不区分实例。
2. **实例分割（Instance Segmentation）**：区分不同对象实例，并为每个实例生成掩码。
3. **全景分割（Panoptic Segmentation）**：结合语义和实例分割，统一处理“可数对象”和“不可数区域”。

**Mask Generation**

**Mask Generation 是什么？**
Mask Generation 的核心是“生成一个描述目标区域的掩码”。它可以应用于多种场景，例如：

给定一个提示（prompt），如点、框、文本，生成对应的物体掩码。
自动为图像中所有显著对象生成掩码（即全景分割或实例分割的一部分）。
交互式分割：用户点击某个点，模型生成该点所在物体的掩码。
它不一定是“先检测再分割”的流程，而更强调从提示出发直接生成掩码。

1. 图像先通过图像编码器提取特征（一次性计算，可缓存）
2. 用户提供prompt（如点击一个点）
3. 提示编码器将prompt编码为向量
4. 掩码解码器融合图像特征和prompt，生成对应的掩码
5. 可输出多个可能的掩码（如一个prompt可能对应多个合理解释）

SAM：可传入多个点、或者预测框的形式。一个prompt可产生多个mask（SAM中是3个），筛选其中最高以获得整个图像中最相关的分割mask。和普通的分割任务不同，各像素生成没有任何label，获取的是像素对于用户prompt的置信度分数。

```python
import numpy as np
import torch
from transformers import pipeline
from helper import create_colored_segmentation, show_mask_on_image

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

"""
    Mask Generation（整体，无prompt）
"""
sam_pipe = pipeline("mask-generation",
                    "Zigeng/SlimSAM-uniform-77", device=device)

from PIL import Image

raw_image = Image.open('old-man.jpg')
raw_image.resize((720, 375))

# points_per_batch=32 控制每次处理多少个点提示（影响速度与显存）
output = sam_pipe(raw_image, points_per_batch=32)

# 显示掩码
image_array = np.array(raw_image)  # (H, W, 3), uint8
masks = output["masks"]  # List of (H, W) bool or float arrays
scores = output["scores"]  # List of float scores
colored_seg = create_colored_segmentation(masks, image_array.shape)

"""
    Mask Generation（有单个prompt）
"""
from transformers import SamModel, SamProcessor

# 导入模型
model = SamModel.from_pretrained("Zigeng/SlimSAM-uniform-77")
processor = SamProcessor.from_pretrained("Zigeng/SlimSAM-uniform-77")
raw_image.resize((720, 375))

# 指定prompt点
input_points = [[[1360, 726]]]
inputs = processor(raw_image, input_points=input_points, return_tensors="pt")

# 获取输出
import torch

with torch.no_grad():
    outputs = model(**inputs)
predicted_masks = processor.image_processor.post_process_masks(
    outputs.pred_masks,
    inputs["original_sizes"],
    inputs["reshaped_input_sizes"]
)

# 分别展示生成的3个mask
predicted_mask = predicted_masks[0]
print(predicted_mask.shape)
for i in range(3):
    show_mask_on_image(raw_image, predicted_mask[:, i], title="Segmentation", add_outline=True)

```

helper.py

```python
import matplotlib.pyplot as plt
import numpy as np
from PIL import Image


def show_mask_on_image(
        image,
        mask,
        alpha=0.5,
        cmap='jet',
        show_colorbar=False,
        title="Segmentation Mask",
        figsize=(8, 6),
        add_outline=False,
        outline_color="white",
        show_plot=True
):
    """
    在原图上叠加并显示分割掩码

    参数：
    - image: PIL.Image 或 numpy array (H, W, 3)，原始图像
    - mask: numpy array 或 torch.Tensor (H, W)，bool 或 0/1，分割掩码
    - alpha: float，掩码透明度 (0~1)
    - cmap: str，颜色映射，如 'jet', 'viridis', 'plasma'
    - show_colorbar: bool，是否显示颜色条
    - title: str，图像标题
    - figsize: tuple，画布大小
    - add_outline: bool，是否添加边缘轮廓
    - outline_color: str 或 tuple，轮廓颜色
    - show_plot: bool，是否立即显示（设为 False 可用于保存）
    """
    # 转换图像为 numpy array
    if isinstance(image, Image.Image):
        image = np.array(image)

    # 转换 mask 为 numpy，并确保是 float 类型
    if isinstance(mask, np.ndarray):
        mask = mask.astype(np.float32)
    else:
        mask = mask.cpu().numpy().astype(np.float32)

    if mask.ndim == 3:
        mask = mask.squeeze()  # 去掉 channel 维度

    # 创建画布
    plt.figure(figsize=figsize)
    plt.imshow(image)
    plt.imshow(mask, cmap=cmap, alpha=alpha)

    if show_colorbar:
        plt.colorbar(fraction=0.046, pad=0.04)

    # 添加轮廓（可选）
    if add_outline:
        from scipy import ndimage
        # 提取边缘：膨胀后减去原图
        mask_bool = mask > 0.5
        dilated = ndimage.binary_dilation(mask_bool, iterations=2)
        outline = dilated ^ mask_bool  # 异或得到边缘
        plt.contour(outline, colors=outline_color, linewidths=1.2)

    plt.title(title, fontsize=14, weight='bold')
    plt.axis('off')
    plt.tight_layout()

    if show_plot:
        plt.show()


def show_mask(mask, ax, random_color=False):
    """在轴上显示单个掩码"""
    if random_color:
        color = np.random.random(3)
    else:
        color = np.array([255 / 255, 133 / 255, 133 / 255])  # 粉红色
    h, w = mask.shape
    mask_image = np.zeros((h, w, 4), dtype=np.float32)
    mask_image[:, :, :3] = color.reshape(1, 1, 3)
    mask_image[:, :, 3] = mask * 0.5  # 透明度
    ax.imshow(mask_image)


def show_masks_on_image(image, masks, scores, num_show=10):
    """显示原图 + 前 N 个最高分的掩码"""
    plt.figure(figsize=(12, 12))
    plt.imshow(image)
    plt.title("SAM Generated Masks", fontsize=16)

    # --- 安全处理 scores 和排序 ---
    if isinstance(scores, (int, float)):
        # 如果 scores 是单个数值（只有一个 mask）
        scores = [scores]
    elif isinstance(scores, np.ndarray):
        scores = scores.tolist()
    elif not isinstance(scores, list):
        scores = list(scores)  # 尝试转为 list

    # 确保 masks 也是 list
    if isinstance(masks, np.ndarray):
        masks = [masks[i] for i in range(len(masks))]  # 假设 masks 是 (N, H, W)

    # 按得分从高到低排序
    try:
        sorted_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)
    except Exception as e:
        print(f"排序失败: {e}")
        sorted_indices = list(range(len(scores)))

    # 取前 num_show 个
    top_indices = sorted_indices[:num_show]

    # 颜色池
    colors = [
        (255 / 255, 133 / 255, 133 / 255), (30 / 255, 144 / 255, 255 / 255),
        (0, 255 / 255, 0), (1, 0.5, 0), (0.6, 0.3, 0.8)
    ]

    for idx, i in enumerate(top_indices):
        mask = masks[i]
        color = colors[idx % len(colors)]
        h, w = mask.shape
        mask_image = np.zeros((h, w, 4), dtype=np.float32)
        mask_image[:, :, :3] = color
        mask_image[:, :, 3] = mask.astype(np.float32) * 0.5  # 透明度
        plt.imshow(mask_image)

        # 添加分数标签
        y_coords, x_coords = np.where(mask)
        if len(y_coords) > 0 and len(x_coords) > 0:
            y, x = np.min(y_coords), np.min(x_coords)
            plt.text(x, y - 5, f'{scores[i]:.2f}',
                     color='white', fontsize=8, weight='bold',
                     bbox=dict(facecolor='red', alpha=0.6, edgecolor='none', boxstyle='round,pad=0.2'))

    plt.axis('off')
    plt.tight_layout()
    plt.show()


def create_colored_segmentation(masks, image_shape):
    """将多个掩码合并为一张彩色分割图"""
    h, w = image_shape[:2]
    colored_seg = np.zeros((h, w, 3), dtype=np.uint8)
    used_mask = np.zeros((h, w), dtype=bool)

    colors = [
        (255, 0, 0), (0, 255, 0), (0, 0, 255),
        (255, 255, 0), (255, 0, 255), (0, 255, 255),
        (128, 0, 0), (0, 128, 0), (0, 0, 128), (128, 128, 0)
    ]

    for i, mask in enumerate(masks):
        color = colors[i % len(colors)]
        # 只绘制未重叠区域（避免覆盖）
        mask = mask & (~used_mask)
        if mask.sum() > 0:
            colored_seg[mask] = color
            used_mask = used_mask | mask

    plt.figure(figsize=(10, 7))
    plt.imshow(colored_seg)
    plt.title("Colored Segmentation (Non-overlapping)")
    plt.axis('off')
    plt.show()

```

**Depth Estimation**

```python
import torch
from PIL import Image
from transformers import pipeline

# 加载模型
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
depth_estimator = pipeline(task="depth-estimation", model="Intel/dpt-hybrid-midas", device=device)


def show_result_pic(raw_image):
    raw_image.resize((806, 621))
    output = depth_estimator(raw_image)

    print(output["predicted_depth"].shape)
    print(output["predicted_depth"].unsqueeze(1).shape)
    print(raw_image.size[::-1])

    # 将输出转换至原图W和H
    prediction = torch.nn.functional.interpolate(
        output["predicted_depth"].unsqueeze(1),
        size=raw_image.size[::-1],
        mode="bicubic",
        align_corners=False,
    )
    print(prediction.shape)

    # 将图像再次转换为数组，进行数值归一化，并根据数组生成图像
    import numpy as np
    output = prediction.squeeze().numpy()
    formatted = (output * 255 / np.max(output)).astype("uint8")
    depth_img = Image.fromarray(formatted)

    return depth_img

    # # 展示图像
    # import matplotlib.pyplot as plt
    # plt.figure(figsize=(8, 6))
    # plt.imshow(depth_img)
    # plt.title("My Image")
    # plt.axis('off')  # 关闭坐标轴
    # plt.show()


# 利用Gradio做一个简单的应用
import gradio as gr

if __name__ == '__main__':
    demo = gr.Interface(
        fn=show_result_pic,
        inputs=gr.Image(label="Input image",
                        type="pil"),
        outputs=gr.Image(label="Output image",
                         type="pil")
    )
    demo.launch(share=True)
    demo.close()

```

## 多模态

即当模型输入不止一种类型

<img src="笔记图保存\PixPin_2025-08-27_10-39-56.png" alt="PixPin_2025-08-27_10-39-56" style="zoom:50%;" />

**图像检索**

```python
from transformers import BlipForImageTextRetrieval
from transformers import AutoProcessor
from PIL import Image
import torch

# 导入模型
model = BlipForImageTextRetrieval.from_pretrained(
    "Salesforce/blip-itm-base-coco").to("cuda")
processor = AutoProcessor.from_pretrained(
    "Salesforce/blip-itm-base-coco")

# 读取图像和文本
raw_image = Image.open('old-man.jpg').convert('RGB')
text = "a man and a bike"

# 将图像和文本放入processor，再放入model，获取输出结果
inputs = processor(images=raw_image,
                   text=text,
                   return_tensors="pt").to("cuda")
"""
model(**inputs)
==model(input_ids=tensor1, attention_mask=tensor2, token_type_ids=tensor3)
** 是用来解包字典（dictionary）的。
**inputs 把字典中的键值对“展开”成函数的 keyword arguments（关键字参数）。
"""
itm_scores = model(**inputs)[0]

# 通过softmax以可解释的方式获取图像和文本的相似分数
itm_score = torch.nn.functional.softmax(itm_scores, dim=1)
print(f"""\
The image and text are matched \
with a probability of {itm_score[0][1]:.4f}""")

```

**图像字幕**

```python
from transformers import BlipForConditionalGeneration
from transformers import AutoProcessor
from PIL import Image

model = BlipForConditionalGeneration.from_pretrained(
    "Salesforce/blip-image-captioning-base").to("cuda")
processor = AutoProcessor.from_pretrained(
    "Salesforce/blip-image-captioning-base")

image = Image.open('old-man.jpg').convert('RGB')

# 可自行设定句子开头作为引导
text = "a photograph of"
inputs = processor(image, text, return_tensors="pt").to("cuda")
out = model.generate(**inputs)
print(processor.decode(out[0], skip_special_tokens=True))

# 也可不设定引导
inputs = processor(image, return_tensors="pt").to("cuda")
out = model.generate(**inputs)
print(processor.decode(out[0], skip_special_tokens=True))

```

**多模态视觉问答**

```python
from transformers import BlipForQuestionAnswering
from transformers import AutoProcessor
from PIL import Image

model = BlipForQuestionAnswering.from_pretrained(
    "Salesforce/blip-vqa-base").to("cuda")
processor = AutoProcessor.from_pretrained(
    "Salesforce/blip-vqa-base")

image = Image.open('old-man.jpg')

question = "how many bikes are in the picture?"

inputs = processor(image, question, return_tensors="pt").to("cuda")
out = model.generate(**inputs)

print(processor.decode(out[0], skip_special_tokens=True))

```

**零样本图像分类**

```python
"""
CLIP模型学习的是：输入文本和每个类别语义的匹配程度。
"""

from transformers import CLIPModel
from transformers import AutoProcessor
from PIL import Image

# 获取模型
model = CLIPModel.from_pretrained(
    "openai/clip-vit-large-patch14").to("cuda")
processor = AutoProcessor.from_pretrained(
    "openai/clip-vit-large-patch14")

# 获取图像和列表
image = Image.open("old-man.jpg")
labels = ["a photo of a cat", "a photo of a bike", "a photo of a man"]

# 输入输出
inputs = processor(text=labels,
                   images=image,
                   return_tensors="pt",
                   padding=True).to("cuda")
outputs = model(**inputs)

# 将输出通过softmax转换为百分比
print(outputs.logits_per_image)
probs = outputs.logits_per_image.softmax(dim=1)[0]
print(probs)

probs = list(probs)
for i in range(len(labels)):
    print(f"label: {labels[i]} - probability of {probs[i].item():.4f}")

```

# 模型轻量化部署

## 架构设计

设计模型架构以减少参数量。如深度可分离卷积架构，通过在2层之间加入额外层，使N×M变为N×K+K×M。

<img src="笔记图保存\PixPin_2025-08-27_19-29-08.png" alt="PixPin_2025-08-27_19-29-08" style="zoom:50%;" />

## 剪枝（Pruning）

在模型中删除一些不重要的权重或神经元，使得模型更小，流程：

1.评估模型剪枝处性能

2.去除剪枝处

3.微调后重新评估剪枝处性能（循环再次微调模型，每次减少模型参数量）

<img src="笔记图保存\PixPin_2025-08-27_17-49-45.png" alt="PixPin_2025-08-27_17-49-45" style="zoom:50%;" />

针对以权重为单位的剪枝，在实战中遇到问题：

Pytorch一般是先定义好网络架构，GPU也无法加速不规则网络架构的矩阵乘法，

如考虑网络架构完整性，将对应位置补0，但实际上模型大小还是没变化。

<img src="笔记图保存\PixPin_2025-08-27_18-04-26.png" alt="PixPin_2025-08-27_18-04-26" style="zoom:50%;" />

考虑以神经元为单位的剪枝，对实战来说比较合理：

<img src="笔记图保存\PixPin_2025-08-27_18-09-43.png" alt="PixPin_2025-08-27_18-09-43" style="zoom:50%;" />

## 知识蒸馏（Knowledge Distillation）

这里Student输入和Teacher输入相同。

Teacher先训练，将Teacher输出结果给Student作为参考，让Student的输出逼近Teacher输出。

<img src="笔记图保存\PixPin_2025-08-27_18-59-37.png" alt="PixPin_2025-08-27_18-59-37" style="zoom:50%;" />

Softmax：加个超参数T（Temperature），让输出更平滑，不会过分集中于某个类别。

<img src="笔记图保存\PixPin_2025-08-27_19-14-16.png" alt="PixPin_2025-08-27_19-14-16" style="zoom:50%;" />

## 量化（Quantization）

### 量化基础

常见方法：

1.使用比较少的空间来存储一个参数（例如将64bit的参数使用8bit存储）

2.对参数聚类后存储（例如将对应区域内参数设置为类均值）

<img src="笔记图保存\PixPin_2025-08-27_19-20-00.png" alt="PixPin_2025-08-27_19-20-00" style="zoom:50%;" />

3.Huffman Encoding（常见的更多参数，少见的更少参数）



对于量化，可以量化模型权重，或者每层中输出的激活值。

<img src="笔记图保存\PixPin_2025-08-27_20-05-33.png" alt="PixPin_2025-08-27_20-05-33" style="zoom:50%;" />

<img src="笔记图保存\PixPin_2025-09-09_09-12-56.png" alt="PixPin_2025-09-09_09-12-56" style="zoom:50%;" />

- 1 字节（byte） = 8 位（bits）
- Float32占用4个字节，因此总共需要 32 bit 
- 而INT8仅占用1个字节，因此只需要8 bit
- 量化的主要问题，是降低量化所带来的模型误差

<img src="笔记图保存\PixPin_2025-08-27_20-13-43.png" alt="PixPin_2025-08-27_20-13-43" style="zoom:50%;" />

整形：使用补码表示法，无符号8位（0,255），有符号8位（-128,127），范围公式和对应计算方式如下

<img src="笔记图保存\PixPin_2025-08-28_08-55-27.png" alt="PixPin_2025-08-28_08-55-27" style="zoom:50%;" />

进行加法运算时，会向左进位，但因为只有4位导致最左边一位被舍弃

<img src="笔记图保存\PixPin_2025-08-28_08-57-13.png" alt="PixPin_2025-08-28_08-57-13" style="zoom:50%;" />

Pytorch整型对应列表：

<img src="笔记图保存\PixPin_2025-08-28_08-59-25.png" alt="PixPin_2025-08-28_08-59-25" style="zoom:50%;" />

浮点数（Floating Point）包含三个组成部分：

- 符号位（Sign）：正/负（始终为 1 位）
- 指数位（Exponent）（决定范围）：影响数值的表示范围
- 尾数位（Fraction）（决定精度）：影响数值的精度

FP32、BF16、FP16、FP8 是具有特定位数用于指数位和尾数位的浮点数格式。

FP32：

<img src="笔记图保存\PixPin_2025-08-28_09-42-13.png" alt="PixPin_2025-08-28_09-42-13" style="zoom:50%;" />

BF16：

<img src="笔记图保存\PixPin_2025-08-28_09-43-00.png" alt="PixPin_2025-08-28_09-43-00" style="zoom:50%;" />

FP16：

<img src="笔记图保存\PixPin_2025-08-28_09-45-19.png" alt="PixPin_2025-08-28_09-45-19" style="zoom:50%;" />

整体比较：

<img src="笔记图保存\PixPin_2025-08-28_09-49-38.png" alt="PixPin_2025-08-28_09-49-38" style="zoom:50%;" />

Pytorch浮点型对应列表：

<img src="笔记图保存\PixPin_2025-08-28_09-52-38.png" alt="PixPin_2025-08-28_09-52-38" style="zoom:50%;" />

```python
import torch

# 查看类型信息
print(torch.iinfo(torch.int64))
print(torch.iinfo(torch.int32))
print(torch.iinfo(torch.int16))
print(torch.iinfo(torch.int8))
print(torch.iinfo(torch.uint8))

# 默认存储float为fp64（pytorch默认使用fp32，numpy默认使用fp64）
value = 1 / 3
print(format(value, '.60f'))

# 将value设置为不同浮点数并查看数值
tensor_fp64 = torch.tensor(value, dtype=torch.float64)
tensor_fp32 = torch.tensor(value, dtype=torch.float32)
tensor_fp16 = torch.tensor(value, dtype=torch.float16)
tensor_bf16 = torch.tensor(value, dtype=torch.bfloat16)
print(f"fp64 tensor: {format(tensor_fp64.item(), '.60f')}")
print(f"fp32 tensor: {format(tensor_fp32.item(), '.60f')}")
print(f"fp16 tensor: {format(tensor_fp16.item(), '.60f')}")
print(f"bf16 tensor: {format(tensor_bf16.item(), '.60f')}")

# 查看类型信息，注意是finfo而不是iinfo
print(torch.finfo(torch.float64))
print(torch.finfo(torch.float32))
print(torch.finfo(torch.float16))
print(torch.finfo(torch.bfloat16))

# 向下类型转换（Downcasting）对数值的影响
tensor_fp32 = torch.rand(1000, dtype=torch.float32)
print(tensor_fp32[:5])
tensor_fp32_to_bf16 = tensor_fp32.to(torch.bfloat16)
print(tensor_fp32_to_bf16[:5])

# Downcasting对矩阵乘法的影响
m_float32 = torch.dot(tensor_fp32, tensor_fp32)
m_bfloat16 = torch.dot(tensor_fp32_to_bf16, tensor_fp32_to_bf16)
print(m_float32)
print(m_bfloat16)

```

### 向下类型转换（Downcasting）

模型参数即模型权重，将以某种数据类型进行存储。

<img src="笔记图保存\PixPin_2025-08-28_10-24-39.png" alt="PixPin_2025-08-28_10-24-39" style="zoom: 67%;" />

```python
import torch
from helper import DummyModel
from copy import deepcopy

# 构造一个虚构模型
model = DummyModel()
print(model)


# 打印模型信息
"""
一旦类继承了 nn.Module，它就会自动拥有以下与参数管理相关的方法：
model.parameters()：返回一个迭代器，遍历模型中所有可训练参数（Parameter 对象）。
model.named_parameters()：返回一个迭代器，遍历模型中所有参数，同时返回参数的名称（字符串）和对应的 Parameter 对象。
"""
def print_param_dtype(model):
    for name, param in model.named_parameters():
        print(f"{name} is loaded in {param.dtype}")


print_param_dtype(model)

# 为进行参数转换，以下方式均可
# target_dtype = torch.float16 #torch.bfloat16
# model = model.to(target_dtype)
# model = model.half()
# model = model.bfloat16()
# 转换为fp16
model_fp16 = model.half()
print_param_dtype(model_fp16)

# 对比fp32和fp16的输出
dummy_input = torch.LongTensor([[1, 0], [0, 1]])
logits_fp32 = model(dummy_input)
logits_fp16 = model_fp16(dummy_input)
print(logits_fp32)
print(logits_fp16)

# 尝试复制一份fp16，转为bf16
model_bf16 = deepcopy(model_fp16)
model_bf16 = model_bf16.to(torch.bfloat16)
logits_bf16 = model_bf16(dummy_input)
print(logits_bf16)

# 对比输出差距
mean_diff = torch.abs(logits_bf16 - logits_fp32).mean().item()
max_diff = torch.abs(logits_bf16 - logits_fp32).max().item()
print(f"Mean diff: {mean_diff} | Max diff: {max_diff}")

"""
在HuggingFace库中，量化模型只需要
model_bf16 = BlipForConditionalGeneration.from_pretrained(
                                               model_name,
                               torch_dtype=torch.bfloat16
)即可
"""

# 以上方式有个缺点，需要先以fp32加载模型，再转换为bf16，能否直接以bf16加载？使用torch.set_default_dtype
desired_dtype = torch.bfloat16
torch.set_default_dtype(desired_dtype)
dummy_model_bf16 = DummyModel()
print_param_dtype(dummy_model_bf16)
# 记得加载完模型后，修改回默认的fp32，以免发生错误
torch.set_default_dtype(torch.float32)

```

helper.py

```python
import torch
import torch.nn as nn
from PIL import Image

import warnings

# Ignore specific UserWarnings related to max_length in transformers
warnings.filterwarnings("ignore",
                        message=".*Using the model-agnostic default `max_length`.*")


class DummyModel(nn.Module):

    def __init__(self):
        super().__init__()

        # 设置随机种子
        torch.manual_seed(123)

        # 设置嵌入层
        self.token_embedding = nn.Embedding(2, 2)

        # Block 1
        self.linear_1 = nn.Linear(2, 2)
        self.layernorm_1 = nn.LayerNorm(2)

        # Block 2
        self.linear_2 = nn.Linear(2, 2)
        self.layernorm_2 = nn.LayerNorm(2)

        self.head = nn.Linear(2, 2)

    def forward(self, x):
        hidden_states = self.token_embedding(x)

        # Block 1
        hidden_states = self.linear_1(hidden_states)
        hidden_states = self.layernorm_1(hidden_states)

        # Block 2
        hidden_states = self.linear_2(hidden_states)
        hidden_states = self.layernorm_2(hidden_states)

        logits = self.head(hidden_states)
        return logits


def get_generation(model, processor, image, dtype):
    inputs = processor(image, return_tensors="pt").to(dtype)
    out = model.generate(**inputs)
    return processor.decode(out[0], skip_special_tokens=True)


def load_image(img):
    image = Image.open(img)

    return image

```

### 线性量化（Linear Quantization）

- **Downcasting**：全程使用低精度类型计算 → 快但精度易损（对数据直接截断），不支持整型，一般不使用。
- **Linear Quantization**：存储用低精度，计算前还原为FP32 → 保持精度，支持更小类型（如 int8），更适合实际部署。

helper.py

```python
import torch

# ################ monkey patch for quanto
def named_module_tensors(module, recurse=False):
    for named_parameter in module.named_parameters(recurse=recurse):
      name, val = named_parameter
      flag = True
      if hasattr(val,"_data") or hasattr(val,"_scale"):
        if hasattr(val,"_data"):
          yield name + "._data", val._data
        if hasattr(val,"_scale"):
          yield name + "._scale", val._scale
      else:
        yield named_parameter

    for named_buffer in module.named_buffers(recurse=recurse):
      yield named_buffer

def dtype_byte_size(dtype):
    """
    Returns the size (in bytes) occupied by one parameter of type `dtype`.
    """
    import re
    if dtype == torch.bool:
        return 1 / 8
    bit_search = re.search(r"[^\d](\d+)$", str(dtype))
    if bit_search is None:
        raise ValueError(f"`dtype` is not a valid dtype: {dtype}.")
    bit_size = int(bit_search.groups()[0])
    return bit_size // 8

def compute_module_sizes(model):
    """
    Compute the size of each submodule of a given model.
    """
    from collections import defaultdict
    module_sizes = defaultdict(int)
    for name, tensor in named_module_tensors(model, recurse=True):
      size = tensor.numel() * dtype_byte_size(tensor.dtype)
      name_parts = name.split(".")
      for idx in range(len(name_parts) + 1):
        module_sizes[".".join(name_parts[:idx])] += size

    return module_sizes
```

```python
from transformers import T5Tokenizer, T5ForConditionalGeneration
from quanto import quantize, freeze
from helper import compute_module_sizes
import quanto

# 加载模型
tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-small")
model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-small").to("cuda")
input_text = "translate English to German: How old are you?"
input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to("cuda")
# 输出结果
outputs = model.generate(input_ids)
print(tokenizer.decode(outputs[0]))
# 打印模型大小
module_sizes = compute_module_sizes(model)
print(f"The model size is {module_sizes[''] * 1e-9} GB")

# 量化模型为int8
# 设置此处不量化激活值
quantize(model, weights=quanto.qint8, activations=None)
# 此时，model在freeze前处于中间状态
freeze(model)
# 输出结果
outputs = model.generate(input_ids)
print(tokenizer.decode(outputs[0]))
# 打印模型大小
module_sizes = compute_module_sizes(model)
print(f"The model size is {module_sizes[''] * 1e-9} GB")

```

### 高级线性量化

#### 量化和反量化张量

<img src="笔记图保存\PixPin_2025-09-09_17-25-21.png" alt="PixPin_2025-09-09_17-25-21" style="zoom:50%;" />

线性量化的思想，就是对 r（original value）和 q（quantized value）进行数学映射。

那么此处的s（scale）和z（zero point）如何获取？

<img src="笔记图保存\PixPin_2025-08-28_15-42-10.png" alt="PixPin_2025-08-28_15-42-10" style="zoom:50%;" />

根据最大最小值，解方程组即可（rmin、rmax从权重矩阵中获取，qmin、qmax由数据类型可知范围）

<img src="笔记图保存\PixPin_2025-08-28_15-43-36.png" alt="PixPin_2025-08-28_15-43-36" style="zoom:50%;" />

zero-point的作用：使原类型中的0与量化后的z对齐。0将量化为z，z反量化为0

<img src="笔记图保存\PixPin_2025-09-02_09-25-34.png" alt="PixPin_2025-09-02_09-25-34" style="zoom:50%;" />

2类特殊情况处理（映射的zero-point超出q范围）：直接设置为q的边界

<img src="笔记图保存\PixPin_2025-09-02_09-38-33.png" alt="PixPin_2025-09-02_09-38-33" style="zoom:50%;" />

线性量化的过程中，s（scale）和r（original value）同类型，z（zero point）和q（quantized value）同类型。

<img src="笔记图保存\PixPin_2025-09-01_10-45-32.png" alt="PixPin_2025-09-01_10-45-32" style="zoom:50%;" />

q = int(round(r/s + z)) = 四舍五入并转换为整数。

.clamp函数的作用是将一个数值（或张量）限制在指定的最小值和最大值之间（==clip函数）。

```python
import torch
from numpy import clip

torch.manual_seed(0)


# 获取scale和zero_point
def get_scale_and_zero_point(r, q_type):
    rmax = r.max().item()
    rmin = r.min().item()

    q_max = torch.iinfo(q_type).max
    q_min = torch.iinfo(q_type).min

    scale = (rmax - rmin) / (q_max - q_min)
    # 防止0点落在量化区间外
    zero_point = int(clip(round((q_min - rmin) / scale), q_min, q_max))
    return scale, zero_point


# 量化张量
def quantify(r, s, z, q_type):
    # 四舍五入
    q = torch.round(r / s + z)
    q__max = torch.iinfo(q_type).max
    q__min = torch.iinfo(q_type).min
    # 限制类型范围
    q = q.clamp(q__min, q__max).to(q_type)
    return q


# 反量化张量
def de_quantify(q, s, z, r_type):
    # 注意这里，q需要进行转换为float的操作
    r = s * (q.to(r_type) - z)
    return r


if __name__ == '__main__':
    # 原始张量
    origin_tensor = torch.rand((3, 3), dtype=torch.float64)
    print(origin_tensor)

    # 获取scale和zero_point
    scale, zero_point = get_scale_and_zero_point(origin_tensor, torch.int8)
    # 量化后张量
    q = quantify(r=origin_tensor, s=scale, z=zero_point, q_type=torch.int8)
    print(q)

    # 反量化张量
    r = de_quantify(q, scale, zero_point, torch.float64)
    print(r)

    # 计算量化前后差距
    print((origin_tensor - r).square().mean().item())

```

#### 对称和非对称模式

<img src="笔记图保存\PixPin_2025-09-02_11-25-48.png" alt="PixPin_2025-09-02_11-25-48" style="zoom:50%;" />

先前的代码中，使用的是Asymmetric（非对称模式）

Symmetric：针对最大值进行映射，此时不需要zero point，因为最大和最小值中间的0点是对称的。

**如何判断使用哪一种？**

**1.范围**：

对称量化不适合处理只在一边有值的数据（比如ReLU输出），会造成量化精度浪费

使用 非对称量化，比如只用 0 到 255 来表示 0 到 10，这样就不会浪费空间

**2.内存**：

相比非对称，对称量化不需要存储zero point

**3.实战**：

量化为8位时一般使用对称模式，更低位时一般使用非对称模式

<img src="笔记图保存\PixPin_2025-09-02_11-28-51.png" alt="PixPin_2025-09-02_11-28-51" style="zoom:50%;" />

#### 量化粒度

##### 按通道

按照量化粒度，可分为全张量、不同通道（沿着行/列）、不同组（N个元素1组）

如划分了通道/组，意味着在每个子集中独立计算scale和zeropoint

```
按通道量化时，一般以W的行作为单位而不是列，原因：

设X形状（B，F1），Y（B，F2），则W必在X右边以转置相乘，且计算时形状：WT=（F1，F2）

W在右边，X在左边时，以X的每行*W的每列，由于计算时会将W进行转置，则实际存储时，需要对W按行进行量化。
```

<img src="笔记图保存\PixPin_2025-09-02_11-46-27.png" alt="PixPin_2025-09-02_11-46-27" style="zoom:50%;" />

```python
import torch

torch.manual_seed(0)


# 按行量化张量（对称模式）
def quantify_raw(r, q_type):
    # 提前算出q_max，以免重复计算
    q_max = torch.iinfo(q_type).max

    # 按行计算出scale列表（注意取绝对值），此处使用amax取最大值，max则是返回元组(values, indices)
    scale = r.abs().amax(dim=1, keepdim=True) / q_max

    # 直接对2个矩阵做除法，较短的scale列表会通过广播机制自动填充
    q = torch.round(r / scale)

    # 限制类型范围
    q = q.clamp(-q_max, q_max).to(q_type)

    return q, scale


# 反量化张量（对称模式）
def de_quantify(q, s, r_type):
    # 注意这里，q需要进行转换为float的操作
    r = s * q.to(r_type)
    return r


if __name__ == '__main__':
    # 原始张量
    origin_tensor = torch.rand((3, 4), dtype=torch.float64)
    print(origin_tensor)

    # 量化后张量
    q, s = quantify_raw(r=origin_tensor, q_type=torch.int8)
    print(q)

    # 反量化张量
    r = de_quantify(q, s, torch.float64)
    print(r)

    # 计算量化前后差距
    print((origin_tensor - r).square().mean().item())

```

##### 按组

实际上，按组量化只需要将原始tensor转换为N行group列即可，之后可直接按通道处理，处理完后再恢复原形状即可

```python
import torch

torch.manual_seed(0)


# 按组量化
def quantify_group(r, q_type, group_size):
    r_shape = r.shape
    r.view(-1, group_size)
    q, s = quantify_raw(r, q_type)
    return q.view(r_shape), s


# 按行量化张量
def quantify_raw(r, q_type):
    # 提前算出q_max，以免重复计算
    q_max = torch.iinfo(q_type).max

    # 按行计算出scale列表（注意取绝对值）
    scale = r.abs().amax(dim=1, keepdim=True) / q_max

    # 直接对2个矩阵做除法，较短的scale列表会通过广播机制自动填充
    q = torch.round(r / scale)

    # 限制类型范围
    q = q.clamp(-q_max, q_max).to(q_type)

    return q, scale


# 反量化张量
def de_quantify(q, s, r_type):
    # 注意这里，q需要进行转换为float的操作
    r = s * q.to(r_type)
    return r


# 按组反量化
def de_quantify_group(q, s, r_type, group_size):
    q_shape = q.shape
    q.view(-1, group_size)
    r = de_quantify(q, s, r_type)
    return r.view(q_shape)


if __name__ == '__main__':
    # 原始张量
    origin_tensor = torch.rand((3, 4), dtype=torch.float64)
    print(origin_tensor)

    # 按组量化后张量
    q, s = quantify_group(r=origin_tensor, q_type=torch.int8, group_size=3)
    print(q)

    # 按组反量化张量
    r = de_quantify_group(q, s, torch.float64, group_size=3)
    print(r)

    # 计算量化前后差距
    print((origin_tensor - r).square().mean().item())

```

#### W8A32和W8A8

情况一：只量化权重（W8A32）（目前方法，且没有完全实现激活值的量化操作）

- 权重被量化为 INT8 存储。
- 激活仍是 FP32。
- 计算时，需要将权重反量化为 FP32，然后与 FP32 激活做矩阵乘法。
- ❌ 无法避免浮点运算。

 情况二：同时量化权重和激活（W8A8）

- 权重：INT8
- 激活：INT8
- 关键：可以用整数运算直接计算卷积或矩阵乘法。

W8A8 是把权重和激活都保持在整数（INT8），直接用整数计算；而 W8A32 是把权重反量化（解压）回浮点，再和 FP32 激活一起用浮点计算。但在一些硬件上可能不支持W8A8直接的INT8运算。

<img src="笔记图保存\PixPin_2025-09-02_17-49-59.png" alt="PixPin_2025-09-02_17-49-59" style="zoom:50%;" />

torch.nn.Linear和torch.nn.functional.linear区别：
实际上，torch.nn.Linear 的 forward 方法内部就是调用 torch.nn.functional.linear
Linear自动创建并注册weight和bias，functional.linear则需手动提供weight和bias，是一个计算函数，不参与model.parameters()

```python
import torch

torch.manual_seed(0)

"""
这段代码没有真正实现 “A32”（Activation 32-bit），
因为输入 x 是 torch.float64（FP64），导致前向传播中的激活值（activation）全程以 FP64 运算
"""


# 按行量化张量（对称模式）
def quantify_raw(r, q_type):
    # 提前算出q_max，以免重复计算
    q_max = torch.iinfo(q_type).max

    # 按行计算出scale列表（注意取绝对值）
    scale = r.abs().amax(dim=1, keepdim=True) / q_max

    # 直接对2个矩阵做除法，较短的scale列表会通过广播机制自动填充
    q = torch.round(r / scale)

    # 限制类型范围
    q = q.clamp(-q_max, q_max).to(q_type)

    return q, scale


# 反量化张量（对称模式）
def de_quantify(q, s, r_type):
    # 注意这里，q需要进行转换为float的操作
    r = s * q.to(r_type)
    return r


# 线性层计算
def quantized_linear_W8A32_without_bias(x, q, s):
    d_w = de_quantify(q, s, torch.float32)
    output = torch.nn.functional.linear(x, d_w)
    return output


if __name__ == '__main__':
    # 原始张量（实际运算时会进行转置操作）
    weight = torch.rand((3, 4), dtype=torch.float32)
    print(weight)

    # 量化后张量（保存时int8）
    q, s = quantify_raw(r=weight, q_type=torch.int8)
    print(q)

    # 输入
    x = torch.rand((1, 4), dtype=torch.float64)

    # 线性层计算（计算时反量化为fp32）
    output = quantized_linear_W8A32_without_bias(x, q, s)
    print(f"This is the W8A32 output: {output}")

    # 线性层计算（无量化操作）
    fp32_output = torch.nn.functional.linear(x, weight)
    print(f"This is the output if we don't quantize: {fp32_output}")

```

#### 自定义量化器

对线性层按通道量化（行），且为对称模式

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

torch.manual_seed(0)

"""
nn.Parameter 是 torch.Tensor 的子类（subclass）
当被赋值给 nn.Module 的成员时，会被自动添加到模型的参数列表中（即 model.parameters() 会包含它）
它默认设置 requires_grad=True（除非手动设为 False）

self.int8_weights = nn.Parameter(torch.Tensor([0, 1]).to(dtype=torch.int8))
使用torch.int8存储权重，报错： 
RuntimeError :  Only Tensors of floating point and complex dtype can require gradients
"""

"""
以下代码，并不是真正意义上的W8A16，
只是量化了权重为INT8，但是WX+B的激活值并没有做任何处理，只是将W还原为FP16，但X还是FP32，WX+B得出的结果还是FP32
"""


# 修改后，使用register_buffer，表明不需要计算该张量的梯度
class W8A16LinearLayer(nn.Module):
    def __init__(self, in_features, out_features, bias, dtype):
        super().__init__()
        # 注意这里是(out_features, in_features)
        self.register_buffer("int8_weights",
                             torch.randint(-128, 127, (out_features, in_features), dtype=torch.int8))
        # 注意这里是(out_features, 1)
        self.register_buffer("scales",
                             torch.randn((out_features, 1), dtype=dtype))
        # 注意这里是(1, out_features)
        if bias:
            self.register_buffer("bias",
                                 torch.randn((1, out_features), dtype=dtype))
        else:
            self.bias = None

    def forward(self, x):
        q = self.int8_weights
        s = self.scales
        bias = self.bias
        # 注意这里，linear是矩阵乘法（带w转置），*是逐元素乘法（支持广播机制）
        # 反量化张量以参与线性运算
        r = s * q.to(torch.float16)
        output = F.linear(x, r)
        if bias is not None:
            output = output + bias
        return output

    # 按行量化张量（对称模式）
    def quantize(self, r, q_type=torch.int8):
        # 提前算出q_max，以免重复计算
        q_max = torch.iinfo(q_type).max

        # 按行计算出scale列表（注意取绝对值）
        scale = r.abs().amax(dim=1, keepdim=True) / q_max

        # 直接对2个矩阵做除法，较短的scale列表会通过广播机制自动填充
        q = torch.round(r / scale)

        # 限制类型范围
        q = q.clamp(-q_max, q_max).to(q_type)

        self.int8_weights = q
        self.scales = scale


# 遍历一个模型，并对其中线性层进行W8A16替换
def replace_linear_with_target_and_quantize(module, target_class, module_name_to_exclude):
    # 遍历模型的 模块名称name、对象实例child
    for name, child in module.named_children():
        if isinstance(child, nn.Linear) and not any([x == name for x in module_name_to_exclude]):
            old_bias = child.bias

            new_module = target_class(child.in_features,
                                      child.out_features,
                                      old_bias is not None,
                                      child.weight.dtype)
            # 将module中指定name的模块实例，替换为new_module
            setattr(module, name, new_module)

            # 在替换对应模块后，进行量化操作
            old_weight = child.weight
            getattr(module, name).quantize(old_weight)

            if old_bias is not None:
                getattr(module, name).bias = old_bias
        else:
            # 递归调用嵌套模块的函数
            replace_linear_with_target_and_quantize(child, target_class, module_name_to_exclude)


# 遍历一个模型，并对其中线性层进行W8A16替换
def replace_linear_with_target(module, target_class, module_name_to_exclude):
    # 遍历模型的 模块名称name、对象实例child
    for name, child in module.named_children():
        if isinstance(child, nn.Linear) and not any([x == name for x in module_name_to_exclude]):
            old_bias = child.bias

            new_module = target_class(child.in_features,
                                      child.out_features,
                                      old_bias is not None,
                                      child.weight.dtype)
            # 将module中指定name的模块实例，替换为new_module
            setattr(module, name, new_module)

            if old_bias is not None:
                getattr(module, name).bias = old_bias
        else:
            # 递归调用嵌套模块的函数
            replace_linear_with_target_and_quantize(child, target_class, module_name_to_exclude)


# 示例模型
class DummyModel(torch.nn.Module):
    def __init__(self):
        super().__init__()
        self.emb = torch.nn.Embedding(1, 1)
        # Try with bias
        self.linear_1 = nn.Linear(1, 1)
        # Try without bias
        self.linear_2 = nn.Linear(1, 1, bias=False)
        # Lm prediction head
        self.lm_head = nn.Linear(1, 1, bias=False)


if __name__ == '__main__':
    # 测试前向传播过程（权重、矩阵和偏置，均随机生成）
    # batch_size, seq_len, input_size
    x = torch.randn((4, 6, 16), dtype=torch.float32)
    layer = W8A16LinearLayer(16, 32, bias=True, dtype=torch.float32)
    print(layer(x).shape)

    # 将一个指定的权重矩阵量化，并进行前向传播
    random_matrix = torch.randn((32, 16), dtype=torch.bfloat16)
    layer.quantize(random_matrix)
    print(layer(x).shape)

    # 查看量化误差
    print((random_matrix - layer.scales * layer.int8_weights).abs().mean())

    # 量化一个实际模型
    # 在创建一个模型时，Pytorch会赋予一个初始化权重，权重将随着训练过程更新
    model = DummyModel()
    # 一般来说，模型末尾层将不会量化，防止严重影响性能
    replace_linear_with_target_and_quantize(model, W8A16LinearLayer, ["lm_head"])
    print(model)

    # 存储量化后的模型权重（假设远程机器完成）
    quantized_state_dict = model.state_dict()
    torch.save(quantized_state_dict, "quantized_state_dict.pth")

    # 本地可以先替换模型层，但是不进行量化操作（远程已完成）
    # torch.device("meta")只创建模型结构（计算图），但不分配实际内存存储权重，从而实现极快的模型初始化和零显存占用
    with torch.device("meta"):
        model = DummyModel()
    print(model)
    replace_linear_with_target(model, W8A16LinearLayer, ["lm_head"])
    # 从远程下载量化后的权重
    state_dict = torch.load("quantized_state_dict.pth")
    model.load_state_dict(state_dict, strict=True, assign=True)
    print(model)

```

#### 权重包装

Pytorch并不支持int8以下的数据类型（如int4、int2），只能将int2以int8的形式存储

<img src="笔记图保存\PixPin_2025-09-04_18-58-44.png" alt="PixPin_2025-09-04_18-58-44" style="zoom:50%;" />

<img src="笔记图保存\PixPin_2025-09-04_19-00-17.png" alt="PixPin_2025-09-04_19-00-17" style="zoom:50%;" />

```python
import torch


def pack_weights(uint8tensor, bits):
    if uint8tensor.shape[0] * bits % 8 != 0:
        raise ValueError(f"The input shape needs to be a mutiple \
        of {8 / bits} - got {uint8tensor.shape[0]}")
    num_values = uint8tensor.shape[0] * bits // 8
    num_steps = 8 // bits
    unpacked_idx = 0
    packed_tensor = torch.zeros((num_values), dtype=torch.uint8)
    # 1 0 3 2 - 01 00 11 10
    # [0000 0000] -> 0000 0001
    # 0000 0001
    # 0000 0000 - 0000 0000
    # 0000 0011 - 0011 0000 - 0011 0001
    # 1011 0001
    for i in range(num_values):
        for j in range(num_steps):
            packed_tensor[i] |= uint8tensor[unpacked_idx] << (bits * j)
            unpacked_idx += 1
    return packed_tensor


def unpack_weights(uint8tensor, bits):
    num_values = uint8tensor.shape[0] * 8 // bits
    num_steps = 8 // bits
    unpacked_tensor = torch.zeros((num_values), dtype=torch.uint8)
    unpacked_idx = 0
    # 1 0 3 2 - 01 00 11 10
    # [00000000 00000000 00000000 00000000]
    # [10110001 00101100 00001011 00000010]
    # [00000001 00000000 00000011 00000010]
    # 10110001
    # 00000011
    # 00000001
    # 1: [10110001]
    # 2: [00101100]
    # 3: [00001011]
    mask = 2 ** bits - 1
    for i in range(uint8tensor.shape[0]):
        for j in range(num_steps):
            unpacked_tensor[unpacked_idx] |= uint8tensor[i] >> (bits * j)
            unpacked_idx += 1
    unpacked_tensor &= mask
    return unpacked_tensor


if __name__ == '__main__':
    pack_weights = pack_weights(torch.tensor([1, 0, 3, 2, 3, 3, 3, 3]), 2)
    print(pack_weights)

    unpack_weights = unpack_weights(pack_weights, 2)
    print(unpack_weights)

```

### PyTorch 2 Export Quantization（PT2E）

```
PT2E量化，Pytorch官方教程：

https://docs.pytorch.org/ao/main/tutorials_source/pt2e_quant_ptq.html  
https://docs.pytorch.org/ao/main/tutorials_source/pt2e_quant_qat.html  
https://docs.pytorch.org/ao/main/tutorials_source/pt2e_quant_x86_inductor.html  
https://docs.pytorch.org/ao/main/tutorials_source/pt2e_quant_xpu_inductor.html  
https://docs.pytorch.org/ao/main/tutorials_source/pt2e_quant_openvino_inductor.html  
https://docs.pytorch.org/ao/main/tutorials_source/pt2e_quantizer.html  
```

#### 概念和对比

现有方法主要分为3类，一般来说，静态量化用于CNN，动态量化用于RNN等时序模型，QAT更适合高精度场景。

动态量化一般用的较少。

针对权重的量化是静态的，因为模型已经固定。而针对激活值的量化是动态的，因为会随着X的min、max而改变。

**PT2E标准四步工作流**

| 步骤                           | API / 操作                                         | 说明                                                         |
| ------------------------------ | -------------------------------------------------- | ------------------------------------------------------------ |
| 1. 捕获 (Capture)              | torch.export.export(model, example_inputs)         | 将动态模型转为静态 ATen IR 图。                              |
| 2. 准备 (Prepare)              | prepare_pt2e (PTQ) 或 prepare_qat_pt2e (QAT)       | 插入 Observer (PTQ) 或 FakeQuant (QAT) 节点，并执行算子融合（如 Conv+BN）。 |
| 3. 校准/训练 (Calibrate/Train) | 运行校准数据 (PTQ) 或 训练循环 (QAT)               | Observer 收集统计数据 / 模型学习适应量化噪声。               |
| 4. 转换 (Convert)              | convert_pt2e(prepared_model)                       | 将 Observer/FakeQuant 替换为真实 Quantize/Dequantize 节点，权重被折叠为 INT8 常量。 |
| 5. (可选) 编译 (Compile)       | torch.compile(quantized_model, backend="inductor") | 通过编译器将 dequant -> op -> quant 模式融合为高效硬件内核，实现真正加速。 |

注意：convert_pt2e 后的模型不会自动加速，必须通过 torch.compile 触发后端优化才能获得性能提升。

- 把 `nn.Module`（动态图） → **捕获成一个“静态计算图”**
- 这个图由 **ATen 算子**（PyTorch 底层 C++ 算子）组成
- **“训练好的模型 → 转成一个‘静态图’ → 插入量化节点 → 编译优化 → 在目标设备（CPU/GPU/手机）上高效运行”**

**主流后端Quantizer对比**

| 后端        | 量化器类             | 适用硬件              | 特点                                            |
| ----------- | -------------------- | --------------------- | ----------------------------------------------- |
| X86Inductor | X86InductorQuantizer | x86 CPU               | 利用 oneDNN 库，支持静态/动态量化，服务器首选。 |
| XNNPACK     | XNNPACKQuantizer     | ARM CPU (移动端)      | 轻量级，适合边缘设备，主要支持静态量化。        |
| OpenVINO    | OpenVINOQuantizer    | Intel CPU/GPU/NPU     | 支持高级算法如 SmoothQuant，需安装 nncf。       |
| XPUInductor | XPUInductorQuantizer | Intel Data Center GPU | 为英特尔数据中心 GPU 优化。                     |

#### 静态量化（PTSQ）

 PTSQ: Post-Training Static Quantization
 👉 训练后静态量化：权重和激活值都在训练后通过校准被量化，推理时使用固定的量化参数。

```
校准：通过校准数据集（Calibration），获取对应层中激活值min、max范围，以获取更好的激活值量化结果。

激活值的量化，是在 WX + B 之前对输入 X 做的 —— 也就是“量化输入激活”，然后和量化权重一起做整数矩阵乘法。

既然激活值会被量化，为何会出现反量化操作：Bias 通常是 FP32，不能直接加 INT；且很多层不能量化，操作必须在FP32域进行

X (FP32)
  ↓ 量化
X_q (int8) ──┐
             ├→ int8 @ int8 → output_q (int32)
W_q (int8) ──┘
  ↓ 反量化（× scale_x * scale_w）
output_fp32 (FP32)
  ↓ + bias (FP32)
output_fp32
  ↓ → LayerNorm / Softmax / 残差 → 下一层
```

```python
import torch
from torchvision.models import resnet18
from torchvision.models import ResNet18_Weights
from torch.export import export
from torchao.quantization.pt2e.quantize_pt2e import prepare_pt2e, convert_pt2e, prepare_qat_pt2e
import torchao.quantization.pt2e.quantizer.x86_inductor_quantizer as xiq
from torchao.quantization.pt2e.quantizer.x86_inductor_quantizer import X86InductorQuantizer
from executorch.backends.xnnpack.quantizer.xnnpack_quantizer import XNNPACKQuantizer, get_symmetric_quantization_config
import warnings

# 压制警告
warnings.filterwarnings("ignore", message="erase_node.*already erased node")
warnings.filterwarnings("ignore", message="The input of maxpool2d is not quantized")

# 加载模型
model = resnet18(weights=ResNet18_Weights.DEFAULT).eval()
# 设计输入，形状一致即可，数值可随机生成（B，C，W，H）
example_inputs = (torch.randn(1, 3, 224, 224),)

# 步骤 1: 捕获模型
# 从 ExportedProgram 中提取出一个可执行的、标准的 torch.nn.Module 对象。
# 内部计算图已经转换为由 ATen 算子（PyTorch 底层 C++ 算子）组成的静态图
exported_model = export(model, example_inputs).module()

# 步骤 2: 配置量化器 (X86 + Inductor 后端)
quantizer = X86InductorQuantizer()
quant_config = xiq.get_default_x86_inductor_quantization_config()
quantizer.set_global(quant_config)

# 步骤 3: 准备模型 (插入 Observer)
prepared_model = prepare_pt2e(exported_model, quantizer)

# 步骤 4: 校准 (使用少量代表性数据)
print("Running calibration...")
with torch.no_grad():
    for _ in range(100):  # 模拟100个校准样本，实际可用dataloader循环
        x = torch.randn(1, 3, 224, 224)
        prepared_model(x)

# 步骤 5: 转换为量化模型
quantized_model = convert_pt2e(prepared_model)

# 步骤 6: 编译以获得性能加速
compiled_model = torch.compile(quantized_model, backend="inductor")

# 测试推理
with torch.no_grad():
    output = compiled_model(torch.randn(1, 3, 224, 224))
print("Static PTQ Inference Done!")

```

#### 动态量化（PTDQ）

 PTDQ: Post-Training Dynamic Quantization
 👉 训练后动态量化：权重在训练后被静态量化，激活值在推理时动态量化。

```
激活值在推理时动态量化：即前向传播过程中，实时计算激活值的min、max
```

```python
import torch
from torchvision.models import resnet18
from torchvision.models import ResNet18_Weights
from torch.export import export
from torchao.quantization.pt2e.quantize_pt2e import prepare_pt2e, convert_pt2e, prepare_qat_pt2e
import torchao.quantization.pt2e.quantizer.x86_inductor_quantizer as xiq
from torchao.quantization.pt2e.quantizer.x86_inductor_quantizer import X86InductorQuantizer
from executorch.backends.xnnpack.quantizer.xnnpack_quantizer import XNNPACKQuantizer, get_symmetric_quantization_config
import warnings

# 压制警告
warnings.filterwarnings("ignore", message="erase_node.*already erased node")
warnings.filterwarnings("ignore", message="The input of maxpool2d is not quantized")

# 加载模型
model = resnet18(weights=ResNet18_Weights.DEFAULT).eval()
# 设计输入，形状一致即可，数值可随机生成（B，C，W，H）
example_inputs = (torch.randn(1, 3, 224, 224),)

# 步骤 1: 捕获模型
# 从 ExportedProgram 中提取出一个可执行的、标准的 torch.nn.Module 对象。
# 内部计算图已经转换为由 ATen 算子（PyTorch 底层 C++ 算子）组成的静态图
exported_model = export(model, example_inputs).module()

# 步骤 2: 配置动态量化器
quantizer = X86InductorQuantizer()
# 关键：设置 is_dynamic=True
quant_config = xiq.get_default_x86_inductor_quantization_config(is_dynamic=True)
quantizer.set_global(quant_config)

# 步骤 3: 准备模型
prepared_model = prepare_pt2e(exported_model, quantizer)

# 步骤 4: 跳过校准，直接转换
quantized_model = convert_pt2e(prepared_model)

# 步骤 5: 编译
compiled_model = torch.compile(quantized_model, backend="inductor")

# 测试推理
x = torch.randn(1, 3, 224, 224)
with torch.no_grad():
    output = compiled_model(x)
print("Dynamic PTQ Inference Done!")
# 对比前后输出
print((model(x) - output).square().mean().item())

```

#### 量化感知训练（QAT）

 QAT: Quantization-Aware Training
 👉 在训练过程中模拟量化误差，使模型在训练阶段就“感知”到量化的影响，从而提升量化后的精度。

```
例如，同时保存了WR（origin）和WQ（quantize），前向传播时使用WQ算出预测值YQ，利用Y和YQ算出损失L，但是更新时，使用WR=WR-αL'(WR)，L对WR而不是对WQ求导。
```

```python
import torch
import torchao
from torchvision.models import resnet18
from torchvision.models import ResNet18_Weights
from torch.export import export
from torchao.quantization.pt2e.quantize_pt2e import prepare_pt2e, convert_pt2e, prepare_qat_pt2e
import torchao.quantization.pt2e.quantizer.x86_inductor_quantizer as xiq
from torchao.quantization.pt2e.quantizer.x86_inductor_quantizer import X86InductorQuantizer
from executorch.backends.xnnpack.quantizer.xnnpack_quantizer import XNNPACKQuantizer, get_symmetric_quantization_config
import warnings

# 压制警告
warnings.filterwarnings("ignore", message="erase_node.*already erased node")
warnings.filterwarnings("ignore", message="The input of maxpool2d is not quantized")

# 加载模型
model = resnet18(weights=ResNet18_Weights.DEFAULT).eval()
# 设计输入，形状一致即可，数值可随机生成（B，C，W，H）
example_inputs = (torch.randn(1, 3, 224, 224),)

# 步骤 1: 捕获模型
# 从 ExportedProgram 中提取出一个可执行的、标准的 torch.nn.Module 对象。
# 内部计算图已经转换为由 ATen 算子（PyTorch 底层 C++ 算子）组成的静态图
exported_model = export(model, example_inputs).module()

# 步骤 2: 配置 QAT 量化器 (以 XNNPACK 为例)
quantizer = XNNPACKQuantizer()
# 关键：设置 is_qat=True
qat_config = get_symmetric_quantization_config(is_qat=True)
quantizer.set_global(qat_config)

# 步骤 3: 准备模型 (插入 FakeQuant 节点)
prepared_model = prepare_qat_pt2e(exported_model, quantizer)

# 步骤 4: 微调训练 (简化示例)
device = "cuda" if torch.cuda.is_available() else "cpu"
prepared_model.to(device)
# 切换为训练模式
torchao.quantization.pt2e.move_exported_model_to_train(prepared_model)

optimizer = torch.optim.SGD(prepared_model.parameters(), lr=1e-5)
criterion = torch.nn.CrossEntropyLoss()

print("Starting QAT fine-tuning...")
# “全参数、低学习率、短周期、带量化噪声的微调” —— 这是 QAT 的标准做法。
for epoch in range(3):  # 只微调3轮
    optimizer.zero_grad()
    # 模拟一个训练批次
    x = torch.randn(8, 3, 224, 224).to(device)
    target = torch.randint(0, 1000, (8,)).to(device)  # 假设1000类

    output = prepared_model(x)
    loss = criterion(output, target)
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch + 1}, Loss: {loss.item():.4f}")

    # 最佳实践：在几轮后冻结 Observer
    # “冻结 Observer” = 停止更新 Observer 的统计值（min/max），让它保持在校准阶段学到的范围，避免训练后期异常值破坏量化参数。
    if epoch >= 1:
        prepared_model.apply(torchao.quantization.pt2e.disable_observer)

# 切换回评估模式
torchao.quantization.pt2e.move_exported_model_to_eval(prepared_model)
# 注意将模型转换回CPU环境
prepared_model = prepared_model.to("cpu")

# 步骤 5: 转换
quantized_model = convert_pt2e(prepared_model)

# 步骤 6: 编译
# pip install numpy onnx onnxscript onnxruntime onnxruntime-extensions
# compiled_model = torch.compile(quantized_model, backend="onnxrt")
"""
不同后端优化，所支持的后端：
print(torch._dynamo.list_backends())：
['cudagraphs', 'inductor', 'onnxrt', 'openxla', 'tvm']

放弃 torch.compile(..., backend='onnxrt')，改用 torch.onnx.export(..., dynamo=False) 导出 QDQ 模型
为什么：
torch.compile(backend='onnxrt') 内部用的是 dynamo_export，不支持量化算子
torch.onnx.export(..., dynamo=False) + convert_pt2e 可以导出标准 QDQ（QuantizeLinear/DequantizeLinear）ONNX 模型
QDQ 格式被 ONNX Runtime 完美支持（包括 XNNPACK 后端）
"""

# 测试推理
x = torch.randn(1, 3, 224, 224)
with torch.no_grad():
    output = quantized_model(x)
print("QAT Done!")
# 对比前后输出
print((model(x) - output).square().mean().item())

```

#### 量化器quantizer定制

当需要精细控制哪些层量化、使用何种量化策略时，需自定义 Quantizer。

```python
import torch
from torch.ao.quantization.quantizer import Quantizer  # ← PT2E 的基类，所有自定义量化器必须继承它
from torch.fx import GraphModule  # ← 代表被 torch.export 捕获后的模型图（FX Graph）
from torch.ao.quantization.quantizer.quantizer import QuantizationSpec, QuantizationAnnotation
# ↑ QuantizationSpec: 定义“如何量化一个张量”
# ↑ QuantizationAnnotation: 定义“一个算子的输入/输出如何被量化”


class MyQuantizer(Quantizer):
    """
    自定义量化器：只为 `torch.nn.Linear` 对应的 ATen 算子（aten.linear.default）添加量化注解。
    这个量化器会：
      - 对 Linear 的输入激活值：使用 per-tensor INT8 对称量化
      - 对 Linear 的权重：使用 per-channel INT8 对称量化（沿输出通道轴）
      - 对 Linear 的输出：使用与输入相同的量化方案
    """

    def annotate(self, model: GraphModule) -> GraphModule:
        """
        核心方法！遍历模型图中的每个节点，为特定算子添加量化注解。
        参数:
            model (GraphModule): 由 torch.export 捕获后的 FX 图模型。
        返回:
            model (GraphModule): 添加了量化注解后的模型图。
        """
        # 遍历图中所有节点（每个节点代表一个算子或常量）
        for node in model.graph.nodes:
            # 检查当前节点是否是函数调用，且目标是 aten.linear.default
            # ⚠️ 注意：在 torch.export 后，nn.Linear 会被转换成 aten.linear.default
            if node.op == "call_function" and node.target == torch.ops.aten.linear.default:
                # 找到 linear 算子的两个主要输入：
                #   - 第0个参数：输入激活值（activation）
                #   - 第1个参数：权重（weight）
                input_act = node.args[0]  # ← 输入张量 (如: [batch_size, in_features])
                weight = node.args[1]  # ← 权重张量 (如: [out_features, in_features])

                # ============ 定义激活值的量化规格 ============
                act_qspec = QuantizationSpec(
                    dtype=torch.int8,  # ← 量化后数据类型：INT8
                    quant_min=-128,  # ← 量化后最小值（INT8 范围）
                    quant_max=127,  # ← 量化后最大值
                    qscheme=torch.per_tensor_affine,  # ← 量化方案：逐张量仿射（支持非对称）
                    is_dynamic=False,  # ← 静态量化（如果是动态量化，设为 True）
                    observer_or_fake_quant_ctr=torch.ao.quantization.MinMaxObserver,
                    # ↑ 使用 MinMaxObserver 在校准阶段统计 min/max，计算 scale 和 zero_point
                )

                # ============ 定义权重的量化规格 ============
                weight_qspec = QuantizationSpec(
                    dtype=torch.int8,
                    quant_min=-127,  # ← 注意：对称量化通常不用 -128，避免溢出
                    quant_max=127,
                    qscheme=torch.per_channel_symmetric,  # ← 关键！逐通道对称量化
                    ch_axis=0,  # ← 沿第0轴（输出通道）进行 per-channel 量化
                    observer_or_fake_quant_ctr=torch.ao.quantization.PerChannelMinMaxObserver,
                    # ↑ 使用 PerChannelMinMaxObserver，为每个通道单独计算 scale
                )

                # ============ 创建量化注解对象 ============
                # 这个对象告诉 PT2E：“当我遇到这个 linear 节点时，应该这样量化它的输入和输出”
                node.meta["quantization_annotation"] = QuantizationAnnotation(
                    input_qspec_map={
                        # 键：输入张量（边），值：该张量的量化规格
                        input_act: act_qspec,  # ← 输入激活用 act_qspec 量化
                        weight: weight_qspec,  # ← 权重用 weight_qspec 量化
                        # 注意：线性层通常有3个参数 (input, weight, bias)，这里忽略了 bias
                        # 如果要量化 bias，需要额外定义 bias_qspec 并加入此字典
                    },
                    output_qspec=act_qspec,  # ← 输出张量使用与输入激活相同的量化方案
                    _annotated=True,  # ← 标记：此节点已被注解（必须设为 True）
                )
        # 返回注解后的模型图
        return model

    def validate(self, model: GraphModule) -> None:
        """
        可选方法：在量化前对模型图进行校验。
        例如：检查是否所有应量化的节点都已注解，或是否存在不支持的模式。
        这里留空，表示不做额外校验。
        """
        pass  # 可添加校验逻辑，如抛出异常或打印警告


"""
然后在量化流程中替换：
quantizer = MyQuantizer()
prepared_model = prepare_pt2e(exported_model, quantizer)
# ... 后续步骤相同
"""

```

#### 后端部署（待更新）

##### ONNX

```
PyTorch（训练） 
   ↓
ONNX（模型导出，通用格式）
   ↙         ↘
TensorRT     TFLite
  ↓            ↓
服务器部署    手机/嵌入式部署
(NVIDIA GPU)  (Android, Raspberry Pi)
```

##### TensorRT



##### TFLite



# 常用基础

## 字符串中添加数值

```
# 推荐：使用f-string
name = "Alice"
greeting1 = f"Hello, {name}!"

# 使用 % 操作符
greeting2 = "Hello, %s %s!" % (name, name)

# 使用 str.format() 方法
greeting3 = "Hello, {}!".format(name)
```

## enumerate遍历

在Python中，enumerate 是一个内置函数，
用于在遍历（如循环）可迭代对象（如列表、元组、字符串等）时，同时获取元素的索引和元素的值。

enumerate(iterable, start=0)
iterable 是一个可迭代对象，比如列表、元组、字符串等。
start 是可选参数，表示索引的起始值，默认为 0。
enumerate 返回一个迭代器，每次迭代会返回一个包含两个值的元组：第一个值是元素的索引，第二个值是元素本身。

```
lis = ['fuck', 'you', 'shit']
for index, value in enumerate(lis, start=1):
    print(f'index:{index}, value:{value}')
```

## 遍历字典

```
for k, v in my_dict.items():
```

```
for key in my_dict.keys():
```

## 遍历文件

列出当前文件夹中所有文件

```
os.listdir()
```

创建文件夹

创建文件夹多级目录

```
os.mkdir()
os.mkdirs()
```

创建文件夹路径

```
os.path.join(os.getcwd(), file)
```

## 读取文件

按照指定1-10的形式，按文件名读取文件

```
sorted_files = sorted(listdir, key=lambda x: int(x))
```

读取文件：先open，然后read，最后close（建议使用with open，结尾自动关闭文件）

```
with open('example.txt', 'r') as file:
    file_content = file.read()

# 自动逐行提取
with open('example.txt', 'r') as file:
    for line in file:
        print(line.strip())
```

读取整个、读取行、读取行列表

注意：读取时会标记索引，会从上次读取的地方继续读取数据，如需要重新标定索引，使用：f.seek(0)

```
read()
readline()
readlines()
```

## Numpy

### 数组类型

默认：int32，float64

```
arr4 = np.array([1.2, 3.4], dtype='float32')
```

修改数组类型

```
data = data.astype('int64')
```

### 生成数组

#### 指定个数（固定范围）

左闭右闭

```
linspace = np.linspace(0, 10, 5)
```

#### 指定步长（固定范围）

左闭右开

```
arange = np.arange(0, 10, 2)
```

#### 均匀分布（随机）

左闭右开

```
arr1 = np.random.uniform(low=0, high=1, size=[3, 4])
```

#### 正态分布（随机）

```
arr1 = np.random.normal(8, 0.4, size=[3, 4])
```

### 按行/列取值

```
print(data[0][:3])  # == data[0][0:3] 左闭右开

print(np.min(arr1, axis=0))  # 按列求最小值
print(np.max(arr1, axis=1))  # 按行求最大值

print(np.argmin(arr1, axis=0))  # 按列取最小值位置索引
print(np.argmax(arr1, axis=1))  # 按行取最大值位置索引
```

### 合并拼接

```
# np.concatenate() 行方向/列方向拼接均可
print(np.concatenate([np.array([1, 2, 3]), np.array([4, 5, 6])], axis=0))
print(np.concatenate([np.array([[1, 2], [3, 4]]), np.array([[5, 6]]).T], axis=1))
```

## Pandas

### index和column修改

```
方法1：df.columns = ['New_Column1', 'New_Column2', 'New_Column3']
方法2：df.rename(columns={'Old_Column1': 'New_Column1', 'Old_Column2': 'New_Column2', ...}, inplace=True)
```

### 删除列

```
# 删除部分无用列数据
data = csv.drop(['ma5', 'ma10', 'ma20', 'v_ma5', 'v_ma10', 'v_ma20'], axis=1)
```

### 索引操作

```
# 直接索引（注意先列后行）
print(data['open']['2018-02-26'])
# 按名字索引（先行后列）
print(data.loc['2018-02-26', 'open'])
# 按位置索引
print(data.iloc[1, 0])
```

### 排序

```
# 在'high'相同的情况下，将按照'close'进行排序
print(data.sort_values(by=['high', 'close'], ascending=False))
```

### 读取/保存数据

```
读取时，header=None 表示数据中没有列名，可使用names=[]参数 手动添加字段作为列名
```

```
# 读取指定的2列数据 usecols=['open', 'high']
csv = pd.read_csv('stock_day/stock_day.csv', usecols=['open', 'high'])
csv.head(10).to_csv('stock_day/指定open列前10行数据保存测试.csv', columns=['open'])

# 发现保存了索引列数据（同时该列并没有指定对应列名），如果不需要该列可舍弃 index=False
csv.head(10).to_csv('stock_day/指定open列前10行数据保存测试.csv', columns=['open'], index=False)

# 如果在每次保存后需要追加数据：指定参数 mode=a，这里在上面的保存后追加了2次数据保存
for i in range(2):
    csv.head(10).to_csv('stock_day/指定open列前10行数据保存测试.csv',
                        columns=['open'], index=False, mode='a')

# header=False 不保存列名
csv.head(10).to_csv('stock_day/指定open列前10行数据保存测试.csv',
                    columns=['open'], index=False, mode='a', header=False)
```

### 缺失值填充

```
# 使用pd.to_numeric将所有列尝试转换为数值类型，无法转换的将自动变为NaN
df = data.apply(pd.to_numeric, errors='coerce')
# 之后的操作和对NaN进行替换/插补操作相同
null_list = pd.isnull(df).any()
for col in null_list.index:
    if null_list.get(col):
        means = df[col].astype(float).replace(to_replace=np.NAN, value=0).mean()
        df[col].fillna(means, inplace=True)
print(df)
```

### 分组和聚合

   color   object  price1  price2
0  white      pen    5.56    4.75
1    red   pencil    4.20    4.12
2  green   pencil    1.30    1.60
3    red  ashtray    0.56    0.75
4  green      pen    2.75    3.15

```
print(col.groupby(by="color")["price1"].max())
```



