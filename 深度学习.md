# 1.线性模型

核心思想：

X是n维，Y是m维，则W一定是n×m维。

X：左横右竖；Y：朝向与X相同。

情况1：X(1,n) × W(n,m) = Y(1,m)

情况2：W^T(m,n) × X(n,1) = Y(m,1)

（以上均为针对单个样本下的计算）

设每次计算单个W对N个样本的的取值结果，则每次N个样本将输出N个预测值构成n维向量，将这个预测向量与实际向量（即标签）进行误差对比，对应的N个样本元素做减法，求平方，再除以N（即取N个样本的均值），可得出MSE，也就是损失函数。

每次对W进行测试都将输出一个损失值（针对n个样本的），则k次测试将生成k个损失值。最终目标是确定k次测试中的某个W，使得对应损失值最小。

**总结：**

1。使用单个权重，去乘以多个样本，获取结果作为预测值集合。

2。将预测值集合与真实值集合计算获取单个权重下的误差。

3。对权重更新，使得对应误差最小。

# 2.梯度下降

这里，如何选择最优的权重向量W？可以尝试多次，这一般在机器学习中也称为epoch。

可以采用visdom，在迭代时同时展示损失值，方便查看模型是否收敛。



这里还有更好的方式：梯度下降（正规方程一般不使用）

梯度下降原理：设损失函数为L，对L求W（权重）的导数，通过导数大于或者小于0，判断L基于W下降的方向，不断靠近L的最低点（贪心算法）。 

W更新：W=W-αL‘，这里α是学习率，即每次向最低点移动的步长，过大会导致模型无法收敛。

局部最优：由于点是连续进行移动的，当点移动至某个位置时，导数可能接近0，但是此时只是局部最优点，而不是全局最优点（但实际上深度学习中，这样的局部最优点很少）。

鞍点：一段曲线内，导数值均为0，将导致点无法移动，模型无法继续迭代，解决方式：随机梯度下降（SGD）：在计算误差时，每次只使用一个样本。如每次选用一小组样本，则称为mini-Batch方法。

# 3.反向传播

在图上进行梯度传播，创建更具有弹性的模型结构。

<img src="笔记图保存\fb67ec5e2611c675e44696d20c730dc3.png" alt="fb67ec5e2611c675e44696d20c730dc3" style="zoom: 33%;" />

输入X是5维向量，第1层h(1)输出的Y是6维向量，则W只能是5行6列，即权重矩阵需要30个元素。

这里的点看作是X中的每个特征，连接线看作是W与X进行线性组合的匹配计算过程，例如这里：

<img src="笔记图保存\78c83073c16ff4f5b488d4445d6a41db.png" alt="78c83073c16ff4f5b488d4445d6a41db" style="zoom: 50%;" />

这里的神经元：针对变量指的是向量维数，而针对矩阵则是通道数量。

<img src="笔记图保存\0d7c23ddef101e97c105c132ac5b90b0.png" alt="0d7c23ddef101e97c105c132ac5b90b0" style="zoom:50%;" />

在一个神经网络中，权重*输入+偏置量=第1层

<img src="笔记图保存\2cd40aba1e9f50bbb868d767a113cb41.png" alt="2cd40aba1e9f50bbb868d767a113cb41" style="zoom: 33%;" />

问题：如果每次都类似上面进行计算，函数展开后形式将始终不变化，变换失去意义。为此，在每次结果输出时需要对结果向量做一个非线性函数的变换（即激活函数）。

<img src="笔记图保存\fa7ae0ef402295b7136744f5146b4dd5.png" alt="fa7ae0ef402295b7136744f5146b4dd5" style="zoom: 33%;" />

回忆链式法则（chain rule）：(f(g(x)))' = f'(g(x)) * g'(x)，它解决了什么问题？其实就是嵌套过多层函数时，多层求导难度较大的问题。也就是在多层网络中，L对W如何得出求导结果的问题。

<img src="笔记图保存\PixPin_2025-08-18_12-01-18.png" alt="PixPin_2025-08-18_12-01-18" style="zoom:33%;" />

这里，Z表示X和W的运算规则，知道Z函数表达式即可轻松获取Z对X导数、Z对W导数。

之后从Loss处返回损失函数L对于Z的导数，

目标是计算L对X的偏导数（这里需要计算的原因是，X不一定就代表数据输入，它可能是一个中间层输出，因此需要有对它进行计算的能力）、以及L对W的偏导数，此时再采用链式法则，由于先前Z对X导数、Z对W导数、L对于Z导数都已经获取，可直接计算出结果。

整体上，先前馈传播，再反向传播，最终得出L对W的梯度（导数）、L对X的梯度。

<img src="笔记图保存\bd4544408b1713c020fb50ddb2f0b4c6.png" alt="bd4544408b1713c020fb50ddb2f0b4c6" style="zoom: 33%;" />

PyTorch中最基础的数据结构是Tensor，它可以存储一维向量、矩阵、多维矩阵等，它是一个类，属性包括W的数值和损失函数对W的导数（这里data和grad均为Tensor（张量））。

```python
import torch

#这里以列表形式设置了初始数据
x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

#设置初始权重值
#可以看到这里设置的是一维权重，因为x_data中每列是一个样本，且单个样本的特征维度是1，Y的维度也是1。
w = torch.Tensor([1.0])
#此处设置了需要计算梯度。即设定之后会需要对这个参数进行更新。
w.requires_grad = True

#forward 前向传播（前面提及的Z函数的表达式）
def forward(x):
    return x * w
#这里w是Tensor，所以x * w自动重载为两个Tensor间的数乘运算，x直接被自动转换为Tensor类型以参与计算。
#针对此处，因为输入时w是需要计算梯度的，所以输出结果z=x * w这个Tensor也被设置为需要计算梯度（回忆之前内容中，存在着L对z求梯度的过程）
 
#定义损失函数，这里每运行一次，就构建一个“计算图”
def loss(x, y):
    y_pred = forward(x)
    return (y_pred - y) ** 2
 
#输出预测数值
print("predict (before training)", 4, forward(4).item())

#设置epoch=100，进行100次完整的样本训练
for epoch in range(100):
    #在每个epoch中，每次是针对单个样本计算，这里的计算方式是随机梯度下降SGD
    for x, y in zip(x_data, y_data):
        #构建“计算图”，获取损失函数的Tensor
        l = loss(x, y)
        #调用backward函数后，将“计算图”中所有设置的-需要梯度的地方计算对应梯度，并保存在对应Tensor中。之后释放“计算图”。
        #这里释放“计算图”，是因为每次构建的“计算图”可能是不同的，因此选择不保留，这种方式也使得Pytorch较为灵活。
        #之后如何获取梯度值：w.grad.data，注意不是直接w.grad，因为w.grad是一个Tensor，不能直接参与计算。
        #区别：w.grad.data允许在张量层面进行运算并对梯度值进行更新，而w.grad.item()则只能在标量层面进行操作。基于w.grad.data这种方式可对梯度进行更新，同时不会修改“计算图”。
        l.backward()
        #展示梯度值和权重数值
        print('\tgrad:', x, y, w.grad.item(), w.data)
        #W更新：W=W-αL'
        w.data = w.data - 0.01 * w.grad.data
        print(w.data)
        #PyTorch 会自动将梯度累积到.grad属性中。也就是说，如果不清除梯度，它们会在每次迭代时叠加，导致梯度越来越大。w.grad.data.zero_()用于将梯度清空为0。
        w.grad.data.zero_()
        #展示更新后的权重数值和更新后的梯度（这里是0）
        print(w.data, w.grad.item())
	#展示每个epoch中第3个样本对应的损失值
    print('progress:', epoch, l.item())
 
print('predict (after taining)', 4, forward(4).item())

#总结：总计3个步骤：
#1.前向：算损失
#2.反向：算梯度
#3.更新：更新W权重
```

```
print(w)  # Tensor，需构造计算图
print(w.data)  # Tensor，不构造计算图
print(w.data.item())
print(w.grad)  # Tensor，构造计算图
print(w.grad.item())
print(w.grad.data)  # Tensor，不构造计算图
print(w.grad.data.item())

w.grad.data这种操作，目前已过时，更推荐使用：
with torch.no_grad():
    w.grad += 1  # 安全地修改梯度

OUT PUT:
tensor([2.0000], requires_grad=True)
tensor([2.0000])
2.0000007152557373
tensor([0.])
0.0
tensor([0.])
0.0
```

# 4.用PyTorch实现线性回归

广播机制：不同形状的矩阵计算中，对矩阵进行自动扩充。即将空的维度部分复制补充至最高维。

<img src="笔记图保存\4c3e3ae5d09a602b788a84e05967f9c8.png" alt="4c3e3ae5d09a602b788a84e05967f9c8" style="zoom:50%;" />

```python
import torch

# 和先前不同的地方在于这里将初始数据直接设置为张量，数据集：样本量为3，X和Y的特征维度均为1。
x_data = torch.Tensor([[1.0], [2.0], [3.0]])
y_data = torch.Tensor([[2.0], [4.0], [6.0]])


# 自己想要实现的方式Pytorch难以实现：
# 1.如果可以使用Python基础方法实现，则将其封装为模型类（继承torch.nn.Module）。
# 2.如果求导效率不高，需要自己手动实现反向传播的计算块，可以继承Functions类，如继承Functions类则需要手动额外实现反向传播（因为反向传播过程涉及求导）。

# 这里将模型定义为继承自Module的类（因Module中具有较多合适方法）
class LinearModel(torch.nn.Module):
    # 这里模型至少需要设置2个函数：构造函数和forward函数
    # 1）设置构造函数，初始化对象时可进行属性定义
    def __init__(self):
        super(LinearModel, self).__init__()
        self.linear = torch.nn.Linear(1, 1)
        # torch.nn.Linear是一个类，此处在构造对象，torch.nn.Linear(in features, out features, bias=True)
        # 这里，in features=每一个输入样本（X）的特征维数，out features=每一个输出样本（Y）的维数，bias=是否需要偏置项，这里通过in features和out features可确定每一个权重矩阵W的形状。

    # 2）必须定义并实现forward函数，用于定义前馈计算Z
    # 此处实际上是重载了父类方法，类似_call_方法，当调用对象时会直接使用该函数中的内容。也称为magic method
    # 之后要使用该类时，直接：model=LinearModel(), model(x)即可，其内部做的就是：wx+b，返回的是y_pred
    def forward(self, x):
        y_pred = self.linear(x)
        return y_pred


# _call_方法示例：
class Shit:
    def __init__(self):
        pass

    def __call__(self, *args, **kwargs):
        print(args)
        print(kwargs)


# 当实例化对象时，隐式调用的是__init__
shit = Shit()
# 当直接调用对象时，调用的是__call__
shit(1, 2, 3, x=6, y=7)
'''
输出结果：
(1, 2, 3)
{'x': 6, 'y': 7}
'''

# 定义线性模型对象
model = LinearModel()

# 定义损失函数，此处设置为MSE，在 PyTorch 的新版本中，size_average 已经被弃用，
# 取而代之的是 reduction 参数。这个参数有三个选项：
# reduction='mean'：类似于 size_average=True，计算损失的平均值。
# reduction='sum'：类似于 size_average=False，计算损失的总和。
# reduction='none'：不进行任何平均或求和，返回每个样本的损失值。
criterion = torch.nn.MSELoss(reduction='mean')

# 构建对W进行更新的优化器（W=W-αL'），此处设置SGD随机梯度下降
# 但是实际上，SGD是一个方法类，确定是使用单个样本，还是全样本，还是mini-batch，是根据输入数据确定。类似此处使用的就是全样本进行统一计算，对所有样本计算损失均值，并更新W权重。Batch-SGD。
# 实例化了一个SGD类，model是LinearModel，LinearModel里有linear，而linear有parameters。
# 这里使用model.parameters()，目的是找出所有LinearModel中需要进行更新的权重参数。
# 设定固定学习率为0.01
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# 训练过程，这里对全样本进行计算更新
for epoch in range(1000):
    # 1.前馈计算，算出y_pred
    y_pred = model(x_data)
    # 2.计算损失，传入y_pred（预测）和y_data（真实），返回损失的Tensor
    loss = criterion(y_pred, y_data)
    # 展示迭代次数和对应Tensor的损失数值
    print(epoch, loss.data.item())
    # 3.先将优化器中的梯度值清零（防止在上一次迭代中遗留下W的梯度）
    optimizer.zero_grad()
    # 4.利用损失的Tensor进行反向传播
    loss.backward()
    # 5.利用优化器进行权值W和偏置b的迭代
    optimizer.step()

print('w = ', model.linear.weight.item())
print('b = ', model.linear.bias.item())

x_test = torch.Tensor([[4.0]])
y_test = model(x_test)
print('y_pred = ', y_test.data)
```

不同优化器的效果展示：

torch.optim.Adagrad
torch.optim.Adam
torch.optim.Adamax
torch.optim.ASGD
torch.optim.LBFGS
torch.optim.RMsprop
torch.optim.Rprop
torch.optim.SGD

<img src="笔记图保存\50606a723f3b467257ce560d69a616d8.png" alt="50606a723f3b467257ce560d69a616d8" style="zoom: 33%;" />

# 5.Logistic（逻辑）回归

n分类问题：和回归问题不同，分类模型的输出应该是和为1的n个概率值，并最终判别结果属于哪个类（概率最大项）。

P(x∈Z) = n

Logistic回归是针对二分类问题的。

经典激活函数Sigmod：用于将线性回归的结果映射至[0, 1]区间内。其导函数类似正态分布形状。

考虑：当线性回归的预测值非常大（正负）时，Sigmoid函数的输出会接近0或1吗？

实际上是会的，解决方案：

1. 特征标准化（缩放特征为0至1范围内）。

2. L1、L2正则化，防止W矩阵调节程度过大。

3. 使用其它激活函数。


<img src="笔记图保存\7ed7aefe0d4cd592c66aea4fc7bc0bf0.png" alt="7ed7aefe0d4cd592c66aea4fc7bc0bf0" style="zoom: 33%;" />

常见的Sigmod函数：

<img src="笔记图保存\PixPin_2025-08-18_16-34-38.png" alt="PixPin_2025-08-18_16-34-38" style="zoom: 33%;" />

使用激活函数之后，仍只能获取范围在[0 ,1]区间的预测值集合。

此时损失函数不再是计算数值之间的距离，而是用于计算两个分布概率值之间的差异。可用KL散度（又称相对熵）、交叉熵等作为损失函数。

例如BCE损失（Binary Cross-Entropy Loss，二元交叉熵损失）的计算公式：

<img src="笔记图保存\4a07772035b7687da9bafad4d2cc8d9c.png" alt="4a07772035b7687da9bafad4d2cc8d9c" style="zoom:33%;" />

当Y_ac（实际值）=1时，Y_pre（线性回归结果经Sigmod映射后的数值）越接近1，将使得整体函数数值越小（类似先前内容中，预测值和真实值越接近，会导致损失函数越小）；Y_pre越接近0，整体函数数值越大。

当Y_ac=0时，Y_pre越接近0，整体函数数值越小；Y_pre越接近1，整体函数数值越大。

<img src="笔记图保存\4a0084535328759b85fce27478daba17.png" alt="4a0084535328759b85fce27478daba17" style="zoom:33%;" />

```python
import torch
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

plt.rcParams['font.family'] = 'Times New Roman'
plt.rcParams['font.size'] = '18'
matplotlib.use('TkAgg')

x_data = torch.Tensor([[1.0], [2.0], [3.0]])
# 真实数据Y变为类别0, 1，而不是数值类型
y_data = torch.Tensor([[0], [0], [1]])


class LogisticRegressionModel(torch.nn.Module):
    def __init__(self):
        super(LogisticRegressionModel, self).__init__()
        self.linear = torch.nn.Linear(1, 1)

    # 区别1，前向传播对预测值进行计算时，添加了sigmoid函数映射
    def forward(self, x):
        y_pred = torch.sigmoid(self.linear(x))
        return y_pred


model = LogisticRegressionModel()
# 区别2，定义BCE为损失函数
criterion = torch.nn.BCELoss(reduction='mean')
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

for epoch in range(1000):
    y_pred = model(x_data)
    loss = criterion(y_pred, y_data)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# 针对不同的x生成预测值并展示
x = np.linspace(0, 10, 200)
# reshape操作
x_t = torch.Tensor(x).view((200, 1))
y_t = model(x_t)
y = y_t.data.numpy()
plt.plot(x, y)
plt.plot([0, 10], [0.5, 0.5], c='r')
plt.xlabel('Hours')
plt.ylabel('Probability of Pass')
plt.grid()
plt.show()
```

# 6.处理多维特征的输入

假设对8维特征的N个样本进行Mini-Batch计算：

注意此处Sigmod函数进行的是向量化运算（即对一个列表中的每个元素，进行逐个元素的计算）。偏置项b直接通过广播机制进行扩充。

这里，Z代表函数值。X矩阵中，每行代表一个样本。

<img src="笔记图保存\3abb0df78cc8b50d3386dc0803db1937.png" alt="3abb0df78cc8b50d3386dc0803db1937" style="zoom:33%;" />

假设需要按照上面的方式进行调整，只需修改此处模型的输入维数即可。 

为什么有时会调整输出维度：

可以看作，这里只是调整中间步骤输出的维度，需要在进行多次变换后，最后再次转换为一维输出结果。

思想：通过多个层的线性变换，去拟合非线性变换。

神经网络目的：寻找一种非线性变换的空间函数。

```python
import numpy as np
import torch
from sklearn.datasets import load_diabetes

diabetes = load_diabetes()
x = diabetes.data
y = diabetes.target

# 这里使用32位浮点数，而不使用64位Double，是因为N卡一般支持32位浮点。
xy = np.array(np.concatenate([x, np.array([y]).T], axis=1), dtype=np.float32)
# 取numpy数据构造tensor
x_data = torch.from_numpy(xy[:, :-1])
# 这里[-1]表示取出的是矩阵，否则是向量
y_data = torch.from_numpy(xy[:, [-1]])


class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.linear1 = torch.nn.Linear(10, 6)
        self.linear2 = torch.nn.Linear(6, 4)
        self.linear3 = torch.nn.Linear(4, 1)
        # 这里定义Sigmoid模块，注意和之前定义的函数torch.sigmod()是不同的，这里定义一个模块用于多次调用，构建计算图。
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x):
        # 这里每层都可以设置不同的激活函数
        x = self.sigmoid(self.linear1(x))
        x = self.sigmoid(self.linear2(x))
        # 这里如果设置激活函数为Relu，所有小于0的数值将映射为0，因此最后一层一般设置为sigmod
        x = self.sigmoid(self.linear3(x))
        return x


model = Model()
criterion = torch.nn.BCELoss(reduction='mean')
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

for epoch in range(100):
    y_pred = model(x_data)
    loss = criterion(y_pred, y_data)
    print(epoch, loss.item())
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

不同激活函数的对比：

[torch.nn — PyTorch 2.5 documentation](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity)

<img src="笔记图保存\ce09fa1d32a28e0e735be501e744f704.png" alt="ce09fa1d32a28e0e735be501e744f704" style="zoom: 50%;" />

# 7.加载数据集

**对比 SGD 和 Mini-batch GD**

- **纯 SGD（单样本）**：每次迭代使用一个样本，随机性最强，有助于逃离局部极小值。**单次迭代速度快，但收敛过程不稳定，需要更多迭代才能达到较好的收敛效果**。
- **Mini-batch SGD**：每次迭代使用一个小批量样本（例如 Batch Size=32 或 64）。兼顾了**收敛速度、稳定性、计算效率**。
- **Batch GD（全数据）**：没有随机性，每次迭代使用全数据，存在鞍点问题。单次迭代**最慢**，不适合大规模数据。

因此，**Mini-batch SGD** 是实际应用中最常用的形式，它在效率和梯度波动之间找到了一个平衡点。



常用词汇解释：

**Epoch**：整个数据集被完整训练一次。

**Batch Size**：每次迭代时所处理的数据量。

**Iteration**：一次权重更新，处理一个Batch。

1个Epoch中，Iteration = 数据样本总数 / Batch Size

<img src="笔记图保存\f5d22739127664a70c8221a9b623b76d.png" alt="f5d22739127664a70c8221a9b623b76d" style="zoom: 33%;" />

Dataset：用于数据索引

DataLoader：用于数据加载mini-batch

<img src="笔记图保存\108f832436b8c37f1534360adfc7a0d7.png" alt="108f832436b8c37f1534360adfc7a0d7" style="zoom:33%;" />

```python
import numpy as np
import pandas as pd
import torch
from sklearn.datasets import load_diabetes

# 这里，Dataset是一个抽象类（不能被实例化，只能被继承）
from torch.utils.data import Dataset, DataLoader

# 读取并创建数据集
diabetes = load_diabetes()
x = diabetes.data
y = diabetes.target
data = np.array(np.concatenate([x, np.array([y]).T], axis=1), dtype=np.float32)
pd.DataFrame(data).to_csv('diabetes.csv', index=False)


class DiabetesDataset(Dataset):
    # 初始化读取数据：一般2种方式
    # 1.如果数据集不大，可直接全部加载进入内存。
    # 2.如果数据集较大，仅保存数据样本的索引，在getitem被调用时再实时读取数据。
    def __init__(self, filepath):
        xy = pd.read_csv(filepath)
        self.len = xy.shape[0]
        self.x_data = xy.iloc[:, :-1].values.astype('float32')
        self.y_data = xy.iloc[:, xy.shape[1] - 1:xy.shape[1]].values.astype('float32')

    # 这里__getitem__是一个magic method，当对象使用下标时自动调用该方法
    # 此处返回元组
    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    # 这里__len__也是一个magic method，当对象调用len()时自动调用该方法
    def __len__(self):
        return self.len


dataset = DiabetesDataset('diabetes.csv')
# 这里，num_workers代表设置去读取batch_size数据的并行线程数。当数据集较小时，使用多线程反而可能降低运行速度。
train_loader = DataLoader(dataset=dataset,
                          batch_size=32, 
                          shuffle=True,
                          num_workers=2)


class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.linear1 = torch.nn.Linear(10, 6)
        self.linear2 = torch.nn.Linear(6, 4)
        self.linear3 = torch.nn.Linear(4, 1)
        self.sigmoid = torch.nn.Sigmoid()

    def forward(self, x):
        x = self.sigmoid(self.linear1(x))
        x = self.sigmoid(self.linear2(x))
        x = self.sigmoid(self.linear3(x))
        return x


model = Model()
criterion = torch.nn.BCELoss(reduction='mean')
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# 此处设置if __name__ == '__main__'，是为了防止进程并发时发生 运行时异常 错误
if __name__ == '__main__':
    # 设置epoch为100
    for epoch in range(100):
        # 这里采用enumerate获取iteration次数i
        # 此处，train_loader将X和Y数据进行封装，形成各个Batch下的数据矩阵（X和Y分别形成矩阵）
        for i, data in enumerate(train_loader, 1):
            inputs, labels = data
            y_pred = model(inputs)
            loss = criterion(y_pred, labels)
            print(epoch, i, loss.item())
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

```

```python
import numpy as np
import torch
import torch.nn as nn
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from torch.utils.data import Dataset, DataLoader

# 使用 sklearn 内置数据
data = load_diabetes()
x = data.data
y = data.target.reshape(-1, 1)

# 划分训练集和测试集
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42
)


# 自定义 Dataset
class MyDataset(Dataset):
    def __init__(self, x, y):
        self.x = torch.from_numpy(x).float()
        self.y = torch.from_numpy(y).float()

    def __getitem__(self, index):
        return self.x[index], self.y[index]

    def __len__(self):
        return len(self.x)


train_loader = DataLoader(dataset=MyDataset(x_train, y_train), batch_size=32, shuffle=True, num_workers=0)
val_loader = DataLoader(dataset=MyDataset(x_test, y_test), batch_size=32, shuffle=False, num_workers=0)


# 定义模型
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(10, 8),
            nn.ReLU(),
            nn.Linear(8, 4),
            nn.ReLU(),
            nn.Linear(4, 2),
            nn.ReLU(),
            nn.Linear(2, 1)
        )

    def forward(self, x):
        return self.network(x)


# 初始化
model = Net()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.MSELoss()

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)

# 训练
epochs = 200
for epoch in range(epochs):
    model.train()
    for i, (inputs, labels) in enumerate(train_loader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        output = model(inputs)
        loss = loss_fn(output, labels)
        loss.backward()
        optimizer.step()

    if (epoch + 1) % 50 == 0:
        print(f"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}")

# 测试
model.eval()
all_preds = []
all_trues = []

with torch.no_grad():
    for inputs, labels in val_loader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)

        all_preds.append(outputs.cpu().numpy())
        all_trues.append(labels.cpu().numpy())

# 合并所有 batch 的结果
pred = np.concatenate(all_preds).flatten()
true = np.concatenate(all_trues).flatten()

# 计算指标
mse = mean_squared_error(true, pred)
rmse = np.sqrt(mse)
mae = mean_absolute_error(true, pred)
r2 = r2_score(true, pred)

print("\n" + "=" * 30)
print("最终测试集性能评估：")
print("=" * 30)
print(f"MSE:  {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"MAE:  {mae:.4f}")
print(f"R²:   {r2:.4f}")
print("=" * 30)

```

<img src="笔记图保存\6d409f6ee94cddcfec9afeb85f5ee46e.png" alt="6d409f6ee94cddcfec9afeb85f5ee46e" style="zoom:33%;" />

总结流程顺序：1.准备数据集 2.设置模型 3.构造损失和优化器 4.训练周期

# 8.多分类问题

第1种方式：将多分类看作多个二分类问题，将某种类别标签记作1，其它类别标签记作0，针对每种类别构造并训练分类器。

“一对多”方法的最大缺点是它将每个类别独立看待，忽视了类别之间的相似性、关系和层次结构。它通过多个独立的二分类器来处理多分类问题，但这可能导致以下问题：

1. 类别之间的相似性被忽视。
2. 决策边界可能过于复杂，不自然。
3. 类别之间的决策可能不一致。
4. 无法捕捉类别的层次结构。
5. 无法处理“新类别”的问题。

所希望的针对各类别的输出：

1.输出均大于零，

2.输出的和等于1，

解决方案：神经网络保持先前的sigmod层不变，输出层使用softmax。

这里，直接控制输出的预测值y_pre维数为样本类别数：

例如，设样本类别为k（k分类问题），输入x为n维（即特征维数），则进行变换的权重矩阵w的形状（n，k）。

<img src="笔记图保存\4285a03e0b7f7cf41970e730c3e4d3c8.png" alt="4285a03e0b7f7cf41970e730c3e4d3c8" style="zoom: 33%;" />

<img src="笔记图保存\PixPin_2025-08-18_16-58-14.png" alt="PixPin_2025-08-18_16-58-14" style="zoom: 33%;" />

对数似然损失：

将标签构造成为One-hot编码形式，例如有5种类别，该样本真实样本标签属于第4类，则对应标签表示为：[0，0，0，1，0]

将Softmax层输出的预测值结果（例如：[0.3，0.2，0.1，0.3，0.1]）和该列表（[0，0，0，1，0]）计算对数似然损失。因为计算过程中的真实标签有很多0项所以相乘为零，只看真实项（此处实际只计算第4类）。

为什么会设计这样的函数：目标是希望第4类预测值结果输出地尽可能接近1，当Y=1时，Y_pre越接近1，整体损失越接近0；Y_pre越接近0，损失越接近无穷大。

<img src="笔记图保存\904a9232babf7377f47b88a0c32de121.png" alt="904a9232babf7377f47b88a0c32de121" style="zoom:33%;" />

交叉熵：Softmax + Negative Log Likelihood Loss

<img src="笔记图保存\981e6b664c88e509a8a0fe35f254f44f.png" alt="981e6b664c88e509a8a0fe35f254f44f" style="zoom:33%;" />

对数据损失计算过程进行测试验证：

```python
import torch

'''
对数据损失计算过程进行测试验证
'''

# 定义损失函数，此处设置reduction='none'，返回每个样本的损失值而不是批量计算均值
criterion = torch.nn.CrossEntropyLoss(reduction='none')

# 设置真实样本类别
Y = torch.LongTensor([2, 0, 1, 0])

# 这里有4个样本，3个类别，在输入softmax层之前已控制每个样本的输出维度为3。
# 针对4个样本，依次大概率预测：2,0,1,0
Y_pred1 = torch.Tensor([[0.1, 0.2, 0.9], 
                        [1.1, 0.1, 0.2],
                        [0.2, 2.1, 0.1],
                        [1.2, 0.1, 0.3]])
# 大概率预测：0,2,2,1
Y_pred2 = torch.Tensor([[0.8, 0.2, 0.3],
                        [0.2, 0.3, 0.5],
                        [0.2, 0.2, 0.5],
                        [0.1, 0.7, 0.4]])

l1 = criterion(Y_pred1, Y)
l2 = criterion(Y_pred2, Y)
print("Loss1 = ", l1.data, "\nLoss2 = ", l2.data)

```

每张图像都可以看作矩阵形式，使用8位（1字节）来表示每个像素的灰度值，8位可以表示的整数范围是0到255（2^8=256）。

将（0-255）进行标准化，则可以构成数据集矩阵。例如下图是（1，28，28）（C(channel)，W(width)，H(height)）。

channel指颜色通道，RGB通道数为3，此处黑白则为1。

<img src="笔记图保存\dc115f30-be5c-4c96-a370-dbbc2695a18d.png" alt="dc115f30-be5c-4c96-a370-dbbc2695a18d" style="zoom:33%;" />

```python
import torch
from torchvision import transforms
from torchvision import datasets
from torch.utils.data import DataLoader
import torch.nn.functional as F

# 设置批量处理的样本数=64
batch_size = 64
# 构造对图像进行处理的转换器transform，对每个元素进行进行pipline处理：
# 首先将每张图片转换为（C，W，H）的Tensor形式，
# 然后将每个Tensor进行标准化缩放，此处缩放至均值0.1307，标准差0.3081的正态分布（MNIST数据集图像的均值和标准差）。
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307,), (0.3081,))
                                ])

# 加载训练和测试数据集，并构造Dataset和DataLoader
train_dataset = datasets.MNIST(root='../dataset/mnist/',
                               train=True,
                               download=True,
                               transform=transform)
train_loader = DataLoader(train_dataset,
                          shuffle=True,
                          batch_size=batch_size, num_workers=0)
test_dataset = datasets.MNIST(root='../dataset/mnist/',
                              train=False,
                              download=True,
                              transform=transform)
test_loader = DataLoader(test_dataset,
                         shuffle=False,
                         batch_size=batch_size, num_workers=0)


class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 这里为何输入是784？因为将28*28的矩阵按行展开后再拼接，构造了一个（1，784）的向量。
        # 全连接网络通常要求输入是一维向量。
        # 因此，如果将一个 28x28 的图像输入到一个全连接网络（例如MLP）时，
        # 通常需要将其展平（flatten）为一个一维向量。
        # 这是因为全连接层将每个输入像素与一个神经元连接，不能直接处理多维结构。
        # 此处以全连接网络为例
        self.l1 = torch.nn.Linear(784, 512)
        self.l2 = torch.nn.Linear(512, 256)
        self.l3 = torch.nn.Linear(256, 128)
        self.l4 = torch.nn.Linear(128, 64)
        # 为何输出10？因为MNIST数据集的图像有10种类别（0-9）
        self.l5 = torch.nn.Linear(64, 10)

    def forward(self, x):
        # 通过view()函数改变Tensor形状。-1代表设置列数后，行数将进行自动计算。
        x = x.view(-1, 784)
        x = F.relu(self.l1(x))
        x = F.relu(self.l2(x))
        x = F.relu(self.l3(x))
        x = F.relu(self.l4(x))
        # 返回时，最后一层无需再做Relu激活，因为交叉熵函数自身将使用Softmax进行激活，并计算对数似然损失
        return self.l5(x)


# 设置模型在GPU上运行
# 2步：1将模型迁移至GPU；2将数据迁移至GPU
model = Net()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)

# 设置损失函数和优化器
# 定义交叉熵损失函数
criterion = torch.nn.CrossEntropyLoss(reduction='mean')
# momentum=0.5：动量方法通过考虑过去梯度的累积来调整当前梯度更新的方向和幅度。缩短训练时间。
# 通常在0到1之间，越接近1，历史梯度的影响越大。
# 简单来说，动量方法在每一步的梯度更新中不仅考虑当前的梯度，还会加上一部分之前更新方向的“记忆”，
# 从而在优化过程中起到“惯性”作用。
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)


# 分别将每个epoch的训练和测试过程封装为函数
def train(epoch):
    # 定义累计损失
    running_loss = 0.0
    for batch_idx, data in enumerate(train_loader, 0):

        inputs, target = data
        inputs, target = inputs.to(device), target.to(device)
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()

        # 每300轮iteration输出1次平均损失（对累计损失求300次的平均），而不是输出所有iteration的损失
        running_loss += loss.item()
        if batch_idx % 300 == 299:
            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))
            running_loss = 0.0


# 定义测试过程的函数
def test():
    # 定义正确数量和总数量
    correct = 0
    total = 0
    # 设置不需要计算梯度，因为此处没有任何训练过程，只验证模型结果的准确性
    with torch.no_grad():
        # 多个Batch，这里的循环次数为：测试集样本总数/Mini-Batch(64)
        for data in test_loader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            # 针对单个Batch的样本，每次输出的是64个10维向量，10维表示其对10个类别的预测概率。
            # 目标是在对应类别的向量位置上，数据尽可能地相比其它类别大。
            # 注意此处还未经过Softmax层和对数似然损失，这里是Relu层输出，目标就是在计算损失前，让该层输出结果尽可能大。
            # （Softmax层不会改变原始数据的大小分布情况）
            outputs = model(images)
            # 针对64个样本按行拼接构成的矩阵，每次按列找出最大值及其对应索引。
            # 其中detach()表示获取数值。dim=1表示按列进行操作并计算最大值。返回=（最大值，最大值索引）。
            _, predicted = torch.max(outputs.detach(), dim=1)
            # 计算真实标签矩阵形状的第1个维度（这里是64）并进行累加
            total += labels.size(0)
            # 计算每64个样本里分类正确的样本数，并将其累加
            correct += (predicted == labels).sum().item()
    # 计算分类的准确率：即分类正确的样本数/总样本数
    print('Accuracy on test set: %d %%' % (100 * correct / total))


if __name__ == '__main__':
    # 在每个epoch中，每次训练完就进行测试，展示模型性能变化
    for epoch in range(10):
        train(epoch)
        test()

```

```python
# 生成特定Batch、特定维度、特定数量的训练和测试集，以对模型测试
import torch

num_batches = 5
batch_size = 32
train_loader = []
test_loader = []

for i in range(num_batches):
    data = torch.randn(batch_size, 1, 28, 28)
    target = torch.randint(0, 10, (batch_size,))
    train_loader.append((data, target))


for i in range(num_batches):
    data = torch.randn(batch_size, 1, 28, 28)
    target = torch.randint(0, 10, (batch_size,))
    test_loader.append((data, target))
```

# 9.卷积神经网络

相比先前全连接神经网络中，直接将图像矩阵展开并构造为一维向量形式。卷积神经网络选择保留图像空间信息，并将输入设置为原始图像矩阵，且首先需确定输入数据通道信息和输出的通道信息，步骤：

1 输入

2 特征提取（做卷积变换）

3 分类（将卷积最后层按照一维展开，并输入全连接层，再通过softmax层映射，对数似然损失）

<img src="笔记图保存\2f8e3e96-40ee-4d81-8b16-01c31c8d4149.png" alt="2f8e3e96-40ee-4d81-8b16-01c31c8d4149" style="zoom:33%;" />

卷积的过程：对每一小块图像进行遍历，遍历时对小块图像进行卷积操作。

<img src="笔记图保存\4c5a652c-c14b-42f0-a0f2-0f90c026461c.png" alt="4c5a652c-c14b-42f0-a0f2-0f90c026461c" style="zoom:33%;" />

单通道卷积操作：按卷积核对矩阵元素进行数乘，遍历整个图像，将结果数值放入输出矩阵。

<img src="笔记图保存\b22b8342-4a10-4dd2-af06-6e413729a2a7.png" alt="b22b8342-4a10-4dd2-af06-6e413729a2a7" style="zoom:33%;" />

多通道卷积操作：分别使用3个卷积核对每个通道进行卷积操作，并将获取的输出矩阵求和。

<img src="笔记图保存\80ee78aa-1a13-4ace-b925-c741dcf829e3.png" alt="80ee78aa-1a13-4ace-b925-c741dcf829e3" style="zoom:33%;" />

输入为n个通道时，采用的卷积核通道数也应为n，并输出通道数为1的卷积结果。

<img src="笔记图保存\efbfeb98-8d2f-47b7-a39d-74a09405c673.png" alt="efbfeb98-8d2f-47b7-a39d-74a09405c673" style="zoom:33%;" />

当进一步将结果输出通道数设定为m，则将以上处理的n通道卷积组数量增多至m个（在m次运算中，n通道卷积组都相同-称为共享权重），进行m次运算后将卷积结果堆叠可形成m个通道：

<img src="笔记图保存\0500c0a3-9d2e-40fb-8db9-ceee67de90a5.png" alt="0500c0a3-9d2e-40fb-8db9-ceee67de90a5" style="zoom:33%;" />

当输入通道数为n，输出通道数为m时，将卷积层设定为4维张量（m，n，w，h）：

类似先前线性回归时的矩阵乘法，通过输入和输出的通道维度，可确定权重矩阵的通道维度。

<img src="笔记图保存\f8d04610-b055-4d6c-a38d-6da561a4d79b.png" alt="f8d04610-b055-4d6c-a38d-6da561a4d79b" style="zoom:33%;" />

演示计算的进行过程：

```python
import torch

in_channels, out_channels = 5, 10
width, height = 100, 100
kernel_size = (3, 3)
batch_size = 1

# 随机生成一个批量大小为1，形状（5，100，100）（C，W，H）的输入张量
input = torch.randn(batch_size,
                    in_channels,
                    width,
                    height)

# 设置卷积层，必须的参数：输入通道、输出通道、卷积核形状
# 卷积层与图像输入的W和H没有关系
conv_layer = torch.nn.Conv2d(in_channels,
                             out_channels,
                             kernel_size=kernel_size)

output = conv_layer(input)

print(input.shape)
# 此处输出的W和H为什么是98：
# 因为卷积核是3，减去中心=2列，即在输入张量中，移动至极限时的左边距和右边距相加等于2，
# 使用100-2=98=卷积核在输入张量中部的可移动距离。
# 只要确定了输入张量的W和H，输出张量的W和H可自动确定。
print(output.shape)
print(conv_layer.weight.shape)

```

原始输入矩阵为5×5，卷积核3×3，因此结果=3×3（5-3+1），但如果想让输出结果=5×5（即保持输出尺寸的W、H=输入尺寸的W、H）？

对原始矩阵填充1圈均为0的padding，使结果输出矩阵变为5×5。

填充圈数：

x=填充后输入维度，input=i（填充前输入维度）, kernel=k（卷积核维度）, out=o（填充后输出维度）,

x-(k-1)=o, 则x=o-1+k,

圈数=(填充后输入维度-填充前输入维度)/2 = (x-i)/2 = (o-1+k-i)/2

因为这里o=i（填充后输出维度=填充前输入维度）, 所以圈数= **(k-1)/2** =3-1/2=1

<img src="笔记图保存\04e7569e-59bb-4458-a09a-4086aa5f51b2.png" alt="04e7569e-59bb-4458-a09a-4086aa5f51b2" style="zoom:33%;" />

填充padding的代码实现：

```python
import torch

in_channels, out_channels = 1, 1

# 手动设置输入的数据
input = [3, 4, 6, 5, 7,
         2, 4, 6, 8, 2,
         1, 6, 7, 8, 4,
         9, 7, 4, 6, 2,
         3, 7, 5, 4, 1]

input = torch.Tensor(input).view(1, in_channels, 5, 5)

# 和先前的不同在于对卷积层设置padding=1
conv_layer = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels,
                             kernel_size=(3, 3), padding=1, bias=False)

# 手动设置卷积核数据
# 注意此处，先out_channels, 再in_channels（不像先前Conv2d中，固定先设置输入通道数，再输出通道数）
kernel = torch.Tensor([1, 2, 3, 4, 5, 6, 7, 8, 9]).view(out_channels, in_channels, 3, 3)
# kernel.data是 kernel 的一个属性，
# 它返回一个与 kernel 共享相同数据的张量（tensor），
# 但不会跟踪计算历史（即不会参与自动求导）。
# 其存在是为了在不影响计算图的情况下访问张量的数据。
# 它通常用于在不需要梯度的情况下操作张量。
conv_layer.weight.data = kernel.data

output = conv_layer(input)
print(output)

```

设置stride：即设置卷积核移动的步长

填充圈数：

x=填充后输入维度，input=i（填充前输入维度）, kernel=k（卷积核维度）, out=o（填充后输出维度）, 移动步长为s

卷积核中部可移动距离 = x-(k-1) = move

o = 1+(move-1)/s = 1+[x-(k-1)-1]/s = **1+(x-k)/s**, （此处可根据该公式，将x看作输入并计算获取输出维度）则 x = s(o−1)+k

圈数=(填充后输入维度-填充前输入维度)/2 = (x-i)/2 = [s(o−1)+k-i]/2

因为这里o=i（填充后输出维度=填充前输入维度）, 所以圈数= **(si−s-i+k)/2**

<img src="笔记图保存\7ec22b15-e1f8-45f1-8659-00b3c849c661.png" alt="7ec22b15-e1f8-45f1-8659-00b3c849c661" style="zoom:33%;" />

设置stride的代码实现：

```python
conv_layer = torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, 
                             stride=2, # 只需修改卷积层中设置参数即可
                             kernel_size=(3, 3), bias=False)
```

Max Pooling最大池化：2×2的Max Pooling先将输入矩阵划分为2*2的子块集合，在每个划分的子块集里寻找最大值后进行组合（这里默认步长为2）。

因此，Max Pooling不会改变通道数，只会改变图像大小（经过Max Pooling后的图像大小将变为原先的1/2）

<img src="笔记图保存\253b7ad9-4fe8-4212-a8e3-9338820b80bb.png" alt="253b7ad9-4fe8-4212-a8e3-9338820b80bb" style="zoom:33%;" />

Max Pooling层代码实现：

```python
input = [3, 4, 6, 5,
         2, 4, 6, 8,
         1, 6, 7, 8,
         9, 7, 4, 6]
input = torch.Tensor(input).view(1, 1, 4, 4)

# 设置Max Pooling层
maxpooling_layer = torch.nn.MaxPool2d(kernel_size=2)
 
output = maxpooling_layer(input)
```

接下来，使用以下架构为例的CNN进行实现：

<img src="笔记图保存\836ba80e-c19b-4c05-9355-f709284c2e94.png" alt="836ba80e-c19b-4c05-9355-f709284c2e94" style="zoom:33%;" />

<img src="笔记图保存\a0fe1a32-af3d-46fd-ad0c-0f1dac5f4fe3.png" alt="a0fe1a32-af3d-46fd-ad0c-0f1dac5f4fe3" style="zoom:33%;" />

基础神经网络实现代码（这里直接在先前 多分类全连接神经网络的基础上，修改模型定义部分即可）：

```python
import torch
from torchvision import transforms
from torchvision import datasets
from torch.utils.data import DataLoader
import torch.nn.functional as F

batch_size = 64
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307,), (0.3081,))
                                ])

train_dataset = datasets.MNIST(root='../dataset/mnist/',
                               train=True,
                               download=True,
                               transform=transform)
train_loader = DataLoader(train_dataset,
                          shuffle=True,
                          batch_size=batch_size, num_workers=0)
test_dataset = datasets.MNIST(root='../dataset/mnist/',
                              train=False,
                              download=True,
                              transform=transform)
test_loader = DataLoader(test_dataset,
                         shuffle=False,
                         batch_size=batch_size, num_workers=0)


class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 修改此处，设置卷积层、池化层、全连接层
        self.conv1 = torch.nn.Conv2d(1, 10, kernel_size=(5, 5))
        self.conv2 = torch.nn.Conv2d(10, 20, kernel_size=(5, 5))
        self.pooling = torch.nn.MaxPool2d(2)
        self.fc = torch.nn.Linear(320, 10)

    def forward(self, x):
        batch_sizes = x.size(0)
        # 这里由于卷积操作是线性的，通过激活函数引入非线性操作
        # 先池化降低W和H，再Relu，减少计算复杂度和运算量
        x = F.relu(self.pooling(self.conv1(x)))
        x = F.relu(self.pooling(self.conv2(x)))
        # 将输出数据（1个batch-size）展平为一维后输入全连接层：64×20×4×4 -> 64×320
        x = x.view(batch_sizes, -1)
        # 此处无需激活，后面函数中包含了Softmax操作
        x = self.fc(x)
        return x


model = Net()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)

criterion = torch.nn.CrossEntropyLoss(reduction='mean')
optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)


def train(epoch):
    running_loss = 0.0
    for batch_idx, data in enumerate(train_loader, 0):

        inputs, target = data
        inputs, target = inputs.to(device), target.to(device)
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if batch_idx % 300 == 299:
            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 300))
            running_loss = 0.0


def test():
    correct = 0
    total = 0

    with torch.no_grad():
        for data in test_loader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)

            _, predicted = torch.max(outputs.detach(), dim=1)

            total += labels.size(0)

            correct += (predicted == labels).sum().item()

    print('Accuracy on test set: %d %%' % (100 * correct / total))


if __name__ == '__main__':

    for epoch in range(10):
        train(epoch)
        test()

```

# 10.卷积神经网络（高级）

之前所学习的多层感知机（即全连接神经网络）、卷积神经网络，在网络架构上都是穿行结构，此处介绍几类更复杂架构。

## GoogleNet

可发现其中存在大量重复卷积模块，称为Inception。

<img src="笔记图保存\6c5281e5-3f8a-4eee-bbc9-25ee2f84bb7b.png" alt="6c5281e5-3f8a-4eee-bbc9-25ee2f84bb7b" style="zoom:33%;" />

<img src="笔记图保存\585e561f-9b37-4520-845c-1cd6ab1d14d3.png" alt="585e561f-9b37-4520-845c-1cd6ab1d14d3" style="zoom:33%;" />

Inception模块中提供了多类卷积核组合预选，通过训练计算权重后，将自动选择其中较优的组合方式。

其中Concatenate：将卷积结果按照通道的方向进行拼接（因此要求W和H必须相同，而C可以不同）。

Average Pooling：均值池化（Max Pool是取区域内最大保留，Average Pooling则是对区域求均值并保留）。

其中，针对3×3、5×5卷积核下的卷积操作，做padding即可保证图像W和H不变。针对AveragePooling也可做padding实现类似效果（类似卷积操作，但并没有卷积核进行运算，而是求卷积核范围内的均值）。

1×1 卷积的主要目标是调整特征图的通道数（同时还有“信息融合”的作用，将多通道信息汇聚在一起），而不改变W和H。

对于（C，W，H）的图像，进行1×1卷积后均变为（1，W，H），如需要输出通道数为m，则设置卷积层为m个即可（m，C，1，1）。

1×1 卷积可以起到类似降维的作用，相较于直接通过卷积操作改变通道C数量，基于1×1卷积能大幅降低运算量。

针对Inception模块的实现代码如下：

<img src="笔记图保存\edc59757-2002-40b7-b6d0-d593a7a1d9a5.png" alt="edc59757-2002-40b7-b6d0-d593a7a1d9a5" style="zoom: 33%;" />

```python
import torch

from torchvision import transforms
from torchvision import datasets
from torch.utils.data import DataLoader

import torch.nn.functional as F
import torch.optim as optim
import torch.nn as nn

batch_size = 64
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307,), (0.3081,))
                                ])

train_dataset = datasets.MNIST(root='../dataset/mnist',
                               train=True,
                               download=True,
                               transform=transform)
train_loader = DataLoader(train_dataset,
                          shuffle=True,
                          batch_size=batch_size)

test_dataset = datasets.MNIST(root='../dataset/mnist',
                              train=False,
                              download=True,
                              transform=transform)
test_loader = DataLoader(test_dataset,
                         shuffle=False,
                         batch_size=batch_size)


# 定义Inception模型
class InceptionA(nn.Module):
    def __init__(self, in_channels):
        super(InceptionA, self).__init__()
        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)

        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)
        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)

        self.branch3x3_1 = nn.Conv2d(in_channels, 16, kernel_size=1)
        self.branch3x3_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)
        self.branch3x3_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)

        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch5x5 = self.branch5x5_1(x)
        branch5x5 = self.branch5x5_2(branch5x5)

        branch3x3 = self.branch3x3_1(x)
        branch3x3 = self.branch3x3_2(branch3x3)
        branch3x3 = self.branch3x3_3(branch3x3)

        # 均值池化，有函数方法可直接调用
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)

        # 连接4个branch的输出张量，dim=1表示按照通道拼接
        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]
        return torch.cat(outputs, dim=1)


# 定义GoogleNet模型，其中包含Inception模块
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)

        self.incep1 = InceptionA(in_channels=10)
        self.incep2 = InceptionA(in_channels=20)

        self.mp = nn.MaxPool2d(2)
        # 此处，1408=[[28-(5-1)]/2 - (5-1)]/2 * (16 + 24*3)
        self.fc = nn.Linear(1408, 10)

    def forward(self, x):
        in_size = x.size(0)
        x = F.relu(self.mp(self.conv1(x)))
        x = self.incep1(x)
        # 在这步操作之后，经过inception模块的张量通道数均为24*3+16=88，所以先前设置conv2的输入通道数为88
        x = F.relu(self.mp(self.conv2(x)))
        x = self.incep2(x)
        x = x.view(in_size, -1)
        x = self.fc(x)

        return x


model = Net()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)


def train(epoch):
    running_loss = 0.0
    for batch_idx, data in enumerate(train_loader, 0):
        inputs, target = data
        inputs, target = inputs.to(device), target.to(device)
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if batch_idx % 300 == 299:
            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 2000))
            running_loss = 0.0


def test():
    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_loader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, dim=1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print('Accuracy on test set: %d %%' % (100 * correct / total))


if __name__ == '__main__':
    for epoch in range(10):
        train(epoch)
        test()

```

当全连接层的输入维度难以确定时，可尝试使用一个张量作为输入，并直接输出结果形状，不用自己手动计算：

```python
import torch
import torch.nn.functional as F
import torch.nn as nn


class InceptionA(nn.Module):
    def __init__(self, in_channels):
        super(InceptionA, self).__init__()
        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size=1)

        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)
        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size=5, padding=2)

        self.branch3x3_1 = nn.Conv2d(in_channels, 16, kernel_size=1)
        self.branch3x3_2 = nn.Conv2d(16, 24, kernel_size=3, padding=1)
        self.branch3x3_3 = nn.Conv2d(24, 24, kernel_size=3, padding=1)

        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size=1)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)

        branch5x5 = self.branch5x5_1(x)
        branch5x5 = self.branch5x5_2(branch5x5)

        branch3x3 = self.branch3x3_1(x)
        branch3x3 = self.branch3x3_2(branch3x3)
        branch3x3 = self.branch3x3_3(branch3x3)

        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)

        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]
        return torch.cat(outputs, dim=1)


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(88, 20, kernel_size=5)

        self.incep1 = InceptionA(in_channels=10)
        self.incep2 = InceptionA(in_channels=20)

        self.mp = nn.MaxPool2d(2)

    def forward(self, x):
        in_size = x.size(0)
        x = F.relu(self.mp(self.conv1(x)))
        x = self.incep1(x)
        x = F.relu(self.mp(self.conv2(x)))
        x = self.incep2(x)
        x = x.view(in_size, -1)

        return x


net = Net()
print(net(torch.randn(64, 1, 28, 28)).data.shape[1])

```

## Residual Network (ResNet)

梯度消失问题：在链式法则中，当每次乘以的梯度都小于1，梯度将会越来越小，最终W将无法有效更新。（如果每次相乘的梯度都大于1则称为梯度爆炸问题，梯度过大将导致模型无法有效收敛）

老方法：划分输入层、多个隐藏层和输出层。逐层训练隐藏层，在对应隐藏层训练完毕后将其锁住，继续训练下一层，以解决梯度消失问题。该方法难以实现，因为神经网络中层数过多。

引入残差连接：输入x经过2个卷积层后得到F（x），先不激活，将F（x）与x相加后再通过Relu激活。

F（x）与x做加法，意味着其C，W，H均要一致。

<img src="笔记图保存\d3b88e18-dae9-4d86-ae5f-0fa79f7d23c5.png" alt="d3b88e18-dae9-4d86-ae5f-0fa79f7d23c5" style="zoom:33%;" />

使用以下网络结构为例进行ResNet实现：

<img src="笔记图保存\eb0081f6-7d11-484b-9367-817bfe2ef048.png" alt="eb0081f6-7d11-484b-9367-817bfe2ef048" style="zoom:33%;" />

<img src="笔记图保存\1675bb24-b189-41b7-9802-5c6e36ecb1cf.png" alt="1675bb24-b189-41b7-9802-5c6e36ecb1cf" style="zoom:33%;" />

```python
import torch

from torchvision import transforms
from torchvision import datasets
from torch.utils.data import DataLoader

import torch.nn.functional as F
import torch.optim as optim
import torch.nn as nn

batch_size = 64
transform = transforms.Compose([transforms.ToTensor(),
                                transforms.Normalize((0.1307,), (0.3081,))
                                ])

train_dataset = datasets.MNIST(root='../dataset/mnist',
                               train=True,
                               download=True,
                               transform=transform)
train_loader = DataLoader(train_dataset,
                          shuffle=True,
                          batch_size=batch_size)

test_dataset = datasets.MNIST(root='../dataset/mnist',
                              train=False,
                              download=True,
                              transform=transform)
test_loader = DataLoader(test_dataset,
                         shuffle=False,
                         batch_size=batch_size)


# 此处定义残差块
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super(ResidualBlock, self).__init__()
        # 定义输入的通道数
        self.channels = channels
        # 这里为保证最后x能够与F（x）相加，需保证通道数不变，故设置卷积层的输入输出不变。
        # 同时设置padding=1保证W和H不变
        self.conv1 = nn.Conv2d(channels, channels,
                               kernel_size=(3, 3), padding=1)
        self.conv2 = nn.Conv2d(channels, channels,
                               kernel_size=(3, 3), padding=1)

    def forward(self, x):
        y = F.relu(self.conv1(x))
        y = self.conv2(y)
        # 注意此处顺序，先相加，后激活
        return F.relu(x + y)


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=(5, 5))
        self.conv2 = nn.Conv2d(16, 32, kernel_size=(5, 5))
        self.mp = nn.MaxPool2d(2)

        self.rblock1 = ResidualBlock(16)
        self.rblock2 = ResidualBlock(32)

        self.fc = nn.Linear(512, 10)

    def forward(self, x):
        in_size = x.size(0)
        x = self.mp(F.relu(self.conv1(x)))
        x = self.rblock1(x)
        x = self.mp(F.relu(self.conv2(x)))
        x = self.rblock2(x)
        x = x.view(in_size, -1)
        x = self.fc(x)

        return x


model = Net()
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device)
criterion = torch.nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)


def train(epoch):
    running_loss = 0.0
    for batch_idx, data in enumerate(train_loader, 0):
        inputs, target = data
        inputs, target = inputs.to(device), target.to(device)
        optimizer.zero_grad()

        outputs = model(inputs)
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        if batch_idx % 300 == 299:
            print('[%d, %5d] loss: %.3f' % (epoch + 1, batch_idx + 1, running_loss / 2000))
            running_loss = 0.0


def test():
    correct = 0
    total = 0
    with torch.no_grad():
        for data in test_loader:
            images, labels = data
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, dim=1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print('Accuracy on test set: %d %%' % (100 * correct / total))


if __name__ == '__main__':
    for epoch in range(10):
        train(epoch)
        test()
```

# 11.循环神经网络

RNN思想：针对带有时序特征的数据，使用权重共享（例如CNN中，对一张图像使用相同的卷积核进行滑动，因此权重数量少）的概念，减少权重运算的计算量。考虑数据序列前后具有依赖关系。

## RNNCell

在RNN Cell中进行循环计算，每次接收上次计算的输出和本次输入进行运算。

<img src="笔记图保存\e4957b6e-00ce-4d77-8a61-4a935bf18760.png" alt="e4957b6e-00ce-4d77-8a61-4a935bf18760" style="zoom:33%;" />

1.输入数据xt维度是i，输出隐藏层ht维度是h，则wih表示线性变换矩阵是i*h

2.输入上个隐层ht-1的维度是h，输出隐藏层ht维度是h，则whh表示线性变换矩阵是h*h

3.先将1和2中进行线性变换的结果相加，再通过tanh进行非线性激活（取值-1至+1），输出隐层ht，并作为下次循环运算的输入

4.此处实际整体上只进行了一次线性层运算：

<img src="笔记图保存\5152085a-dfbc-412e-a30a-15dafc2481b1.png" alt="5152085a-dfbc-412e-a30a-15dafc2481b1" style="zoom: 50%;" />

<img src="笔记图保存\59d88b0c-3b9a-462a-ac53-6a09d03dab4a.png" alt="59d88b0c-3b9a-462a-ac53-6a09d03dab4a" style="zoom:33%;" />

在Pytorch中，实现RNN有2种方式（按照输入分别设置为数据列表中单个向量/全部向量）：

1.定义RNN Cell，并自己实现循环

```python
import torch

batch_size = 1
seq_len = 3
input_size = 4
hidden_size = 2

# 定义RNNCell，需要的定义参数有输入维度和隐藏层维度
cell = torch.nn.RNNCell(input_size=input_size, hidden_size=hidden_size)

# 设置数据集：序列长度，批量大小，输入维度
dataset = torch.randn(seq_len, batch_size, input_size)
# 设置隐藏层（初始设置为0）：批量大小，隐藏层维度
hidden = torch.zeros(batch_size, hidden_size)
print(dataset)

# 遍历数据集序列，将每次的序列数据和上次输出作为当前输入
for idx, input in enumerate(dataset):
    print('=' * 20, idx, '=' * 20)
    # 将当前序列数据、隐藏层输入RNNCell中计算，获取hidden作为输出（也是下次的输入）
    hidden = cell(input, hidden)
    print('outputs size: ', hidden.shape)
    print(hidden)

```

由观察可知，RNNCell只要设置输入输出的维度即可，**无需设置Batch**。而输入数据和输出数据则均需要设置维度和批量大小（输入还需设置序列长度）。总结：

**RNNCell：Inputsize、Hiddensize**

**Input：Seqlen、Batchsize、Inputsize**

**Out：Batchsize、Hiddensize**

## RNN

2.直接使用RNN，不用自己实现循环。

其中，输入设置为整个序列集合，输出包括隐藏层序列集合(out)+最终隐藏层输出结果(hidden)，且可设置RNN层数。

<img src="笔记图保存\cf875432-c9bb-4dd9-98e6-859b7572babc.png" alt="cf875432-c9bb-4dd9-98e6-859b7572babc" style="zoom:33%;" />

本质上，多层RNN在单个RNN的基础上，将当前时刻输入修改为上层输出。这样RNN便完成了空间上的数据变换。

<img src="笔记图保存\1d47b07a-8478-4035-b374-5b264264d637.png" alt="1d47b07a-8478-4035-b374-5b264264d637" style="zoom:33%;" />

```python
import torch

batch_size = 1
seq_len = 3
input_size = 4
hidden_size = 2
num_layers = 1

# 直接定义RNN，参数：输入维度、隐藏层维度、RNN层数
cell = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size,
                    num_layers=num_layers)

# 定义输入，参数：序列长度、批量大小、输入维度
inputs = torch.randn(seq_len, batch_size, input_size)
# 定义初始隐藏层，参数：RNN层数、批量大小、隐藏层维度
hidden = torch.zeros(num_layers, batch_size, hidden_size)

# 获取结果：最后一层Cell的输出结果集合（上），最终时序于不同层下的输出结果（右）
# 这里，out的最后一个输出应该等于hidden的最后一个输出
out, hidden = cell(inputs, hidden)
print('Output size: ', out.shape)
print('Output: ', out)
print('Hidden size: ', hidden.shape)
print('Hidden: ', hidden)
```

RNN可以设置参数batch_first使得BatchSize和序列长度进行位置交换：

<img src="笔记图保存\bd304dfb-a619-4d44-9a7b-3f8d9898ffda.png" alt="bd304dfb-a619-4d44-9a7b-3f8d9898ffda" style="zoom: 33%;" />

```python
import torch

batch_size = 1
seq_len = 3
input_size = 4
hidden_size = 2
num_layers = 1

# 设置交换顺序
cell = torch.nn.RNN(input_size=input_size, hidden_size=hidden_size,
                    num_layers=num_layers, batch_first=True)

# 输入数据：交换顺序
inputs = torch.randn(batch_size, seq_len, input_size)
# 注意这里不要交换顺序
hidden = torch.zeros(num_layers, batch_size, hidden_size)

out, hidden = cell(inputs, hidden)
print('Output size: ', out.shape)
print('Output: ', out)
print('Hidden size: ', hidden.shape)
print('Hidden: ', hidden)
```

总结：

**RNN：Inputsize、Hiddensize、Numlayers**

**Input：Batchsize、Seqlen、Inputsize**

**Out：Numlayers、Batchsize、Hiddensize**

## RNNCell-Seq2Seq

以针对“hello”单词的处理为例，进行序列到序列（Sequence-to-Sequence，常简写为Seq2Seq）的转换任务：

RNN只能处理向量形式，如何将“hello”单词转换为向量序列？

<img src="笔记图保存\84f724e2-7e56-4362-b404-c4c20768e220.png" alt="84f724e2-7e56-4362-b404-c4c20768e220" style="zoom:33%;" />

1.使用RNNCell进行实现

```python
import torch

batch_size = 1
# 因此处构建ont-hot矩阵来表示向量，因此输入维度为4
input_size = 4
hidden_size = 4

# 定义字典
idx2char = ['e', 'h', 'l', 'o']
# 对应输入：hello
x_data = [1, 0, 2, 2, 3]
# 对应输出：0hlol
y_data = [3, 1, 2, 3, 2]

# 构造one_hot索引矩阵
one_hot_lookup = [[1, 0, 0, 0],
                  [0, 1, 0, 0],
                  [0, 0, 1, 0],
                  [0, 0, 0, 1]]
# 定义输入的向量序列，维度：Seqlen, inputsize
x_one_hot = [one_hot_lookup[x] for x in x_data]

inputs = torch.Tensor(x_one_hot).view(-1, batch_size, input_size)
labels = torch.LongTensor(y_data).view(-1, 1)


class Model(torch.nn.Module):
    def __init__(self, input_size, hidden_size, batch_size):
        super(Model, self).__init__()
        # self.num_layers = num_layers
        self.batch_size = batch_size
        self.input_size = input_size
        self.hidden_size = hidden_size
        # 注意这里是RNNCell而不是RNN
        self.rnncell = torch.nn.RNNCell(input_size=self.input_size,
                                        hidden_size=self.hidden_size)

    # 1次前向传播，就是进行1次RNN的input、hidden输入和hidden输出
    def forward(self, input, hidden):
        hidden = self.rnncell(input, hidden)
        return hidden

    # 在init中设置batch_size的目的，就是在此处初始化hidden，实际上这里的初始化操作也可放在函数外部
    def init_hidden(self):
        return torch.zeros(self.batch_size, self.hidden_size)


# 分别定义模型、损失函数、优化器
net = Model(input_size, hidden_size, batch_size)
# 将其看作多分类问题
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=0.1)

for epoch in range(15):
    loss = 0
    optimizer.zero_grad() 
    # 初始化输入的hidden
    hidden = net.init_hidden()
    print('Predicted string: ', end='')
    # 这里实际上就是按seq_len在遍历向量列表
    for input, label in zip(inputs, labels):
        hidden = net(input, hidden)
        # 损失函数？计算hidden和label的对比
        # 在序列建模任务中（如文本生成、时间序列预测），一个样本通常指一个完整的序列。
        #（此处示例中，相当于1次完整循环中只有1个样本“hello”）
        #（可发现，RNN中，单个样本序列元素中每个元素都有label标签，但是整体也算作一个样本）
        #（序列中的每个元素（如每个字符）对应一个时间步。每个时间步的输入和输出可能都有标签，但这些时间步的标签共同构成一个样本的标签序列。）
        # 注意这里，loss是在单个样本（"hello"）中进行累加计算，且没有使用.item()，因为需要构造计算图。
        # 通过下图说明了RNN和CNN在loss值更新中的差异。
        loss += criterion(hidden, label)
        _, idx = hidden.max(dim=1)
        print(idx2char[idx.item()], end='')


    loss.backward()
    optimizer.step()
    print(', Epoch [%d/15] loss = %.4f' % (epoch + 1, loss.item()))

```

<img src="笔记图保存\0c83098f-6e5a-4d7a-a05d-608ee3a4df6e.png" alt="0c83098f-6e5a-4d7a-a05d-608ee3a4df6e" style="zoom: 50%;" />

## RNN-Seq2Seq

2.使用RNN进行实现

```python
import torch

batch_size = 1
# 添加定义了序列长度
seq_len = 5
input_size = 4
hidden_size = 4
# 添加定义了RNN层次
num_layers = 1

idx2char = ['e', 'h', 'l', 'o']
x_data = [1, 0, 2, 2, 3]
y_data = [3, 1, 2, 3, 2]

one_hot_lookup = [[1, 0, 0, 0],
                  [0, 1, 0, 0],
                  [0, 0, 1, 0],
                  [0, 0, 0, 1]]
x_one_hot = [one_hot_lookup[x] for x in x_data]
inputs = torch.Tensor(x_one_hot).view(seq_len, batch_size, input_size)
labels = torch.LongTensor(y_data)


class Model(torch.nn.Module):
    def __init__(self, input_size, hidden_size, batch_size, num_layers=1):
        super(Model, self).__init__()
        # 此处就是多定义了一个RNN层次
        self.num_layers = num_layers
        self.batch_size = batch_size
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.rnn = torch.nn.RNN(input_size=self.input_size,
                                hidden_size=self.hidden_size,
                                num_layers=num_layers)

    def forward(self, input):
        # 初始化hidden输入
        hidden = torch.zeros(self.num_layers,
                             self.batch_size,
                             self.hidden_size)
        # RNN针对整个数据序列（就算是整个序列，也属于单个样本）进行处理，可忽视中间对序列的遍历过程
        # 最终输出：最后1层每个Cell的输出集合，最后1个时间步上的各层输出
        # 这里的输出结果：out(seqlen, batchsize, hiddensize)
        out, _ = self.rnn(input, hidden)
        # 返回结果：(seqlen × batchsize, hiddensize)，这里将前2个维度拼接在一起，方便后续计算
        return out.view(-1, self.hidden_size)


net = Model(input_size, hidden_size, batch_size, num_layers)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=0.05)

for epoch in range(15):
    # 这里因为使用的是RNN，发现没有对序列的循环过程
    optimizer.zero_grad()
    # 返回结果：(seqlen × batchsize, hiddensize)
    outputs = net(inputs)
    # 这里labels的维度：(seqlen × batchsize, 1)，直接与(seqlen × batchsize, hiddensize)进行交叉熵计算
    # 注意：CrossEntropyLoss此处 处理的是两个序列向量集合，而不是先前RNNCell中成双的向量，
    # 默认对所有时间步的损失求平均。若需与RNNCell的累加结果一致，需设置reduction='sum'。
    # 一般采用求均值的方式更优。
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    _, idx = outputs.max(dim=1)
    idx = idx.data.numpy()
    print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')
    print(', Epoch [%d/15] loss = %.3f' % (epoch + 1, loss.item()))

```

## Embedding和线性变换

在进行自然语言处理时，独热向量存在哪些缺点？

1.维度过高（例如，词级别进行划分？每个向量都是万级维度，维度诅咒）

2.分布稀疏（万级节点仅映射至坐标轴上一个点）

3.硬编码（一 一对应，1个向量特征的独热向量固定）

能否变为：1.低维度 2.稠密 3.可学习？

为此引入**EMBEDDING**（嵌入层），进行数据降维。

<img src="笔记图保存\f5e5109b-256d-4307-a9f0-0c6c58ebf6ff.png" alt="f5e5109b-256d-4307-a9f0-0c6c58ebf6ff" style="zoom:33%;" />

关键参数：词汇表维度（inputSize）和嵌入维度（embeddingSize）。

根据对应元素在词汇表中位置，索引寻找对应向量即可。

<img src="笔记图保存\f997ae82-0557-4f35-bd1b-bd24f407a38b.png" alt="f997ae82-0557-4f35-bd1b-bd24f407a38b" style="zoom:33%;" />

<img src="笔记图保存\039d3583-c19c-422d-a3f7-5555c62fed7a.png" alt="039d3583-c19c-422d-a3f7-5555c62fed7a" style="zoom:33%;" />

使用时，在序列输入RNN之前，对序列元素（维度是one-hot）分别进行embedding映射（输入需要是长整形）即可。

最后可以接一个线性层，将hiddensize映射至需要分类的维度。

（这里，隐藏层维度hiddensize未必等于需分类维度。有时隐藏层需足够大，以编码中间特征）

这里的线性层，可对长序列中每个元素进行处理（参考先前corssenpty）

<img src="笔记图保存\f68aa152-7027-457a-9bcc-9fd9a83d6df1.png" alt="f68aa152-7027-457a-9bcc-9fd9a83d6df1" style="zoom:33%;" />

案例实现

```python
import torch

# 批量大小设置1
batch_size = 1
# 序列长度设置5（例：样本hello）
seq_len = 5

# 词汇表维度设置4（词汇表索引中的行数）
input_size = 4
# 嵌入维度设置10（词汇表索引中的列数）
embedding_size = 10

# 隐藏层设置8（这里不是分类类别数）
hidden_size = 8
# 2层RNN
num_layers = 2
# 定义分类类别数
num_class = 4

idx2char = ['e', 'h', 'l', 'o']
# 因为之后设置batch_first，输入数据需将batchsize设置为第1个维度
x_data = [[1, 0, 2, 2, 3]]
y_data = [3, 1, 2, 3, 2]
# embedding的输入需要长整形
inputs = torch.LongTensor(x_data)
labels = torch.LongTensor(y_data)


class Model(torch.nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        # 定义embedding层，作用：将整数索引映射为稠密向量，注意理解这里不是对输入数据进行映射
        self.emb = torch.nn.Embedding(input_size, embedding_size)
        # 设置RNN，batch_first=True
        self.rnn = torch.nn.RNN(input_size=embedding_size,
                                hidden_size=hidden_size,
                                num_layers=num_layers,
                                batch_first=True)
        # 设置线性层
        self.fc = torch.nn.Linear(hidden_size, num_class)

    def forward(self, x):
        # 设置初始隐层
        hidden = torch.zeros(num_layers, batch_size, hidden_size)
        # 将初始数据x(batch_size,seqlen)输入Embedding(input_size,embedding_size)，
        # 由Embedding转换为x(batch_size,seqlen,embedding_size)
        x = self.emb(x)
        # 将x(batch_size,seqlen,embedding_size)输入RNN，
        # 获取各个时间步上最后层输出(batch_size,seqlen,hidden_size)
        x, _ = self.rnn(x, hidden)
        # 将最后层输出(batch_size,seqlen,hidden_size)基于线性层变换为(batch_size,seqlen,num_class)
        x = self.fc(x)
        # 返回(batch_size×seqlen,num_class)
        return x.view(-1, num_class)


net = Model()
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=0.05)

for epoch in range(15):
    optimizer.zero_grad()
    outputs = net(inputs)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    _, idx = outputs.max(dim=1)
    idx = idx.data.numpy()

    print('Predicted: ', ''.join([idx2char[x] for x in idx]), end='')
    print(', Epoch [%d/15] loss = %.3f' % (epoch + 1, loss.item()))

```

# 循环神经网络（高级-GRU）

<img src="笔记图保存\161eadaf-bf2f-44b8-bb08-1126a9c5daef.png" alt="161eadaf-bf2f-44b8-bb08-1126a9c5daef" style="zoom:33%;" />

问题描述：输入名字，输出对应国家（18种）。这里由于不是Seq2Seq任务，而是分类任务，

所采用的模型可以简化，直接输出最后1个时间步上最后1层RNNCell输出的hidden即可，以下以GRU为例进行实现。

<img src="笔记图保存\8a11da1f-219a-4f4d-a106-827cc7bd5afc.png" alt="8a11da1f-219a-4f4d-a106-827cc7bd5afc" style="zoom:33%;" />

<img src="笔记图保存\ebdd4dcf-34a2-43cc-b275-4fe2866a9634.png" alt="ebdd4dcf-34a2-43cc-b275-4fe2866a9634" style="zoom:33%;" />

数据预处理：如何将原始姓名列表转换为向量列表？

先划分为字符，再转换为ASCLL表（每个字母都可看作128维独热向量）

<img src="笔记图保存\0f23ae06-611a-4bfb-bc03-4fff5e45492d.png" alt="0f23ae06-611a-4bfb-bc03-4fff5e45492d" style="zoom:33%;" />

词汇表中每行序列的长短不一？对序列进行padding（按0填充），使得输入模型的seq_len统一

<img src="笔记图保存\fa269a0b-8962-44d7-9f26-fc90ec51694d.png" alt="fa269a0b-8962-44d7-9f26-fc90ec51694d" style="zoom:33%;" />

label怎么获取？统计国家，作出词典映射即可

<img src="笔记图保存\f5412d16-d2d1-4ef0-9ab1-1acfd456a116.png" alt="f5412d16-d2d1-4ef0-9ab1-1acfd456a116" style="zoom:33%;" />

```python
import torch
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pack_padded_sequence
import time
import matplotlib.pyplot as plt
import numpy as np
import gzip
import csv
import math

HIDDEN_SIZE = 100
BATCH_SIZE = 256
N_LAYER = 2
N_EPOCHS = 100
N_CHARS = 128
USE_GPU = True


# 数据集获取设置，注意这里继承了Dataset
class NameDataset(Dataset):

    # 这里使用is_train_set作为类初始化时的输入参数，之后可选择数据集是否为训练集/测试集
    def __init__(self, is_train_set=True):
        filename = 'names_train.csv.gz' if is_train_set else 'names_test.csv.gz'
        # 读取压缩文件中的数据
        with gzip.open(filename, 'rt') as f:
            reader = csv.reader(f)
            # 这里获取的是一个类似[['Adsit', 'Czech'], ['Ajdrna', 'Czech']]的二维列表数组（名字，国家）
            rows = list(reader)
        # 获取姓名样本和总样本数
        self.names = [row[0] for row in rows]
        self.len = len(self.names)
        # 获取姓名样本对应的国家类别标签列表（之后需转换为数字索引形式）
        self.countries = [row[1] for row in rows]
        # set：去重，sorted：排序，list：转列表
        self.country_list = list(sorted(set(self.countries)))
        # 设置国家字典，（键：国家名称，值：对应的数值索引0，1，2，3...）
        self.country_dict = self.getCountryDict()
        # 获取国家类别数（即最终分类数）
        self.country_num = len(self.country_list)

    # 为数据集提供索引访问，magic方法，当对象使用下标时自动调用该方法。获取样本、和对应标签
    # 此处：样本-字符串。标签-数字索引
    def __getitem__(self, index):
        return self.names[index], self.country_dict[self.countries[index]]

    # 这里__len__也是一个magic method，当对象调用len()时自动调用该方法
    def __len__(self):
        return self.len

    # 将国家列表转换为字典
    def getCountryDict(self):
        country_dict = dict()
        for idx, country_name in enumerate(self.country_list, 0):
            country_dict[country_name] = idx

        return country_dict

    # 方法：返回国家类别数（即最终分类数）
    def getCountriesNum(self):
        return self.country_num


# 数据准备
trainset = NameDataset(is_train_set=True)
trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)
testset = NameDataset(is_train_set=False)
testloader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)
N_COUNTRY = trainset.getCountriesNum()


# 函数：将创建的tensor迁移至GPU运行
def create_tensor(tensor):
    if USE_GPU:
        device = torch.device("cuda:0")
        tensor = tensor.to(device)

    return tensor


# 模型设计
class RNNClassifier(torch.nn.Module):
    """
    这里：
    input_size=词汇表维度
    hidden_size=词汇表嵌入维度=GRU隐藏层维度
    output_size=最终分类数
    n_layers=GRU层数
    bidirectional=设置双向网络
    """

    def __init__(self, input_size, hidden_size, output_size, n_layers=1, bidirectional=True):

        super(RNNClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.n_layers = n_layers
        self.n_directions = 2 if bidirectional else 1
        # 定义嵌入层、GRU、线性层
        self.embedding = torch.nn.Embedding(input_size, hidden_size)
        self.gru = torch.nn.GRU(hidden_size, hidden_size, n_layers,
                                bidirectional=bidirectional)
        self.fc = torch.nn.Linear(hidden_size * self.n_directions, output_size)

    def _init_hidden(self, batch_size):

        hidden = torch.zeros(self.n_layers * self.n_directions,
                             batch_size, self.hidden_size)

        # 设置初始隐藏层时，设置了在GPU上创建
        return create_tensor(hidden)

    # 一次前向传播的过程，
    # 分类器进行函数调用时需要的输入参数包括：输入的姓名向量列表，seq_len列表
    def forward(self, input, seq_lengths):

        # 此处.t()方法表示对矩阵进行转置操作
        # input.size(0)：每个姓名的长度。input.size(1)：处理姓名的batch大小
        input = input.t()
        batch_size = input.size(1)

        # 初始化隐层、嵌入层
        hidden = self._init_hidden(batch_size)
        embedding = self.embedding(input)

        # pack_padded_sequence是PyTorch中用于处理变长序列数据的一个重要工具，
        # 其主要作用是高效地跳过填充部分的计算，从而节省计算资源并提高模型性能。
        # 在调用 pack_padded_sequence 之前，将 seq_lengths 移动到 CPU
        # 先前保留seq_lengths、以及对序列进行降序排列的目的，就是在此处使用
        seq_lengths = seq_lengths.cpu()
        gru_input = pack_padded_sequence(embedding, seq_lengths)

        # 将处理后的数据传入GRU处理，并获得输出
        output, hidden = self.gru(gru_input, hidden)
        if self.n_directions == 2:
            hidden_cat = torch.cat([hidden[-1], hidden[-2]], dim=1)
        else:
            hidden_cat = hidden[-1]
        # 将hidden输出，输入线性层进行维度变换
        fc_output = self.fc(hidden_cat)

        return fc_output


# 函数：将单个人名转换为对应ASCLL值的列表，同时返回人名列表和人名长度
def name2list(name):
    arr = [ord(c) for c in name]

    return arr, len(arr)


# 函数主要工作：对人名（padding操作）、国家进行处理
def make_tensors(names, countries):
    # 获取（ASCLL人名，长度）的列表
    sequences_and_lengths = [name2list(name) for name in names]
    # 单独获取ASCLL人名列表
    name_sequences = [s1[0] for s1 in sequences_and_lengths]
    # 单独获取长度列表（转换为LongTensor类型）
    seq_lengths = torch.LongTensor([s1[1] for s1 in sequences_and_lengths])

    # 创建全0向量矩阵（向量长度设置为最长人名）
    seq_tensor = torch.zeros(len(name_sequences), seq_lengths.max()).long()

    # 同时遍历人名、长度列表、全0向量矩阵，
    # 对矩阵遍历的过程中，每行前seq_len个元素被设置为人名，其余元素保持为0（即padding操作）
    for idx, (seq, seq_len) in enumerate(zip(name_sequences, seq_lengths), 0):
        seq_tensor[idx, :seq_len] = torch.LongTensor(seq)

    # 对人名样本，按长度进行排序（同时，国家标签也对应修改顺序）
    # 返回值：
    # seq_lengths：排序后的新长度列表（从大到小）。
    # perm_idx：一个索引张量，表示原始数据应该按什么顺序排列才能得到排序后的结果。
    """
        举例 ：
        若原始 seq_lengths = [3, 5, 2]，
        排序后 seq_lengths = [5, 3, 2]，
        perm_idx = [1, 0, 2]（表示原索引1的数据排第一，索引0的排第二，索引2的排第三）。
    """
    seq_lengths, perm_idx = seq_lengths.sort(dim=0, descending=True)
    # 对人名，按长度进行样本排序
    seq_tensor = seq_tensor[perm_idx]
    # 对国家，同样按照索引进行排序
    countries = countries[perm_idx]

    # 返回：迁移至GPU运行、并进行排序后的
    # 人名样本、长度序列、国家标签
    return create_tensor(seq_tensor), \
        create_tensor(seq_lengths), \
        create_tensor(countries)


# 计算耗时的函数
def time_since(since):
    # 当前时间-过去时间=运行时长
    s = time.time() - since
    m = math.floor(s / 60)
    s -= m * 60

    return '%dm %ds' % (m, s)

# 模型的训练函数
def trainModel():
    total_loss = 0
    for i, (names, countries) in enumerate(trainloader, 1):
        inputs, seq_lengths, target = make_tensors(names, countries)

        # 在调用 pack_padded_sequence 之前，将 seq_lengths 移动到 CPU
        seq_lengths = seq_lengths.cpu()

        output = classifier(inputs, seq_lengths)
        loss = criterion(output, target)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # 对每个batch的平均损失进行累加
        total_loss += loss.item()
        # 每10个batch进行一次结果输出
        if i % 10 == 0:
            print(f'[{time_since(start)}] Epoch {epoch}', end='')
            print(f'[{i * len(inputs)} / {len(trainset)}]', end='')
            print(f'loss = {total_loss / (i * len(inputs))}')

    return total_loss

# 模型的测试函数
def testModel():
    correct = 0
    total = len(testset)
    print("evaluating trained model ...")
    with torch.no_grad():
        for i, (names, countries) in enumerate(testloader, 1):
            inputs, seq_lengths, target = make_tensors(names, countries)
            output = classifier(inputs, seq_lengths)
            pred = output.max(dim=1, keepdim=True)[1]
            correct += pred.eq(target.view_as(pred)).sum().item()

        percent = '%.2f' % (100 * correct / total)
        print(f'Test set: Accuracy {correct} / {total} {percent} %')

    return correct / total


if __name__ == '__main__':
    # 此处GRU分类器，输入定义：词汇表维度，隐藏层维度，国家数（分类数），GRU层数
    classifier = RNNClassifier(N_CHARS, HIDDEN_SIZE, N_COUNTRY, N_LAYER)
    # 将分类器迁移至GPU运行
    if USE_GPU:
        device = torch.device("cuda:0")
        classifier.to(device)

    # 构造损失函数和优化器
    criterion = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(classifier.parameters(), lr=0.001)

    # 记录训练开始时间
    start = time.time()
    # 进行模型训练与测试（已分别封装为模块）
    print("Traning for %d epochs..." % N_EPOCHS)
    acc_list = []
    for epoch in range(1, N_EPOCHS + 1):
        trainModel()
        acc = testModel()
        # 将每个epoch的测试结果添加进列表
        acc_list.append(acc)

epoch = np.arange(1, len(acc_list) + 1, 1)
acc_list = np.array(acc_list)
plt.plot(epoch, acc_list)
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.grid()
plt.show()

```

改进的双向循环神经网络：

对序列进行正向递归计算，同时对序列进行方向递归计算，并将正向与反向的递归结果拼接。

<img src="笔记图保存\fccc6872-c9ff-4425-88e3-5681e8ed86a0.png" alt="fccc6872-c9ff-4425-88e3-5681e8ed86a0" style="zoom:33%;" />

输出是2倍（结果进行拼接）

<img src="笔记图保存\73f77a7e-e80a-403f-98b5-ee7623d12d7f.png" alt="73f77a7e-e80a-403f-98b5-ee7623d12d7f" style="zoom:33%;" />

​                                                                                                                                                                                                                                                                                                                                                                                                                 打包的原理：对变长序列排序后，进行拼接操作

<img src="笔记图保存\27d3505e-78d0-4790-9d48-6c5eb2c06186.png" alt="27d3505e-78d0-4790-9d48-6c5eb2c06186" style="zoom:33%;" />

# LSTM

```python
对于LSTM的基本原理，其形式基本与GRU相同，
不同点在于，LSTM需要两个初始隐藏状态张量：h_0（隐藏状态）和 c_0（细胞状态），
而GRU和RNN类似，只需一个初始隐藏状态张量 h_0。 
```

# TCN模型

内容见知乎[TCN（Temporal Convolutional Networks）算法详解 - 知乎](https://zhuanlan.zhihu.com/p/648890779)

可同时将时间步上的长序列数据，整个一起作为输入。

# Transformer

见B站视频，重点在于QKV的注意力机制。

[第四节 2021 - 自注意力机制(Self-attention)(上)_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1Wv411h7kN?spm_id_from=333.788.videopod.episodes&vd_source=f931e43e5bf927b47a1004851b8915f3&p=38)

```python
import torch
import torch.nn as nn
import math

# 设置设备（优先使用GPU）
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


# =============== image1. 位置编码 ===============
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        # 创建位置编码矩阵：shape [max_len, d_model]
        # 这里max_len就是序列长度，d_model即序列每个元素的数据维度
        pe = torch.zeros(max_len, d_model)
        # 位置索引：shape [max_len, image1]
        position = torch.arange(0, max_len).unsqueeze(1)
        # 计算分母项：image1/(10000^(2i/d_model))，使用指数衰减来构建不同频率的正弦波
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))

        # 偶数位置使用sin编码，奇数位置使用cos编码
        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度
        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度

        # 增加batch维度：shape [image1, max_len, d_model]，使其可以与输入张量相加
        pe = pe.unsqueeze(0)
        # 注册为缓冲区（不参与梯度计算）
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x: [batch_size, seq_len, d_model]
        # 取出对应序列长度的位置编码并相加
        x = x + self.pe[:, :x.size(1), :]
        return x


# =============== image2. 多头注意力 ===============
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        assert d_model % n_heads == 0  # 确保维度可以整除头数

        self.n_heads = n_heads
        self.head_dim = d_model // n_heads  # 每个头的维度

        # 定义Q、K、V的线性变换层
        self.wq = nn.Linear(d_model, d_model)
        self.wk = nn.Linear(d_model, d_model)
        self.wv = nn.Linear(d_model, d_model)
        # 输出线性变换层
        self.fc_out = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        B, S, D = x.size()  # batch_size, seq_len, d_model

        # 线性变换得到Q、K、V
        Q = self.wq(x)  # [B, S, D]
        K = self.wk(x)  # [B, S, D]
        V = self.wv(x)  # [B, S, D]

        # 拆分多头：将维度拆分为n_heads × head_dim
        # [B, S, D] → [B, S, n_heads, head_dim] → [B, n_heads, S, head_dim]
        Q = Q.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)
        K = K.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)
        V = V.view(B, S, self.n_heads, self.head_dim).transpose(1, 2)

        # 计算注意力分数：Q·K^T / sqrt(d_k)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        # 应用mask（如果有）
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))
        # 计算softmax得到注意力权重
        attention = torch.softmax(scores, dim=-1)
        # 加权求和得到输出
        out = torch.matmul(attention, V)  # [B, n_heads, S, head_dim]

        # 合并多头：[B, n_heads, S, head_dim] → [B, S, D]
        out = out.transpose(1, 2).contiguous().view(B, S, D)
        # 最终线性变换
        return self.fc_out(out)


# =============== 3. 编码器单层 ===============
class TransformerEncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, dim_feedforward, dropout):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads)
        # 前馈神经网络层
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.linear2 = nn.Linear(dim_feedforward, d_model)
        # Dropout层
        self.dropout = nn.Dropout(dropout)
        # 层归一化
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        # 激活函数
        self.activation = nn.ReLU()

    def forward(self, src, src_mask=None):
        # 多头自注意力 + 残差连接 + 层归一化
        attn_output = self.self_attn(src, src_mask)
        src = self.norm1(src + self.dropout(attn_output))  # 残差连接

        # 前馈神经网络 + 残差连接 + 层归一化
        ffn_output = self.linear2(
            self.dropout(
                self.activation(
                    self.linear1(src)
                )
            )
        )
        src = self.norm2(src + self.dropout(ffn_output))  # 残差连接
        return src


# =============== 4. 多层 Encoder 模块 ===============
class TransformerEncoder(nn.Module):
    def __init__(self, num_layers, d_model, n_heads, dim_feedforward, dropout, max_len=5000):
        super().__init__()
        # 位置编码层
        self.pos_encoder = PositionalEncoding(d_model, max_len)
        # 堆叠多个编码器层
        self.layers = nn.ModuleList([
            TransformerEncoderLayer(d_model, n_heads, dim_feedforward, dropout)
            for _ in range(num_layers)
        ])
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_mask=None):
        # 添加位置编码
        src = self.pos_encoder(src)
        src = self.dropout(src)

        # 逐层处理
        for layer in self.layers:
            src = layer(src, src_mask)
        return src


## 实例化展示
if __name__ == "__main__":
    # 模型参数
    d_model = 512
    n_heads = 8
    dim_ff = 2048
    dropout = 0.1
    num_layers = 6

    # 实例化模型并移动到GPU
    model = TransformerEncoder(
        num_layers, d_model, n_heads, dim_ff, dropout
    ).to(device)

    # 创建输入张量并移动到GPU
    src = torch.rand(32, 100, d_model).to(device)  # (batch_size, seq_len, d_model)

    # 前向传播（自动在GPU上运行）
    output = model(src)

    # 打印输出形状
    print("Output shape:", output.shape)  # 应显示：torch.Size([32, 100, 512])

```



# Mamba-1，Mamba-2

见B站视频、知乎

[介绍Transfomer有力竞争者：Mamba-1，Mamba-2，TTT模型_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1hPeqevECK/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=f931e43e5bf927b47a1004851b8915f3)

[Mamba模型底层技术详解，与Transformer到底有何不同？ - 知乎](https://zhuanlan.zhihu.com/p/685567307)

## Mamba-1

1.将激活函数替换为线性变换

<img src="笔记图保存\61f562a1-ae39-4c4a-a0c3-940581e3ec01.png" alt="61f562a1-ae39-4c4a-a0c3-940581e3ec01" style="zoom: 50%;" />

<img src="笔记图保存\52578aa2-0df1-4baf-abe7-bcf96ca68a76.png" alt="52578aa2-0df1-4baf-abe7-bcf96ca68a76" style="zoom:50%;" />

2.类似LSTM的门控机制

<img src="笔记图保存\e443e81f-80f7-4e0d-9f85-7e26a2ba6c24.png" alt="e443e81f-80f7-4e0d-9f85-7e26a2ba6c24" style="zoom:50%;" />

3.时间序列上门控参数的动态变化（和当前时间步的输入有关）

<img src="笔记图保存\31c2034b-bc86-477c-a102-d94710eab3ae.png" alt="31c2034b-bc86-477c-a102-d94710eab3ae" style="zoom: 50%;" />

4.并行计算

<img src="笔记图保存\896c34fa-a005-4049-b00b-701b13a4f3d0.png" alt="896c34fa-a005-4049-b00b-701b13a4f3d0" style="zoom: 50%;" />

<img src="笔记图保存\303bea1f-88f9-4c8d-bd23-cff3b64d414b.png" alt="303bea1f-88f9-4c8d-bd23-cff3b64d414b" style="zoom:50%;" />

## Mamba-2

引入SSD，在SSM和注意力之间构建桥梁。（分组注意力头结构）

<img src="笔记图保存\3a7da05f-0530-4e7e-8e0c-12fac364b8dd.png" alt="3a7da05f-0530-4e7e-8e0c-12fac364b8dd" style="zoom:50%;" />

这里和基础的注意力机制有些不同：

<img src="笔记图保存\f9dcb0fb-af47-49c5-8f61-d865f3a53bbf.png" alt="f9dcb0fb-af47-49c5-8f61-d865f3a53bbf" style="zoom:50%;" />

<img src="笔记图保存\a13e478c-23c2-4e9e-9183-4b37992b15e5.png" alt="a13e478c-23c2-4e9e-9183-4b37992b15e5" style="zoom:50%;" />

**问题1: SSM 和 Attention 是如何被联系起来的？**

这个问题是本文所讨论的核心问题，在前文中用了比较复杂的理论进行推导。简单来说就是，SSM 的卷积形式可以写成矩阵相乘的形式，当 A 为标量时，可以进一步改写为和 Linear Attention 相同的形式，不同之处在于SSM掩码矩阵是一个 1-SS 矩阵，因果掩码只是其一种特殊形式。

**问题2: 为什么要放弃 softmax，其收益与代价?**

去掉了 softmax，attention 部分就可以进行自由的矩阵变换了，这也就是 Linear Attention 和 RetNet 放弃 softmax 的根本原因，其代价就是需要 Mask 来进行因果掩码及部分远程衰减的功能，同时要考虑增加非线性单元。如果保留 softmax，则无法与卷积形式的 SSM 构成等价关系。

# GNN

## 1.图神经网络基础知识

1.同质图（节点和边的类型，在1张图中一样）；异质图
2.无向图；有向图；混合图
3.赋权图；无权图

同构图和同质图区别

<img src="笔记图保存\c6d2fb3e-3644-4dfe-9a2e-beed08555014.png" alt="c6d2fb3e-3644-4dfe-9a2e-beed08555014" style="zoom:50%;" />

邻接矩阵（节点-节点）和关联矩阵（节点-边）

<img src="笔记图保存\ef4bda0a-3663-45fc-ba22-4bda496eb529.png" alt="ef4bda0a-3663-45fc-ba22-4bda496eb529" style="zoom: 33%;" />

邻居、度、入度、出度

<img src="笔记图保存\ef6c1325-2d6b-4682-82a0-4f5d5719e312.png" alt="ef6c1325-2d6b-4682-82a0-4f5d5719e312" style="zoom:50%;" />

下面这张图是一个**图神经网络“族谱”或“技术分解图”**，帮助我们理解不同GNN模型的核心组件和设计思想。它将复杂的GNN模型分解为三个核心模块：**传播模块（Propagation Module）**、**采样模块（Sampling Module）和池化模块（Pooling Module）**。

<img src="笔记图保存\3-1.png" alt="3-1" style="zoom: 33%;" />

**1. 传播模块 (Propagation Module)**

这是GNN最核心的部分，负责定义节点如何聚合其邻居节点的信息来更新自身的表示（Representation）。这个过程就像在社交网络中，你的想法会受到朋友们的影响一样。该模块主要分为三类：

- **卷积算子 (Convolution Operator)**：这是目前最主流的传播方式，借鉴了卷积神经网络（CNN）的思想。它又分为两类：
  - **谱方法 (Spectral)**：这类方法基于图谱理论，将图信号转换到谱空间（频域）进行处理，理论性很强。代表模型有 **GCN (图卷积网络)** 和 **ChebNet**。
  - **空间方法 (Spatial)**：这类方法直接在图的空间结构上（即节点和其邻居）进行信息聚合，更直观、灵活。它又可细分为：
    - **基础 (Basic)**：如 **GraphSAGE**，它通过聚合邻居节点的特征来学习。
    - **注意力 (Attentional)**：如 **GAT (图注意力网络)**，它在聚合邻居信息时引入了注意力机制，可以为不同的邻居分配不同的重要性权重。
    - **框架 (Framework)**：如 **MPNN (消息传递神经网络)**，它提供了一个统一的框架来描述各种空间方法。
- **循环算子 (Recurrent Operator)**：这类方法借鉴了循环神经网络（RNN）的思想，通过迭代更新节点状态直至收敛。这在早期的GNN模型中比较常见，例如 **GNN** 和 **GGNN (门控图神经网络)**。
- **跳跃连接 (Skip Connection)**：借鉴于ResNet，这种技术允许信息从浅层直接“跳跃”到深层，以构建更深、更强大的GNN模型，有效缓解了深度网络中的梯度消失问题。代表模型有 **JKN (Jumping Knowledge Networks)** 和 **DeepGCN**。

**2. 采样模块 (Sampling Module)**

当图的规模非常巨大（例如，拥有数百万甚至数十亿个节点的社交网络或电商推荐网络）时，将所有节点的邻居信息都纳入计算是不现实的。采样模块就是为了解决这个问题，它通过对图进行采样来缩小计算规模。

- **节点采样 (Node)**：如 **GraphSAGE** 和 **PinSAGE** (由Pinterest提出)，它们为每个节点采样固定数量的邻居，而不是使用所有邻居。
- **层采样 (Layer)**：如 **FastGCN**，它在每一层都独立地采样一部分节点，进一步提高了效率。
- **子图采样 (Subgraph)**：如 **ClusterGCN** 和 **GraphSAINT**，它们将图划分为多个子图（Cluster），然后在子图上进行训练，大大减少了计算和内存开销。

**3. 池化模块 (Pooling Module)**

池化（或称“读出”，Readout）操作用于将图中所有节点的特征聚合成一个单一的、固定大小的向量，从而得到整个图的表示。这对于图分类、图属性预测等任务至关重要。

- **直接池化 (Direct)**：这类方法比较简单，例如直接对所有节点的特征进行求和、求平均或使用更复杂的 **Set2Set** 模型。
- **层次化池化 (Hierarchical)**：这类方法借鉴了CNN中池化层的思想，通过逐步“粗化”（Coarsening）或对节点进行聚类来学习图的层次化表示。代表模型有 **DiffPool** 和 **SAGPool**。

## 2.使用NetworkX可视化图

```python
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np

# 创建有向图
G = nx.DiGraph()

# 添加节点
G.add_nodes_from(['A', 'B', 'C'])
# 添加边
G.add_edges_from([
    ('A', 'A'),
    ('A', 'B'),
    ('B', 'C'),
    ('C', 'B')
])

# 设置节点特征
node_feature = {
    'A': '[1,3]',
    'B': '[2,4]',
    'C': '[5,7]'
}
nx.set_node_attributes(G, node_feature, name='node_feature')
# 设置边权重
edge_weights = {
    ('A', 'B'): 4.5,
    ('B', 'C'): 2.0,
    ('C', 'B'): 6.0,
    ('A', 'A'): 1.0
}
nx.set_edge_attributes(G, edge_weights, name='edge_weights')

# 获取属性用于绘图
node_labels = nx.get_node_attributes(G, 'node_feature')
edge_labels = nx.get_edge_attributes(G, 'edge_weights')

# 绘图
plt.figure(figsize=(8, 6))
# 设置布局格式
pos = nx.spring_layout(G, seed=42)
# 创建基础绘图
nx.draw(G, pos, with_labels=True)
# 添加节点属性
nx.draw_networkx_labels(G, pos, labels=node_labels)
# 添加边权重
nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)
plt.show()

# 获取图信息
print(G.nodes)  # 获取节点
print(G.edges)  # 获取边
print(G.degree())  # 获取各个节点的度

# 判断图信息
print(nx.is_directed(G))  # 是否为有向图
print(nx.is_weighted(G))  # 是否为加权图

# 邻接矩阵
A = nx.adjacency_matrix(G)
print(A)
print(A.todense())

# 度矩阵
D = np.diag(list(dict(nx.degree(G)).values()))
print(D)
print(G.in_degree)  # 节点入度数
print(G.out_degree)  # 节点出度数
print(list(G.neighbors('A')))  # 获取某个节点的邻居

```

## 3.理解图神经网络和信息传递机制

<img src="笔记图保存\3-2.png" alt="3-2" style="zoom:50%;" />

1.挑选信息 2.聚合邻居信息 3.将聚合后的邻居信息与自身信息聚合。

消息传递机制：

```python
import torch
from torch_geometric.nn import MessagePassing

# 定义4个节点特征
x = torch.tensor([[1, 2],
                  [2, 3],
                  [8, 3],
                  [2, 4]])
# 定义边（2维矩阵形式，第1行：入，第2行：出）
edge_index = torch.tensor([[0, 0, 1, 2, 2, 3],
                           [0, 1, 2, 1, 3, 2]])


class MessagePassingLayer(MessagePassing):
    def __init__(self):
        # 设置邻居的聚合方式为相加
        super(MessagePassingLayer, self).__init__(aggr='add')

    def forward(self, x, edge_index):
        # propagate()内部调用message()函数，自动拆分edge_index为索引对应的节点特征，并将x_j传入给message()
        return self.propagate(x=x, edge_index=edge_index)

    def message(self, x_i, x_j):
        print(x_i)  # 中心节点特征（出）
        print(x_j)  # 邻居节点特征（入）
        # 返回邻居节点特征+自身特征，这里在求和前进行运算，对每条边的消息进行信息设置
        return x_j + x_i


MessagePassingLayer = MessagePassingLayer()
output = MessagePassingLayer(x, edge_index)
print(output)

```

<img src="笔记图保存\25024bd8-67d4-4261-9ce4-c1ab3afd662f.png" alt="25024bd8-67d4-4261-9ce4-c1ab3afd662f" style="zoom:50%;" />

<img src="笔记图保存\01d956a6-9868-44e5-9134-197c6d148d63.png" alt="01d956a6-9868-44e5-9134-197c6d148d63" style="zoom:50%;" />

## 4.图卷积网络(GCN)的原理与实现

邻接矩阵*特征矩阵=对节点特征的1次更新（这里直接作矩阵乘法，是特征相加）

<img src="笔记图保存\3a3e1dd5-0bea-44fa-96e4-c66d3613907f.png" alt="3a3e1dd5-0bea-44fa-96e4-c66d3613907f" style="zoom: 33%;" />

但由于图结构不包含自环，邻接矩阵对角线上均为0，造成无法结合节点自身特征实现更新，因此常在原始邻接矩阵A上加上自环（即对角线上元素为1）。

此处，考虑对特征进行缩放，可采用度矩阵的逆。而在GCN过程中，度和图论中不同，自环在此处的度只增加1（图论中由于同时增加入度和出度，会增加2）。

此处，使用度矩阵对邻接矩阵进行左乘，实现行变换，使邻接矩阵数值缩放（不再是1），可看成对特征进行了加权，例如度是3时，将每个邻居节点的特征都*1/3。

但此时仅实现行变换而缺少列变换（对发送节点、接收节点均需要进行归一化），因此使用度矩阵对邻接矩阵进行右乘，实现列变换。同时因为做了2次变换，需要平衡一下所以开了根号。

整体上，GCN就是以节点的度为依据，对边的权重进行了修改缩放，并实现信息传递。GCN的规范化，使得中心节点本身和邻居特征聚合的权重发生变化；是一种根据度矩阵（连接数）加权的聚合方式；对中心节点而言，邻居的度越大，传递给中心节点的信息越少。

<img src="笔记图保存\d67081d0-d3f7-4640-967d-9c04b2c28ac2.png" alt="d67081d0-d3f7-4640-967d-9c04b2c28ac2" style="zoom:50%;" />

代码实现，核心公式：

<img src="笔记图保存\dfab5805-1292-48b8-8fd0-fd93f6739b66.png" alt="dfab5805-1292-48b8-8fd0-fd93f6739b66" style="zoom:33%;" />

```python
from typing import Optional, Union, List, Dict, Any

import torch
from torch.nn import Linear, Parameter
from torch_geometric.nn import MessagePassing, Aggregation
from torch_geometric.utils import add_self_loops, degree

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class GCNConv(MessagePassing):
    # 初始化类，这里添加了（in_channels, out_channels）的输入定义
    def __init__(self, aggr: Optional[Union[str, List[str], Aggregation]] = 'sum', *,
                 aggr_kwargs: Optional[Dict[str, Any]] = None, flow: str = "source_to_target", node_dim: int = -2,
                 decomposed_layers: int = 1, in_channels, out_channels) -> None:
        super().__init__(aggr, aggr_kwargs=aggr_kwargs, flow=flow, node_dim=node_dim,
                         decomposed_layers=decomposed_layers)
        # 设置线性变换层
        self.lin = Linear(in_channels, out_channels)
        # 设置偏置项
        # Parameter() 函数的作用是将一个 Tensor 包装成模型的可学习参数，使其能够被自动管理并在训练过程中更新。
        self.bias = Parameter(torch.empty(out_channels))

    def forward(self, x, edge_index):
        # 添加自环信息
        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))

        # 对节点特征进行线性变换
        x = self.lin(x)

        # 获取输入（邻居节点xj）和输出（中心节点xi）的索引
        xj, xi = edge_index
        # 获取中心节点的度
        deg = degree(xi, x.size(0), dtype=x.dtype)
        # 计算中心节点度的-1/2次方
        deg_inv_sqrt = deg.pow(-0.5)
        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0

        # 边权重缩放：入度^-1/2 * 出度^-1/2
        norm = deg_inv_sqrt[xj] * deg_inv_sqrt[xi]

        # 调用propagate函数，传入edge_index、特征x、和归一化计算方法norm
        # 内部调用message()函数，自动拆分edge_index为索引对应的节点特征，并将x_j传入给message()
        out = self.propagate(edge_index, x=x, norm=norm)

        # 最终添加偏置项
        out += self.bias

        return out

    def message(self, x_j, norm):
        # 边信息处理（优先于邻居加法聚合）：入度^-1/2 * 出度^-1/2 * 邻居节点特征
        return norm.view(-1, 1) * x_j


if __name__ == '__main__':
    # 定义4个节点属性
    x = torch.tensor([[1, 2],
                      [2, 3],
                      [8, 3],
                      [2, 4]], dtype=torch.float32)
    # 定义边（2维矩阵形式，第1行：入，第2行：出）
    edge_index = torch.tensor([[0, 0, 1, 2, 2, 3],
                               [0, 1, 2, 1, 3, 2]], dtype=torch.int64)

    x = x.to(device)
    edge_index = edge_index.to(device)
    gcn_conv = GCNConv(in_channels=2, out_channels=4).to(device)

    result = gcn_conv(x, edge_index)
    print(result)

```

# 计算机视觉

图像分类、目标检测、风格转换

问题在于，如使用全连接神经网络，图像过大：

1M图像有1000*1000个像素，再乘以3个通道是输入X的特征维度，假设第1层神经元维度1000，中间矩阵：1000 * 1000*1000 * 3，计算过于复杂，且参数量过大极易导致过拟合。

## CNN

**边缘探测**

如下图，对垂直信息进行检测，如果左右列向量很类似的话，会被相加抵消，如果差距较大，则会出现数值。（寻找逻辑：忽视中间数据，而左边很明亮，右边较暗的垂直边缘）

<img src="笔记图保存\2eca5bbc-4a28-47a9-b964-c8da79f3e6a0.png" alt="2eca5bbc-4a28-47a9-b964-c8da79f3e6a0" style="zoom:33%;" />

检测的 正负边界：卷积结果数值和原本的1、-1 是成正或者负相关

<img src="笔记图保存\f77fe6cc-7c43-47e6-93d2-00a9b1b22896.png" alt="f77fe6cc-7c43-47e6-93d2-00a9b1b22896" style="zoom:33%;" />

算子可有多种，可考虑不手动设置，而是利用反向传播学习对应数值，目的是能够更准确地划分出目标边界。

<img src="笔记图保存\c7d44e65-a45b-4eb6-9693-22fb1ba9b22e.png" alt="c7d44e65-a45b-4eb6-9693-22fb1ba9b22e" style="zoom:33%;" />

**填充**

初始卷积缺点：

1.每次卷积将缩小图片，导致图片不断变小

2.对图中角落像素的使用次数过少，失去图片靠近边界的信息

<img src="笔记图保存\14a69009-a79a-4dba-a148-3e60f272ce25.png" alt="14a69009-a79a-4dba-a148-3e60f272ce25" style="zoom:33%;" />

通过padding，能充分防止图像变小，且能有效利用边缘像素信息

<img src="笔记图保存\3de285d4-1e05-4409-b6a9-b0f4901e1c40.png" alt="3de285d4-1e05-4409-b6a9-b0f4901e1c40" style="zoom:33%;" />

同时，如下可以很好地解释，为何算子都是奇数，如3×3，5×5：p=(f-1)/2

<img src="笔记图保存\f73e496b-bf22-40ed-baa9-c552898812c7.png" alt="f73e496b-bf22-40ed-baa9-c552898812c7" style="zoom:33%;" />

**步长**

设置步长，即进行卷积时上下都会跳过特定步数

<img src="笔记图保存\5ae89261-be0f-49c6-a029-8e1fd7a28afe.png" alt="5ae89261-be0f-49c6-a029-8e1fd7a28afe" style="zoom:33%;" />

问题：卷积算子有可能会超出图像边界，解决方法：向下取整

<img src="笔记图保存\25a4233a-819d-4ac5-8f6f-fa7039efe487.png" alt="25a4233a-819d-4ac5-8f6f-fa7039efe487" style="zoom:33%;" />

卷积整体公式：

<img src="笔记图保存\88d8295c-e322-4ad7-8e8c-8407c39aebba.png" alt="88d8295c-e322-4ad7-8e8c-8407c39aebba" style="zoom:33%;" />

数学教材中，实际上会对卷积进行翻转操作，而在机器学习中，则实际上会忽略这个步骤，

如果在数学上，不进行翻转的操作实际上应该称为cross-correlation而不是convolution。

深度学习中的“卷积”实际上是 cross-correlation，但大家仍习惯称为 convolution。

<img src="笔记图保存\c4b13662-a93f-486f-9bc7-d1f118b9d565.png" alt="c4b13662-a93f-486f-9bc7-d1f118b9d565" style="zoom:33%;" />

**3层卷积**

可分别对RGB3个通道，实现不同的卷积操作

<img src="笔记图保存\9f0d6c9d-83ac-4788-b4e9-1037f66edcfc.png" alt="9f0d6c9d-83ac-4788-b4e9-1037f66edcfc" style="zoom:33%;" />

可采用不同的滤波器组，实现水平、垂直....边缘检测，最后将卷积结果叠在一起

<img src="笔记图保存\d80991b2-b682-4192-9a04-1c04cf495c56.png" alt="d80991b2-b682-4192-9a04-1c04cf495c56" style="zoom:33%;" />

## MobileNet

深度可分离卷积：

**Depthwise卷积（逐通道卷积）**

这里，逐通道卷积将保留中间结果而不是直接求和。

<img src="笔记图保存\09f4a18b-d5b7-43c1-a136-411ae43eac9d.png" alt="09f4a18b-d5b7-43c1-a136-411ae43eac9d" style="zoom:33%;" />

**Pointwise卷积（逐点卷积）**

5个1 * 1 * 3的滤波器，对先前逐通道卷积的结果进一步卷积计算

<img src="笔记图保存\432add1f-1216-40a2-bdfb-3ea8ef1de21a.png" alt="432add1f-1216-40a2-bdfb-3ea8ef1de21a" style="zoom:33%;" />

**MobileNetV2**

特点：增加了残差结构、增加了Expansion和Projection操作（Bottleneck）

<img src="笔记图保存\526dc68b-1f54-4141-b26a-219a02bca1b7.png" alt="526dc68b-1f54-4141-b26a-219a02bca1b7" style="zoom:33%;" />



## EfficientNet

EfficientNet：考虑特定设备，对原始图像分辨率（r），神经网络深度（d），滤波器宽度（w），

通过一个复合缩放方法（compound scaling）同时、均衡地缩放这三个维度。

<img src="笔记图保存\045c780b-3dc3-4587-8740-e1dfac97ec1a.png" alt="045c780b-3dc3-4587-8740-e1dfac97ec1a" style="zoom:33%;" />

## 迁移学习

迁移学习一般发生在已有数据集较小的情况下。

可选择性冻结某些层，固定其参数设置，仅对一部分层的参数进行训练（只改动softmax？只改动后几层？替换后几层？...等等）

此类情况一般在数据集比较小的时候适用，数据集越大，越不需要实现迁移学习，可训练的层数和参数也越多。

极端情况：可下载别人的全部权重，作为初始化参数对整个模型更新，而这可能也比自己的初始化参数设置更好。

<img src="笔记图保存\17e96f6b-1c86-48ff-a99a-5a1b25498cbe.png" alt="17e96f6b-1c86-48ff-a99a-5a1b25498cbe" style="zoom:33%;" />

## 数据增强

对计算机视觉任务来说，和机器学习不同，数据集越大越好（其它机器学习任务中，可能数据集达到一定规模后就不用再继续增加了）

**几何变换**

镜像、随机裁剪、旋转、Shearing（剪切变换，变为平行四边形）、Local Warping（局部扭曲）等

<img src="笔记图保存\ead322fc-fbdd-4d2f-a6a3-0088c1dc2b48.png" alt="ead322fc-fbdd-4d2f-a6a3-0088c1dc2b48" style="zoom:33%;" />

**颜色变换**

例如对RGB进行轻微修改（Alex-Net：PCA色彩增强？）

<img src="笔记图保存\bedc544c-3228-4577-aedf-6fd9e5c95139.png" alt="bedc544c-3228-4577-aedf-6fd9e5c95139" style="zoom:33%;" />

通常来说，CPU对数据增强的部分，和GPU进行训练的过程，会在多个进程下同步进行（CPU 数据增强通常在多进程下实现，GPU 训练在主进程中进行，整体采用“多进程数据加载 + 单进程 GPU 训练”的混合架构，以最大化硬件利用率）。

<img src="笔记图保存\9865c591-6900-43c8-a847-c053854a21dc.png" alt="9865c591-6900-43c8-a847-c053854a21dc" style="zoom:33%;" />

## YOLO算法

**目标定位**

图像分类：判断整个图像的类别

目标定位：通常是单个对象，判断类别的同时，需要框出对应位置

目标检测：通常是多个对象，判断类别的同时，需要框出对应位置

<img src="笔记图保存\94a7f3f2-6aca-492e-8f01-d5ed3bff017d.png" alt="94a7f3f2-6aca-492e-8f01-d5ed3bff017d" style="zoom:33%;" />

标签：图像类别 + x(框中心横坐标),y(框中心纵坐标),w(框宽度),h(框高度)

softmax不仅需要输出4种类别，还需输出4个对应的框坐标

<img src="笔记图保存\7afae81f-2bbf-4f5d-8659-d572909c8f4f.png" alt="7afae81f-2bbf-4f5d-8659-d572909c8f4f" style="zoom:33%;" />

第1个元素：0-1，表示是否存在对象

4个元素：位置

4个元素：具体类别/具体类别概率

如果第1个元素为0，剩下的元素预测都不会在乎

<img src="笔记图保存\dfa271ea-db1c-497a-b494-f3074c2fdb0a.png" alt="dfa271ea-db1c-497a-b494-f3074c2fdb0a" style="zoom:33%;" />

损失函数如何设计：分段函数

例如，针对C1-C4：交叉熵？针对bx-bw：均方根误差？Pc：逻辑回归？

<img src="笔记图保存\cc62c2cb-6495-480f-898d-790d03501a41.png" alt="cc62c2cb-6495-480f-898d-790d03501a41" style="zoom: 50%;" />

**特征点检测**

例如人脸关键点检测：

第1个元素表示是否识别到脸

64*2个关键点表示脸部关键特征点位置（x和y）

对于人体骨骼检测，同理。

<img src="笔记图保存\05da6967-66a9-4e3c-90a5-2680f41caf3d.png" alt="05da6967-66a9-4e3c-90a5-2680f41caf3d" style="zoom:33%;" />

**目标检测**

训练集：裁剪好的贴近车身的图片，带有标签：0-1（是否为汽车）

<img src="笔记图保存\304ba2f5-482a-48ec-a75b-a41a5f98797e.png" alt="304ba2f5-482a-48ec-a75b-a41a5f98797e" style="zoom:33%;" />

通过不同大小的窗口在图中滑动，判定每个方框内的区域是否含有车（0-1）

<img src="笔记图保存\3ba73454-747d-489d-a759-8c2fb305be54.png" alt="3ba73454-747d-489d-a759-8c2fb305be54" style="zoom:33%;" />

**卷积滑窗检测**

下图可见，全连接层的操作均可通过卷积操作替代

<img src="笔记图保存\54a6a7bc-1a0f-449a-96e0-8a94b208841e.png" alt="54a6a7bc-1a0f-449a-96e0-8a94b208841e" style="zoom:33%;" />

不是通过滑动窗口分别对不同区域进行卷积（会存在大量重复的区域操作），

而是直接基于卷积对整个图片进行变换，使用1次前向传播过程代替多次。

最终的卷积结果，就是对应在原图相应位置窗口上依次滑动进行的卷积操作结果。

<img src="笔记图保存\8f8d13a6-b61c-4446-b527-9a0892bfd8b6.png" alt="8f8d13a6-b61c-4446-b527-9a0892bfd8b6" style="zoom:33%;" />

<img src="笔记图保存\546aa0c3-0b4a-4193-a1fe-235bf3b08ad2.png" alt="546aa0c3-0b4a-4193-a1fe-235bf3b08ad2" style="zoom:33%;" />

**边框预测**

YOLO（You Only Look Once）算法：

将图像划分为N*N的区域，每个区域具有标签：

Pc：0-1是否存在目标（中心点是否在对应网格内）（此处如中间图，虽含有部分目标，但目标中心点未在对应区域，因此也为0）

bx-bw：框位置

c1-c3：类别判断

例如针对此处整个图，卷积操作应该输出：3×3×8

此时每个网格中，不能出现超过1个的检测物

<img src="笔记图保存\bc5e4950-18d3-4a9f-82f6-699a2a93e151.png" alt="bc5e4950-18d3-4a9f-82f6-699a2a93e151" style="zoom:33%;" />

bx，by：相对于网格（3*3边框中的1个）左上角的距离确定（0-1之间）

bh，bw：相对于网格（3*3边框中的1个）的长度比例决定（可大于1）

<img src="笔记图保存\0f5418d8-11e9-4dd3-b2c5-fd1f58789175.png" alt="0f5418d8-11e9-4dd3-b2c5-fd1f58789175" style="zoom:33%;" />

**交并比**

交并比 IOU（Intersection Over Union）：

预测、实际检测框的交集/并集，一般取0.5作为预测正确基准，越高表示检测效果越好（1重合）

<img src="笔记图保存\e8fc1b17-95d8-41aa-a342-e85d26cb6942.png" alt="e8fc1b17-95d8-41aa-a342-e85d26cb6942" style="zoom:33%;" />

**非极大值抑制**

由于很多检测框可能指向同个对象，因此进行非极大值抑制，如下为单类别检测时的做法（筛选、排序、去重）：

1.去除所有小于某个Pc的检测框

2.排序模型预测的可能性Pc，找出最高Pc对应的检测框

3.对于最高Pc对应的检测框，计算其它检测框与其IOU，并去除其中 IOU>0.5 的检测框，以防止对同一目标重复检测

<img src="笔记图保存\3c87225a-6640-444f-9b8f-6309dc2a09f0.png" alt="3c87225a-6640-444f-9b8f-6309dc2a09f0" style="zoom:33%;" />

如果是多类别？

<img src="笔记图保存\8ac86d5b-9b13-460d-8658-99d63cd459bc.png" alt="8ac86d5b-9b13-460d-8658-99d63cd459bc" style="zoom: 50%;" />

对每一个类别，都独立执行一次NMS流程

1.去除所有小于某个阈值（Pc×类别概率）的检测框

2.排序模型预测的可能性得分（Pc×类别概率），找出最高得分（Pc×类别概率）对应的检测框

3.对于最高得分检测框，计算其它检测框与其IOU，并去除其中 IOU>0.5 的检测框，以防止对同一目标重复检测

**锚框（Anchor box）**

在先前模型中，单个网格中仅能检测单个对象，如要检测多个对象，则可利用锚框思想。

锚框：在预定义好的形状上微调检测框，而不是从头开始去预测目标。

每次检测时，如下图所示，向量中依次包含对每个锚框的检测结果，实现一个网格中的多目标检测。

<img src="笔记图保存\7154b9c0-0461-40b7-a3c3-478ddda2dc17.png" alt="7154b9c0-0461-40b7-a3c3-478ddda2dc17" style="zoom:33%;" />

预定义好不同的锚框形状，定义好锚框的初始位置。

<img src="笔记图保存\PixPin_2025-08-18_15-00-23.png" alt="PixPin_2025-08-18_15-00-23" style="zoom: 50%;" />

<img src="笔记图保存\PixPin_2025-08-18_15-09-24.png" alt="PixPin_2025-08-18_15-09-24" style="zoom:50%;" />

<img src="笔记图保存\PixPin_2025-08-18_15-06-23.png" alt="PixPin_2025-08-18_15-06-23" style="zoom:50%;" />

最后，通过NMS等后处理步骤，去除掉对于同一个物体的大量重叠的、置信度较低的预测框，保留下最终的检测结果。

## R-CNN

针对整个图片进行卷积操作，可能会针对大量不包含目标的无效区域进行，造成浪费。

R-CNN（Region-based Convolutional Neural Network）：对图片进行区域划分，然后针对每个区域（约2000个）放置边界框，并针对特定对应边界框进行卷积运算。

<img src="笔记图保存\PixPin_2025-08-18_15-35-25.png" alt="PixPin_2025-08-18_15-35-25" style="zoom:50%;" />

R-CNN问题：针对每个区域都要单独进行卷积操作，速度太慢

Fast R-CNN：使用卷积滑窗检测，1次卷积操作替代对每个区域的窗口卷积生成整个图的特征，并将候选区域映射到特征图上

Faster R-CNN：Fast R-CNN进行区域聚类的速度还是很慢，提出用 RPN（Region Proposal Network）替代 Selective Search，实现完全端到端的区域建议生成（即使用卷积操作进行区域划分），进一步提升速度

Mask R-CNN：在 Faster R-CNN 基础上增加分支用于实例分割

但大部分的应用中，区域划分下的目标检测算法，通常还是会比YOLO算法慢一些。

## U-Net

**语义分割**

和目标检测不同，图像分割任务将输出整个图的标签，对不同元素进行划分

<img src="笔记图保存\PixPin_2025-08-19_09-14-59.png" alt="PixPin_2025-08-19_09-14-59" style="zoom:50%;" />

原始卷积：先缩小-再缩小，语义分割：先缩小-再放大

核心在于，取消卷积中将图像进一步缩小的操作后，通过什么操作将图像放大？

<img src="笔记图保存\PixPin_2025-08-19_09-16-29.png" alt="PixPin_2025-08-19_09-16-29" style="zoom:50%;" />

**反卷积**

将原始图像中的元素，与对应滤波器中的每个元素依次相乘，将结果放入待生成区域。

下图中，输出期望4×4，padding=1，stride=2

<img src="笔记图保存\PixPin_2025-08-19_09-26-23.png" alt="PixPin_2025-08-19_09-26-23" style="zoom:50%;" />

<img src="笔记图保存\PixPin_2025-08-19_09-30-10.png" alt="PixPin_2025-08-19_09-30-10" style="zoom:50%;" />

**U-Net架构**

先卷积W-，H-，C+，后反卷积W+，H+，C-。

跳跃连接（Skip connection）：将左边的信息沿通道C方向与右侧信息拼接。

U-Net 采用编码器（下采样路径）和解码器（上采样路径）的对称结构：

- 编码器：通过卷积和池化操作提取高层次特征，逐步降低空间分辨率，增强语义信息。
- 解码器：通过上采样和卷积操作逐步恢复空间分辨率，实现像素级预测。

这种结构能有效结合低层细节和高层语义信息。

<img src="笔记图保存\PixPin_2025-08-19_09-53-39.png" alt="PixPin_2025-08-19_09-53-39" style="zoom: 33%;" />



## 人脸识别

**单样本学习**

人脸识别场景中，往往只有单个人的脸部样本，称为单样本（one-shot）问题。

此时并没有足够数据支持模型分类学习，思路是利用神经网络寻找一种函数，d(img1,img2)=N，用于确定2张图片之间的差异。

通过设定N的阈值，确定2张图片是否属于同一人。

**孪生神经网络**

孪生神经网络（Siamese network）：训练2个神经网络，分别生成128维向量，根据损失函数输出任意2张图片的差距。

<img src="笔记图保存\PixPin_2025-08-19_10-29-41.png" alt="PixPin_2025-08-19_10-29-41" style="zoom: 50%;" />

**三元损失函数**

三元即分别查看3张图片：Anchor（锚照片）、Positive（正例照片）、Negative（负例照片）

Anchor和Positive：同一人

Anchor和Negative：不同人

<img src="笔记图保存\PixPin_2025-08-19_15-12-24.png" alt="PixPin_2025-08-19_15-12-24" style="zoom: 50%;" />

||f(A) - f(P)||^2 <= ||f(A) - f(N)||^2

公式中，||f(A) - f(P)||^2 可看作 d（A，P）即Anchor和Positive间的距离

||f(A) - f(N)||^2 可看作 d（A，N）即Anchor和Negative间的距离

转换公式，变为 ||f(A) - f(P)||^2 - ||f(A) - f(N)||^2 <= 0，但此时神经网络可能会倾向于将f(A)、 f(P)、f(N)全部输出0，

为防止退化解，这里添加超参数margin：||f(A) - f(P)||^2 - ||f(A) - f(N)||^2  + margin <= 0

整体损失函数：

<img src="笔记图保存\PixPin_2025-08-19_15-46-45.png" alt="PixPin_2025-08-19_15-46-45" style="zoom:50%;" />

此时，训练数据集需要A，P，N的三元组形式，意味着必须需要有些成对的A和P，如果1个人只有1张图片，将无法进行训练。

如果随意寻找A和N，则 d（A，N）一定很大，这个式子很容易得到满足，导致神经网络无法学习有效内容，

因此需寻找较“难”学习的训练数据，也就是||f(A) - f(P)||^2和||f(A) - f(N)||^2需很接近。

在 FaceNet 等系统中，使用的就是“孪生式三元网络”（有时叫 Triplet Network），即三个共享权重的子网络分别处理 A、P、N，然后计算三元损失。严格来说，使用三元损失的网络有时被称为 三胞胎网络（Triplet Network），是孪生网络思想的扩展。

## **神经风格迁移**

**迁移学习的代价函数**

下图中展示了代价函数的构成：

内容损失+风格损失

内容损失：原图C-生成图G

风格损失：风格图S-生成图G

<img src="笔记图保存\PixPin_2025-08-19_16-38-09.png" alt="PixPin_2025-08-19_16-38-09" style="zoom:50%;" />

1.随机生成特定大小图片（充满噪点）

2.利用梯度下降，最小化代价函数，使得图像发生变化

<img src="笔记图保存\PixPin_2025-08-19_16-41-01.png" alt="PixPin_2025-08-19_16-41-01" style="zoom:50%;" />

**内容代价函数**

使用预训练网络，输出中间层（可自选），计算C和G特征图的差异，作为内容损失。

对应矩阵在对应通道上做减法，求平方，做和，每个通道得到一个数值，

算出每个通道的差平方和后，最后再相加，得到一个数值类型标量。

<img src="笔记图保存\PixPin_2025-08-19_16-47-23.png" alt="PixPin_2025-08-19_16-47-23" style="zoom:50%;" />

**风格代价函数**

如何量化1张图的风格？

选择一个中间层输出，计算其不同通道之间的相关性（correlation）

具体如何衡量：将第k个和第k'个通道上的激活元相乘再求和

<img src="笔记图保存\PixPin_2025-08-19_16-55-31.png" alt="PixPin_2025-08-19_16-55-31" style="zoom:50%;" />

最终可以得到一个Gram矩阵，用于衡量一张图片的风格：

<img src="笔记图保存\PixPin_2025-08-19_17-03-51.png" alt="PixPin_2025-08-19_17-03-51" style="zoom:50%;" />

而要定义风格代价函数，对S和G的Gram矩阵作差求平方即可。

对矩阵每个位置的差值平方求和，最终得到一个数值类型标量。

<img src="笔记图保存\PixPin_2025-08-19_17-08-17.png" alt="PixPin_2025-08-19_17-08-17" style="zoom:50%;" />

这里仅选择一个中间层输出进行风格代价函数计算，但实际上可自选多个（如不同的早期层、晚期层以实现风格控制）（同步设置多个权重参数），使结果更优。
